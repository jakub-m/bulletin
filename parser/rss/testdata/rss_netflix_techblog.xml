<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[Netflix TechBlog - Medium]]></title>
        <description><![CDATA[Learn about Netflix’s world class engineering efforts, company culture, product developments and more. - Medium]]></description>
        <link>https://netflixtechblog.com?source=rss----2615bd06b42e---4</link>
        <image>
            <url>https://cdn-images-1.medium.com/proxy/1*TGH72Nnw24QL3iV9IOm4VA.png</url>
            <title>Netflix TechBlog - Medium</title>
            <link>https://netflixtechblog.com?source=rss----2615bd06b42e---4</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Fri, 30 Jul 2021 09:13:38 GMT</lastBuildDate>
        <atom:link href="https://netflixtechblog.com/feed" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[Data Movement in Netflix Studio via Data Mesh]]></title>
            <link>https://netflixtechblog.com/data-movement-in-netflix-studio-via-data-mesh-3fddcceb1059?source=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/3fddcceb1059</guid>
            <category><![CDATA[data-mesh]]></category>
            <category><![CDATA[data-streaming]]></category>
            <category><![CDATA[data-movement]]></category>
            <category><![CDATA[data-pipeline]]></category>
            <category><![CDATA[stream-processing]]></category>
            <dc:creator><![CDATA[Netflix Technology Blog]]></dc:creator>
            <pubDate>Mon, 26 Jul 2021 18:00:56 GMT</pubDate>
            <atom:updated>2021-07-28T21:30:35.048Z</atom:updated>
            <content:encoded><![CDATA[<p>By <a href="https://www.linkedin.com/in/andrewnguonly/"><em>Andrew Nguonly</em></a><em>, </em><a href="https://www.linkedin.com/in/armandomagalhaes/"><em>Armando Magalhães</em></a><em>, </em><a href="https://www.linkedin.com/in/onwoke/"><em>Obi-Ike Nwoke</em></a>, <a href="https://www.linkedin.com/in/shervinafshar/"><em>Shervin Afshar</em></a><em>, </em><a href="https://www.linkedin.com/in/sreyashidas/"><em>Sreyashi Das</em></a>, <a href="https://www.linkedin.com/in/tonylxc/"><em>Tongliang Liu</em></a>, <a href="https://www.linkedin.com/in/davidliusde/"><em>Wei Liu</em></a>, <a href="https://www.linkedin.com/in/yuchengzeng/"><em>Yucheng Zeng</em></a></p><h3>Background</h3><p>Over the next few years, most content on Netflix will come from Netflix’s own Studio. From the moment a Netflix film or series is pitched and long before it becomes available on Netflix, it goes through <a href="https://www.youtube.com/watch?v=pDu8Ccpr6Us&amp;t=119s">many phases</a>. This happens at an unprecedented scale and introduces many interesting challenges; one of the challenges is how to provide visibility of Studio data across multiple phases and systems to facilitate operational excellence and empower decision making. Netflix is known for its loosely coupled microservice architecture and with a global studio footprint, surfacing and connecting the data from microservices into a studio data catalog in real time has become more important than ever.</p><p>Operational Reporting is a reporting paradigm specialized in covering high-resolution, low-latency data sets, serving detailed day-to-day activities and processes of a business domain. Such a paradigm aspires to assist front-line operations personnel and stakeholders in performing their tasks through means such as ad hoc analysis, decision-support, and tracking (of tasks, assets, schedules, etc). The paradigm spans across methods, tools, and technologies and is usually defined in contrast to analytical reporting and predictive modeling which are more strategic (vs. tactical) in nature.</p><p>At Netflix Studio, teams build various views of business data to provide visibility for day-to-day decision making. With dependable near real-time data, Studio teams are able to track and react better to the ever-changing pace of productions and improve efficiency of global business operations using the most up-to-date information. Data connectivity across Netflix Studio and availability of Operational Reporting tools also incentivizes studio users to avoid forming data silos.</p><h3>The Journey</h3><p>In the past few years, Netflix Studio has gone through few iterations of data movement approaches. In the initial stage, data consumers set up ETL pipelines directly pulling data from databases. With this batch style approach, several issues have surfaced like data movement is tightly coupled with database tables, database schema is not an exact mapping of business data model, and data being stale given it is not real time etc. Later on, we moved to event driven streaming data pipelines (powered by <a href="https://netflixtechblog.com/delta-a-data-synchronization-and-enrichment-platform-e82c36a79aee">Delta</a>), which solved some problems compared to the batch style, but had its own pain points, such as a high learning curve of stream processing technologies, manual pipeline setup, a lack of schema evolution support, inefficiency of onboarding new entities, inconsistent security access models, etc.</p><p>With the latest Data Mesh Platform, data movement in Netflix Studio reaches a new stage. This configuration driven platform decreases the significant lead time when creating a new pipeline, while offering new support features like end-to-end schema evolution, self-serve UI and secure data access. The high level diagram below indicates the latest version of data movement for Operational Reporting.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*132y5b5L4XniSaVj2jpiBQ.png" /><figcaption>Operational Reporting Architecture Overview</figcaption></figure><p>For data delivery, we leverage the Data Mesh platform to power the data movement. Netflix Studio applications expose GraphQL queries via <a href="https://netflixtechblog.com/how-netflix-scales-its-api-with-graphql-federation-part-1-ae3557c187e2">Studio Edge</a>, which is a unified graph that connects all data in Netflix Studio and provides consistent data retrieval. Change Data Capture(CDC) source connector reads from studio applications’ database transaction logs and emits the change events. The CDC events are passed on to the Data Mesh enrichment processor, which issues GraphQL queries to Studio Edge to enrich the data. Once the data has landed in the Iceberg tables in Netflix Data Warehouse, they could be used for ad-hoc or scheduled querying and reporting. Centralized data will be moved to third party services such as Google Sheets and Airtable for the stakeholders. We will deep dive into Data Delivery and Data Consumption in the following sections.</p><h3>Data Delivery via Data Mesh</h3><h4>What is Data Mesh?</h4><p>Data Mesh is a fully managed, streaming data pipeline product used for enabling Change Data Capture (CDC) use cases. In Data Mesh, users create sources and construct pipelines. Sources mimic the state of an externally managed source — as changes occur in the external source, corresponding CDC messages are produced to the Data Mesh source. Pipelines can be configured to transform and store data to externally managed sinks.</p><p>Data Mesh provides a drag-and-drop, self-service user interface for exploring sources and creating pipelines so that users can focus on delivering business value without having to worry about managing and scaling complex data streaming infrastructure.</p><h4>CDC and data source</h4><p>Change data capture or <a href="https://netflixtechblog.com/dblog-a-generic-change-data-capture-framework-69351fb9099b">CDC</a>, is a semantic for processing changes in a source for the purpose of replicating those changes to a sink. The table changes could be row changes (insert row, update row, delete row) or schema changes (add column, alter column, drop column). As of now, CDC sources have been implemented for data stores at Netflix (MySQL, Postgres). CDC events can also be sent to Data Mesh via a Java Client Producer Library.</p><h4>Reusable Processors and Configuration Driven</h4><p>In Data Mesh, a processor is a configurable data processing application that consumes, transforms, and produces CDC events. A processor has 1 or more inputs and 0 or more outputs. Processors with 0 outputs are sink connectors; which write events to externally managed sinks (e.g. Iceberg, ElasticSearch, etc).</p><figure><img alt="Processors with Different Inputs/Outputs" src="https://cdn-images-1.medium.com/max/1024/1*fGdwKTItvkw0nawRdzdjSg.png" /><figcaption>Processors with Different Inputs/Outputs</figcaption></figure><p>Data Mesh allows developers to contribute processors to the platform. Processors are not necessarily centrally developed and managed. However, the Data Mesh platform team strives to provide and manage the most highly leveraged processors (e.g. source connectors and sink connectors)</p><p>Processors are reusable. The same processor image package is used multiple times for all instances of the processor. Each instance is configured to fit each use case. For example, a GraphQL enrichment processor can be provisioned to query GraphQL Services to enrich data in different pipelines; an Iceberg sink processor can be initialized multiple times to write data to different databases/tables with different schema.</p><h4>End-to-End Schema Evolution</h4><p>Schema is a key component of Data Mesh. When an upstream schema evolves (e.g. schema change in the MySQL table), Data Mesh detects the change, checks the compatibility and applies the change to the downstream. With schema evolution, Data Mesh ensures the Operational Reporting pipelines always produce data with the latest schema.</p><p>We will cover a few core concepts in the Data Mesh Schema domain.</p><p><strong>Consumer schema<br></strong>Consumer schema defines how data is consumed by the downstream processors. See example below.</p><figure><img alt="Consumer Schema Example" src="https://cdn-images-1.medium.com/max/1024/1*d7ue973r-fpr6xh4mOl7VQ.png" /><figcaption>Consumer Schema Example</figcaption></figure><p><strong>Schema Compatibility</strong><br>Data Mesh uses Consumer Schema compatibility to achieve flexible yet safe schema evolution. If a field consumed by an Operational Reporting pipeline is removed from CDC source, Data Mesh categorizes this change as <strong>incompatible</strong>, pauses the pipeline processing and notifies the pipeline owner. On the other hand, if a required field is not consumed by any consumer, dropping such fields would be <strong>compatible</strong>.</p><p><strong>Two Types of Processors<br></strong>1. Pass through all fields from upstream to downstream.</p><ul><li>Example: Filter Processor, Sink Processors</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*wowPrRfYZDtEq0aywOmihg.png" /><figcaption>Opt in to schema Evolution example</figcaption></figure><p>2. Only uses a subset of fields from upstream.</p><ul><li>Example: Project Processor, Enrichment Processor</li></ul><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*C37WtYzNuqoK1T3_4STdkw.png" /><figcaption>Opt out to schema Evolution example</figcaption></figure><p>In Data Mesh, we introduce the<em> </em><strong><em>Opt-in to Schema Evolution </em></strong>boolean flag to differentiate those two types of use cases.</p><ul><li><strong>Opt in:</strong> All the upstream fields will be propagated to the processor. For example, when a new field is added upstream, it will be propagated automatically.</li><li><strong>Opt out:</strong> Only a subset of fields (defined using ‘Is Consumed’ checkboxes) is propagated and used in the processor. Upstream changes to the rest of the fields won’t affect this processor.</li></ul><p><strong>Schema Propagation<br></strong>After the Schema Compatibility is checked, Data Mesh Platform will propagate the schema change based on the end user’s intention. With the opt-in to schema Evolution flag, Operational Reporting pipelines can keep the schema up-to-date with upstream data stores. As part of schema propagation, the platform also syncs the schema from the pipeline to the Iceberg sink.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Trl8JdVCgWwZs6u7VftmRA.png" /><figcaption>Schema Evolution Diagram</figcaption></figure><h4>Enrichment Processor via GraphQL</h4><p>In the current Data Mesh Operational Reporting pipelines, the most commonly used intermediate processor is the GraphQL Enrichment Processor. It takes in the column value from CDC events coming from Source Connector as GraphQL query input, then submits a query to Studio Edge to enrich the data. With Studio Edge’s single data model, it centralizes data modeling efforts, which is highly leveraged by Studio UI Apps, Backend services and Search platforms. Enriching the data via Studio Edge helps us achieve consistent data modeling across the whole ecosystem for Operational Reporting.</p><p>Here is the example of GraphQL processor configuration, pipeline builder only need config the following fields to provision an enrichment processor:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*My-XjCIfKUp6CbAzLpIHmg.png" /><figcaption>GraphQL Enrichment Processor Configuration Example</figcaption></figure><p>The image below is a sample Operational Reporting pipeline in the production environment to sink the Movie related data. Teams who want to move their data no longer need to learn and write customized Stream Processing jobs. Instead they just need to configure the pipeline topology in the UI while getting other features like schema evolution and secure data access out of the box.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*LibXzGQpg1oZl_lksKLVjw.png" /><figcaption>Operational Reporting Pipeline Example</figcaption></figure><h4>Iceberg Sink</h4><p><a href="https://iceberg.apache.org/">Apache Iceberg</a> is an open source table format for huge analytics datasets. Data Mesh leverages Iceberg tables as data warehouse sinks for downstream analytics use cases. Currently Iceberg sink is appended only. Views are built on top of the raw Iceberg tables to retrieve the latest record for every primary key based on the operational timestamp, which indicates when the record is produced in the sink. Current pipeline consumers are directly consuming Views instead of raw tables.</p><p>The compaction process is needed to optimize the performance of downstream queries on the business view as well as lower costs of S3 GET OBJECT operations. A daily process ranks the records by timestamp to generate a data frame of compacted records. Old data files are overwritten with a set of new data files that contain only the compacted data.</p><h4>Data Quality</h4><p>Data Mesh provides metrics and dashboards at both the processor and pipeline level for operational observability. Operational Reporting pipeline owners will get alerts if something goes wrong with their pipelines. We also have two types of auditing on the data tables generated from Data Mesh pipelines to guarantee data quality: end-to-end auditing and synthetic events.</p><p>Most of the business views created on top of the Iceberg tables can tolerate a few minutes of latency. However, it is paramount that we validate the complete set of identifiers such as a list of movie ids across producers and consumers for higher overall confidence in the data transport layer of choice. For end-to-end audits, the objective is to run the audits hourly via Big data Platform Scheduler, which is a centralized and integrated tool provided by Netflix data platform for running workflows in an efficient, reliable and reproducible way. The audits check for equality (i.e. query results should be the same), the symmetric difference between two data sets should be empty across multiple runs, and the eventual consistency within the SLA. An hourly notification is sent when a set of primary keys consistently do not match between source of truth and target Data Mesh tables.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*HrZ_KECmxpcYNJwG_flK1w.png" /><figcaption>End to End (Black Box) Auditing Example</figcaption></figure><p>Synthetic events audits are artificially triggered change events to imitate common CUD operations of services. It is generating heartbeat signals at a constant frequency with the objective of using them as a baseline to verify the health of the pipeline regardless of traffic patterns or occasional silences.</p><h3>Data Consumption</h3><p>Our studio partners rely on data to make informed decisions and to collaborate during all the phases related to production. The Studio Tech Solutions team provides near real-time reports in some data tool of choice, which we call trackers<strong> </strong>to empower the decision making.</p><p>For the past few years, many of these trackers were powered by hand-curated SQL scripts and API calls being managed by CRON schedulers implemented in a Java Service called Lego. Lego was the main tool for the STS team, and at its peak, Lego managed 300+ trackers.</p><p>This strategy had its own set of challenges: being schema-less and treating every report column like a string not always worked out, the volatile reliance on direct RDS connections and rate limits from third party APIs would often make jobs fail. We had a set of “core views” which would be specifically tailored for reports, but this caused queries that just required a very small subset of fields to be slow and expensive due to the view doing a huge amount of joining and aggregation work before being able to retrieve that small subset.</p><p>Besides the issues, this worked fine when we didn’t have many trackers to maintain, but as we created more trackers to the point of having many hundreds, we started having issues around <strong>maintenance</strong>, <strong>awareness</strong>, <strong>knowledge sharing</strong> and <strong>standardization</strong>. New team members had a hard time getting onboard, figuring out which SQL powered which tracker was tough, the lack of standards made every SQL look different and having to update trackers as the data sources changed was a nightmare.</p><p>With this in mind, the Studio Tech Solutions focused efforts in building <strong>Genesis</strong>, a Semantic Data Layer that allows the team to map data points in Data Source Definitions defined as YAML files and then use those to generate the SQL needed for the trackers, based on a selection of fields, filters and formatters specified in an Input Definition file. Genesis takes care of joining, aggregating, formatting and filtering data based on what is available in the Data Source Definitions and specified by the user through the Input Definition being executed.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*RSmNKuMfWx70tl4hRPhA0A.png" /><figcaption>Genesis Data Source and Input definition example</figcaption></figure><p>Genesis is a stateless CLI written in Node.js that reads everything it needs from the file system based on the paths specified in the arguments. This allows us to hook Genesis into Jenkins Jobs, providing a GitOps and CI experience to maintain existing trackers, as well as create new trackers. We can simply change the data layer, trigger an empty pull request, review the changes and have all our trackers up to date with the data source changes.</p><p>As of the date of writing, Genesis powers 240+ trackers and is growing everyday, empowering thousands of partners in our studios globally to collaborate, annotate and share information using near-real-time data.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*93RAp2LoZLJO2uT1LIQDUA.png" /><figcaption>Git-based Tracker management workflow powered by Genesis and the Big Data Scheduler</figcaption></figure><p>The generated queries are then used in Workflow Definitions for multiple trackers. The Netflix Data Warehouse offers support for users to create data movement workflows that are managed through our Big Data Scheduler, powered by <a href="https://netflix.github.io/titus/">Titus</a>.</p><p>We use the scheduler to execute our queries and move the results to a data tool, which often is a Google Sheet Tab, Airtable base or Tableau dashboard. The scheduler offers templated jobs for moving data from a Trino output to these tools, making it easy to create and maintain hundreds of data movement workflows.</p><p>The diagram below summarizes the data consumption flow when building trackers:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*jnKF7kqsbSLbTm2hI2AAfg.png" /><figcaption>Data Consumption Overview</figcaption></figure><p>As of July 2021, the Studio Tech Solutions team is finishing a migration from all the trackers built in Lego to use Genesis and the Data Portal. This strategy has increased the Studio Tech Solutions team performance and stability. Trackers are now easy for the team to create, review, change, monitor and discover.</p><h3>Now and Future</h3><p>In conclusion, our studio partners have a tracker available to them, populated with near real-time data and tailored to their needs. They can manipulate, annotate, and collaborate using a flexible tool they are familiar with.</p><p>Along the journey, we have learned that evolving data movement in complex domains could take multiple iterations and needs to be driven by the business impact. The great cross-functional partnership and collaboration among all data stakeholders is crucial to shape the ideal data product.</p><p>However, our story doesn’t end here. We still have a long journey ahead of us to fulfill the vision of such ideal data product, especially in areas such as:</p><ul><li>Self-servicing data pipelines provisioning via configuration</li><li>Providing toolings for data discoverability, understandability, usage visibility and change management</li><li>Enabling data domain orientation and ownership/governance management</li><li>Bootstrapping trackers in our Studio ecosystem instead of third party tools. Along the same line as the point above, this would allow us to maintain high standards of data governance, lineage, and security.</li><li>Read-write reports and trackers using GraphQL mutations</li></ul><p>These are some of the interesting areas that Netflix Studio is planning to invest in. We will have follow up blog posts on these topics in future. Please stay tuned!</p><h3>Acknowledgements</h3><p>Data Movement via Data Mesh has been a success in Netflix Studio owing to multiple teams’ efforts. We would like to acknowledge the following colleagues: <a href="https://www.linkedin.com/in/amandabenhamou/">Amanda Benhamou</a>, <a href="https://www.linkedin.com/in/andreas-andreakis/">Andreas Andreakis</a>, <a href="https://www.linkedin.com/in/apreza/">Anthony Preza</a>, <a href="https://www.linkedin.com/in/bolei1007/">Bo Lei</a>, <a href="https://www.linkedin.com/in/czhao/">Charles Zhao</a>, <a href="https://www.linkedin.com/in/justincinmd/">Justin Cunningham</a>,<a href="https://www.linkedin.com/in/kasturi-chatterjee-a900715/"> Kasturi Chatterjee</a>,<a href="https://www.linkedin.com/in/justincinmd/"> </a><a href="https://www.linkedin.com/in/kevinzhu/">Kevin Zhu</a>, <a href="https://www.linkedin.com/in/barreyro/">Stephanie Barreyro</a>, <a href="https://www.linkedin.com/in/yoomikoh/">Yoomi Koh</a>.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=3fddcceb1059" width="1" height="1" alt=""><hr><p><a href="https://netflixtechblog.com/data-movement-in-netflix-studio-via-data-mesh-3fddcceb1059">Data Movement in Netflix Studio via Data Mesh</a> was originally published in <a href="https://netflixtechblog.com">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Data Engineers of Netflix — Interview with Kevin Wylie]]></title>
            <link>https://netflixtechblog.com/data-engineers-of-netflix-interview-with-kevin-wylie-7fb9113a01ea?source=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/7fb9113a01ea</guid>
            <category><![CDATA[data-engineering]]></category>
            <category><![CDATA[culture]]></category>
            <category><![CDATA[big-data]]></category>
            <category><![CDATA[analytics]]></category>
            <category><![CDATA[data]]></category>
            <dc:creator><![CDATA[Netflix Technology Blog]]></dc:creator>
            <pubDate>Thu, 15 Jul 2021 14:15:59 GMT</pubDate>
            <atom:updated>2021-07-15T15:04:12.304Z</atom:updated>
            <content:encoded><![CDATA[<h3>Data Engineers of Netflix — Interview with Kevin Wylie</h3><p>This post is part of our <strong>“Data Engineers of Netflix”</strong> series, where our very own data engineers talk about their journeys to <strong>Data Engineering @ Netflix</strong>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*sAcyqrQkNiqb6HPfiq4WhQ.png" /></figure><p><a href="https://www.linkedin.com/in/kevinwylie/"><strong>Kevin Wylie</strong></a><strong> is a Data Engineer on the Content Data Science and Engineering team.</strong> In this post, Kevin talks about his extensive experience in content analytics at Netflix since joining more than 10 years ago.</p><p>Kevin grew up in the Washington, DC area, and received his undergraduate degree in Mathematics from Virginia Tech. Before joining Netflix, he worked at MySpace, helping implement page categorization, pathing analysis, sessionization, and more. In his free time he enjoys gardening and playing sports with his 4 kids.</p><p><strong>His favorite TV shows:</strong> Ozark, Breaking Bad, Black Mirror, Barry, and Chernobyl</p><p>Since I joined Netflix back in 2011, my favorite project has been designing and building the first version of our entertainment knowledge graph. The knowledge graph enabled us to better understand the trends of movies, TV shows, talent, and books. Building the knowledge graph offered many interesting technical challenges such as entity resolution (e.g., are these two movie names in different languages really the same?), and distributed graph algorithms in Spark. After we launched the product, analysts and scientists began surfacing new insights that were previously hidden behind difficult-to-use data. The combination of overcoming technical hurdles and creating new opportunities for analysis was rewarding.</p><h3>Kevin, what drew you to data engineering?</h3><p>I stumbled into data engineering rather than making an intentional career move into the field. I started my career as an application developer with basic familiarity with SQL. I was later hired into my first purely data gig where I was able to deepen my knowledge of big data. After that, I joined MySpace back at its peak as a data engineer and got my first taste of data warehousing at internet-scale.</p><blockquote><em>What keeps me engaged and enjoying data engineering is giving super-suits and adrenaline shots to analytics engineers and data scientists.</em></blockquote><p>When I make something complex seem simple, or create a clean environment for my stakeholders to explore, research and test, I empower them to do more impactful business-facing work. I like that data engineering isn’t in the limelight, but instead can help create economies of scale for downstream analytics professionals.</p><h3>What drew you to Netflix?</h3><p>My wife came across the Netflix job posting in her effort to keep us in Los Angeles near her twin sister’s family. As a big data engineer, I found that there was an enormous amount of opportunity in the Bay Area, but opportunities were more limited in LA where we were based at the time. So the chance to work at Netflix was exciting because it allowed me to live closer to family, but also provided the kind of data scale that was most common for Bay Area companies.</p><p>The company was intriguing to begin with, but I knew nothing of the talent, culture, or leadership’s vision. I had been a happy subscriber of Netflix’s DVD-rental program (no late fees!) for years.</p><blockquote><strong>After interviewing, it became clear to me that this company culture was different than any I had experienced.</strong></blockquote><p>I was especially intrigued by the trust they put in each employee. Speaking with fellow employees allowed me to get a sense for the kinds of people Netflix hires. The interview panel’s humility, curiosity and business acumen was quite impressive and inspired me to join them.</p><p>I was also excited by the prospect of doing analytics on movies and TV shows, which was something I enjoyed exploring outside of work. It seemed fortuitous that the area of analytics that I’d be working in would align so well with my hobbies and interests!</p><h3>Kevin, you’ve been at Netflix for over 10 years now, which is pretty incredible. Over the course of your time here, how has your role evolved?</h3><p>When I joined Netflix back in 2011, our content analytics team was just 3 people. We had a small office in Los Angeles focused on content, and significantly more employees at the headquarters in Los Gatos. The company was primarily thought of as a tech company.</p><p>At the time, the data engineering team mainly used a data warehouse ETL tool called Ab Initio, and an MPP (Massively Parallel Processing) database for warehousing. Both were appliances located in our own data center. Hadoop was being lightly tested, but only in a few high-scale areas.</p><p>Fast forward 10 years, and Netflix is now the leading streaming entertainment service — serving members in over 190 countries. In the data engineering space, very little of the same technology remains. Our data centers are retired, Hadoop has been replaced by Spark, Ab Initio and our MPP database no longer fits our big data ecosystem.</p><p>In addition to the company and tech shifting, my role has evolved quite a bit as our company has grown. When we were a smaller company, the ability to span multiple functions was valued for agility and speed of delivery. The sooner we could ingest new data and create dashboards and reports for non-technical users to explore and analyze, the sooner we could deliver results. But now, we have a much more mature business, and many more analytics stakeholders that we serve.</p><p>For a few years, I was in a management role, leading a great team of people with diverse backgrounds and skill sets. However, I missed creating data products with my own hands so I wanted to step back into a hands-on engineering role. My boss was gracious enough to let me make this change and focus on impacting the business as an individual contributor.</p><p>As I think about my future at Netflix, what motivates me is largely the same as what I’ve always been passionate about. I want to make the lives of data consumers easier and to enable them to be more impactful. As the company scales and as we continue to invest in storytelling, the opportunity grows for me to influence these decisions through better access to information and insights. The biggest impact I can make as a data engineer is creating economies of scale by producing data products that will serve a diverse set of use cases and stakeholders.</p><blockquote>If I can build beautifully simple data products for analytics engineers, data scientists, and analysts, we can all get better at Netflix’s goal: entertaining the world.</blockquote><h3>Learning more</h3><p>Interested in learning more about data roles at Netflix? You’re in the right place! Keep an eye out for our open roles in Data Science and Engineering by visiting our jobs site <a href="https://jobs.netflix.com/search?team=Data%20Science%20and%20Engineering">here</a>. Our <a href="https://www.instagram.com/wearenetflix/?hl=en">culture</a> is key to our impact and growth: read about it <a href="https://jobs.netflix.com/culture">here</a>. To learn more about our Data Engineers, check out our chats with <a href="https://netflixtechblog.medium.com/data-engineers-of-netflix-interview-with-dhevi-rajendran-a9ab7c7b36e5">Dhevi Rajendran</a> and <a href="https://netflixtechblog.medium.com/data-engineers-of-netflix-interview-with-samuel-setegne-f3027f58c2e2">Samuel Setegne</a>.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=7fb9113a01ea" width="1" height="1" alt=""><hr><p><a href="https://netflixtechblog.com/data-engineers-of-netflix-interview-with-kevin-wylie-7fb9113a01ea">Data Engineers of Netflix — Interview with Kevin Wylie</a> was originally published in <a href="https://netflixtechblog.com">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Exploring Data @ Netflix]]></title>
            <link>https://netflixtechblog.com/exploring-data-netflix-9d87e20072e3?source=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/9d87e20072e3</guid>
            <category><![CDATA[redis]]></category>
            <category><![CDATA[nodejs]]></category>
            <category><![CDATA[dynomite]]></category>
            <category><![CDATA[vuejs]]></category>
            <category><![CDATA[cassandra]]></category>
            <dc:creator><![CDATA[Netflix Technology Blog]]></dc:creator>
            <pubDate>Fri, 25 Jun 2021 11:03:04 GMT</pubDate>
            <atom:updated>2021-06-25T11:03:03.265Z</atom:updated>
            <content:encoded><![CDATA[<p><em>By Gim Mahasintunan on behalf of Data Platform Engineering.</em></p><p>Supporting a rapidly growing base of engineers of varied backgrounds using different data stores can be challenging in any organization. Netflix’s internal teams strive to provide leverage by investing in easy-to-use tooling that streamlines the user experience and incorporates best practices.</p><p>In this blog post, we are thrilled to share that we are open-sourcing one such tool: the Netflix Data Explorer. The Data Explorer gives our engineers fast, safe access to their data stored in Cassandra and Dynomite/Redis data stores.</p><p><a href="https://github.com/Netflix/nf-data-explorer">Netflix Data Explorer on GitHub</a></p><h3>History</h3><p>We began this project several years ago when we were onboarding many new <a href="https://github.com/Netflix/dynomite">Dynomite</a> customers. Dynomite is a high-speed in-memory database, providing highly available cross datacenter replication while preserving Redis-like semantics. We wanted to lower the barrier for adoption so users didn’t need to know datastore-specific CLI commands, could avoid mistakenly running commands that might negatively impact performance, and allow them to access the clusters they frequented every day.</p><p>As the project took off, we saw a similar need for our other datastores. <a href="http://cassandra.apache.org/">Cassandra</a>, our most significant footprint in the fleet, seemed like a great candidate. Users frequently had questions on how they should set up replication, create tables using an appropriate compaction strategy, and craft CQL queries. We knew we could give our users an elevated experience, and at the same time, eliminate many of the common questions on our support channels.</p><p>We’ll explore some of the Data Explorer features, and along the way, we’ll highlight some of the ways we enabled the OSS community while still handling some of the unique Netflix-specific use cases.</p><h3>Multi-Cluster Access</h3><p>By simply directing users to a single web portal for all of their data stores, we can gain a considerable increase in user productivity. Furthermore, in production environments with hundreds of clusters, we can reduce the available data stores to those authorized for access; this can be supported in OSS environments by implementing a Cluster Access Control Provider responsible for fetching ownership information.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/953/0*I4FwiCmDidV95GkL" /><figcaption>Browsing your accessible clusters in different environments and regions</figcaption></figure><h3>Schema Designer</h3><p>Writing CREATE TABLE statements can be an intimidating experience for new Cassandra users. So to help lower the intimidation factor, we built a schema designer that lets users drag and drop their way to a new table.</p><p>The schema designer allows you to create a new table using any primitive or collection data type, then designate your partition key and clustering columns. It also provides tools to view the storage layout on disk; browse the supported sample queries (to help design efficient point queries); guide you through the process of choosing a compaction strategy, and many other advanced settings.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/640/0*uDe0nWGjLXQftTgd" /><figcaption>Dragging and dropping your way to a new Cassandra table</figcaption></figure><h3>Explore Your Data</h3><p>You can quickly execute point queries against your cluster in Explore mode. The Explore mode supports full CRUD of records and allows you to export result sets to CSV or download them as CQL insert statements. The exported CQL can be a handy tool for quickly replicating data from a PROD environment to your TEST environment.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*6ZS1XuvzPsgcirMg" /><figcaption>Explore mode gives you quick access to table data</figcaption></figure><h3>Support for Binary Data</h3><p>Binary data is another popular feature used by many of our engineers. The Data Explorer won’t fetch binary value data by default (as the persisted data might be sizable). Users can opt-in to retrieve these fields with their choice of encoding.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*L-sftGPu3AKPnNcD" /><figcaption>Choosing how you want to decode blob data</figcaption></figure><h3>Query IDE</h3><p>Efficient point queries are available in the Explore mode, but you may have users that still require the flexibility of CQL. Enter the Query mode, which includes a powerful CQL IDE with features like autocomplete and helpful snippets.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/640/0*DwOLCMcZAAl7fmut" /><figcaption>Example of free-form Cassandra queries with autocomplete assistance</figcaption></figure><p>There are also guardrails in place to help prevent users from making mistakes. For instance, we’ll redirect the user to a bespoke workflow for deleting a table if they try to perform a “DROP TABLE…” command ensuring the operation is done safely with additional validation. (See our integration with Metrics later in this article.)</p><p>As you submit queries, they will be saved in the Recent Queries view as well — handy when you are trying to remember that WHERE clause you had crafted before the long weekend.</p><h3>Dynomite and Redis Features</h3><p>While C* is feature-rich and might have a more extensive install base, we have plenty of good stuff for Dynomite and Redis users too. Note, the terms <em>Dynomite</em> and <em>Redis</em> are used interchangeably unless explicitly distinguished.</p><h3>Key Scanning</h3><p>Since Redis is an in-memory data store, we need to avoid operations that inadvertently load all the keys into memory. We perform <a href="https://redis.io/commands/scan">SCAN</a> operations across all nodes in the cluster, ensuring we don’t strain the cluster.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/640/0*A9nkGEIbRuKZe0DF" /><figcaption>Scanning for keys on a Dynomite cluster</figcaption></figure><h3>Dynomite Collection Support</h3><p>In addition to simple String keys, Dynomite supports a rich collection of data types, including Lists, Hashes, and sorted and unsorted Sets. The UI supports creating and manipulating these collection types as well.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*opt6HjvGKdStU6zv" /><figcaption>Editing a Redis hash value</figcaption></figure><h3>Supporting OSS</h3><p>As we were building the Data Explorer, we started getting some strong signals that the ease-of-use and productivity gains that we’d seen internally would benefit folks outside of Netflix as well. We tried to balance codifying some hard-learned best practices that would be generally applicable while maintaining the flexibility to support various OSS environments. To that end, we’ve built several adapter layers into the product where you can provide custom implementations as needed.</p><p>The application was architected to enable OSS by introducing seams where users could provide their implementations for discovery, access control, and data store-specific connection settings. Users can choose one of the built-in service providers or supply a custom provider.</p><p>The diagram below shows the server-side architecture. The server is a Node.js Express application written in TypeScript, and the client is a Single Page App written in <a href="https://vuejs.org/">Vue.js</a>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*kFBD2JKdpgH8fz7T" /><figcaption>Data Explorer architecture and service adapter layers</figcaption></figure><h3>Demo Environment</h3><p>Deploying a new tool in any real-world environment is a time commitment. We get it, and to help you with that initial setup, we have included a dockerized demo environment. It can build the app, pull down images for Cassandra and Redis, and run everything in Docker containers so you can dive right in. Note, the demo environment is not intended for production use.</p><h3>Overridable Configuration</h3><p>The Data Explorer ships with many default behaviors, but since no two production environments are alike, we provide a mechanism to override the defaults and specify your custom values for various settings. These can range from which port numbers to use to which features should be disabled in a production environment. (For example, the ability to drop a Cassandra table.)</p><h3>CLI Setup Tool</h3><p>To further improve the experience of creating your configuration file, we have built a CLI tool that provides a series of prompts for you to follow. The CLI tool is the recommended approach for building your configuration file, and you can re-run the tool at any point to create a new configuration.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*CLLkxf2yrNbr5Jet" /><figcaption>The CLI allows you to create a custom configuration</figcaption></figure><p>You can also generate multiple configuration files and easily switch between them when working with different environments. We have instructions on GitHub on working with more than one configuration file.</p><h3>Service Adapters</h3><p>It’s no secret that Netflix is a big proponent of microservices: we have discovery services for identifying Cassandra and Dynomite clusters in the environment; access-control services that identify who owns a data store and who can access it; and LDAP services to find out information about the logged-in user. There’s a good chance you have similar services in your environment too.</p><p>To help enable such environments, we have several pre-canned configurations with overridable values and adapter layers in place.</p><h4>Discovery</h4><p>The first example of this adapter layer in action is how the application finds <strong>Discovery</strong> information — these are the names and IP addresses of the clusters you want to access. The CLI allows you to choose from a few simple options. For instance, if you have a process that can update a JSON file on disk, you can select “file system.” If instead, you have a REST-based microservice that provides this information, then you can choose “custom” and write a few lines of code necessary to fetch it.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/851/0*tUDhnAUKAwYgpCet" /><figcaption>Choosing to discover our data store clusters by reading a local file</figcaption></figure><h4>Metrics</h4><p>Another example of this service adapter layer is integration with an external metrics service. We progressively enhance the UI by displaying keyspace and table metrics by implementing a metrics service adapter. These metrics provide insight into which tables are being used at a glance and help our customers make an informed decision when dropping a table.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*C9r2aTKHWGTm87U5" /><figcaption>Without metrics support</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*0hvEXejRfhQuscPl" /><figcaption>With optional metrics support</figcaption></figure><p>OSS users can enable the optional <strong>Metrics</strong> support via the CLI. You then just need to write the custom code to fetch the metrics.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/473/0*JCHD7Z0Ew2RRPwic" /><figcaption>CLI enabling customization of advanced features</figcaption></figure><h3>i18n Support</h3><p>While internationalization wasn’t an explicit goal, we discovered that providing Netflix-specific messages in some instances yielded additional value to our internal users. Fundamentally, this is similar to how resource bundles handle different locales.</p><p>We are making en-NFLX.ts available internally and en-US.ts available externally. Enterprise customers can enhance their user’s experience by creating custom resource bundles (en-ACME.ts) that link to other tools or enhance default messages. Only a small percentage of the UI and server-side exceptions use these message bundles currently — most commonly to augment messages somehow (e.g., provide links to internal slack channels).</p><h3>Final Thoughts</h3><p>We invite you to check out the project and let us know how it works for you. By sharing the Netflix Data Explorer with the OSS community, we hope to help you explore your data and inspire some new ideas.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=9d87e20072e3" width="1" height="1" alt=""><hr><p><a href="https://netflixtechblog.com/exploring-data-netflix-9d87e20072e3">Exploring Data @ Netflix</a> was originally published in <a href="https://netflixtechblog.com">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Introducing Netflix Timed Text Authoring Lineage]]></title>
            <link>https://netflixtechblog.com/introducing-netflix-timed-text-authoring-lineage-6fb57b72ad41?source=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/6fb57b72ad41</guid>
            <category><![CDATA[specifications]]></category>
            <category><![CDATA[localization]]></category>
            <category><![CDATA[dubbing]]></category>
            <category><![CDATA[subtitles]]></category>
            <category><![CDATA[timed-text]]></category>
            <dc:creator><![CDATA[Netflix Technology Blog]]></dc:creator>
            <pubDate>Tue, 22 Jun 2021 20:49:31 GMT</pubDate>
            <atom:updated>2021-06-22T20:49:30.836Z</atom:updated>
            <content:encoded><![CDATA[<p><em>A Script Authoring Specification</em></p><p>By: Bhanu Srikanth, Andy Swan, Casey Wilms, Patrick Pearson</p><h3>The Art of Dubbing and Subtitling</h3><p>Dubbing and subtitling are inherently creative processes. At Netflix, we strive to make shows as joyful to watch in every language as in the original language, whether a member watches with original or dubbed audio, closed captions, forced narratives, subtitles or any combination they prefer. Capturing creative vision and nuances in translation is critical to achieving this goal. Creating a dub or a subtitle is a complex, multi-step process that involves:</p><ul><li>Transcribing and timing the dialogue in the original language from a completed show to create a source transcription text</li><li>Notating dialogue events with character information and other annotations</li><li>Generating localization notes to guide further adaptation</li><li>Translating the dialogue to a target language</li><li>Adapting the translation to the dubbing and subtitling specifications; ex. matching the actor’s lip movements in the case of dubs and considering reading speeds and shot changes for subtitles</li></ul><h3>Authoring Scripts</h3><p>Script files are the essence and the driving force in the localization workflow. They carry dialogue, timecodes and other information as they travel from one tool to another to be transcribed, translated, and adapted for performance by voice artists. Dub scripts, Audio Description, Forced Narratives, Closed Captions, and Subtitles all need to be authored in complex tools that manage the timing, location, and formatting of the text on screen.</p><p>Currently, scripts get delivered to Netflix in various ways — Microsoft Word, PDF, Microsoft Excel, Rich Text files, etc., to name a few. These carry crucial information such as dialogues, timecodes, annotations, and other localization contexts. However, the variety of these file formats and inconsistent way of specifying such information across them has made efforts to streamline the localization workflow unattainable in the past.</p><h3>Timed Text Authoring Lineage, an Authoring Specification</h3><p>We decided to remove this stumbling block by developing a new authoring specification called Timed Text Authoring Lineage (TTAL). It enables a seamless exchange of script files between various authoring and prompting tools in the localization pipeline. A TTAL file carries all pertinent information such as type of script, dialogues, timecode, metadata, original language text, transcribed text, language information etc. We have designed TTAL to be robust and extensible to capture all of these details.</p><p>By defining vocabulary and annotations around timed text, we strive to simplify our approach to capturing, storing, and sharing materials across the localization pipeline. The name TTAL is carefully crafted to convey its purpose and usage:</p><ul><li>“Timed Text” in the name means it carries the dialogue along with the corresponding timecode</li><li>“Authoring” underscores that this is used for authoring scripts in dubbing and subtitling</li><li>The “Lineage” part of the name speaks to how the script has evolved from the time the show was produced in one language to the time when it was performed in another language by the voice actors or subtitled in other languages.</li></ul><p><strong>In a nutshell, TTAL has been designed to simplify script authoring, so the creative energy is spent on the art of dubbing and subtitling rather than managing adapted and recorded script delivery.</strong></p><h3>Example TTAL Workflow In Dubbing</h3><p>We have been piloting the authoring and exchange of TTAL scripts as well as the associated workflow with our technology partners and English dubbing partners over the last few months. We receive adapted scripts before recording and again once recording is complete. This workflow, illustrated below, has enabled our dubbing partners to deliver more accurate scripts at crucial moments.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*3vIDjpH9dP9PVd4_ggJblw.png" /></figure><h3>Prompting Tools</h3><p>As an initial step, we worked closely with several dubbing technology providers to incorporate TTAL into their product using JSON as the underlying format. We appreciate the efforts put forth by the developers of these products for test driving TTAL and giving us crucial feedback to improve it.</p><p>Third-party tools that support import and export of scripts in TTAL are:</p><ul><li><a href="https://www.voiceq.com/releases">VoiceQ </a>version 4.7.2 &amp; above</li><li><a href="https://www.noblurway.com/en/">Mosaic 2.0</a></li><li><a href="https://non-lethal-applications.com/products/adr-master-2/adr-master-2-editor">ADR Master 2</a></li><li><a href="https://new.steinberg.net/nuendo/">Nuendo</a>, a <a href="https://pta.netflixstudios.com">Netflix Production Technology Alliance</a> partner product, is currently being updated to include support for TTAL.</li></ul><h3>Servicing The Ultimate Goal</h3><p>Having tools in the localization pipeline adopt TTAL as a unified way to exchange scripts will be beneficial to all players in the ecosystem in more ways than one. It will improve the capture of consistently structured dub scripts giving us the ability to better parse and leverage the contents of scripts, pave the path for streamlining the workflow, and enable interoperability between tools in the localization pipeline. Ultimately, all these will serve Netflix’s unwavering goal of fulfilling and maintaining the creative vision throughout the localization process.</p><h3>Moving Forward</h3><p>This is just the beginning. We have laid a solid foundation for enabling interoperability by developing a specification for script authoring. We have worked with a few dubbing technology developers to incorporate TTAL into their products, and have modified the specification based on feedback from these early adopters. In addition, we have piloted the workflow with our English dubbing partners.</p><p>These efforts have proven that Timed Text Authoring Lineage fills a crucial gap and benefits the entire localization ecosystem, from individual transcribers and script authors, dubbing and subtitling service providers, to technology developers and content creators. We are confident that enabling tools to exchange scripts seamlessly will remove operational headaches and make additional time and effort available for the art of transcribing, translation and adaptation of subtitles and dubs.</p><p>Finally, TTAL is an evolving specification. As the adoption of TTAL continues, we expect to learn more and improve the specifications. We are committed to continued collaboration with our localization partners and tool developers to mature this further. If you are interested in incorporating TTAL in the tools you are developing, please reach out to us at <a href="mailto:TTAL_Format@netflix.com">TTAL_Format@netflix.com</a> to learn more about this exciting new specification and explore how you can use TTAL in your workflows. Please check out <a href="https://www.youtube.com/watch?v=4epj7mBRrBo">this video</a> to learn how TTAL exports work in VoiceQ, one of the first prompting tools to incorporate TTAL.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=6fb57b72ad41" width="1" height="1" alt=""><hr><p><a href="https://netflixtechblog.com/introducing-netflix-timed-text-authoring-lineage-6fb57b72ad41">Introducing Netflix Timed Text Authoring Lineage</a> was originally published in <a href="https://netflixtechblog.com">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How Netflix uses eBPF flow logs at scale for network insight]]></title>
            <link>https://netflixtechblog.com/how-netflix-uses-ebpf-flow-logs-at-scale-for-network-insight-e3ea997dca96?source=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/e3ea997dca96</guid>
            <category><![CDATA[ebpf]]></category>
            <category><![CDATA[cloud-networking]]></category>
            <category><![CDATA[network-analytics]]></category>
            <category><![CDATA[druid]]></category>
            <category><![CDATA[big-data]]></category>
            <dc:creator><![CDATA[Netflix Technology Blog]]></dc:creator>
            <pubDate>Mon, 07 Jun 2021 19:12:44 GMT</pubDate>
            <atom:updated>2021-06-07T15:52:22.303Z</atom:updated>
            <content:encoded><![CDATA[<p>By <a href="https://www.linkedin.com/in/alok-tiagi-99205015/">Alok Tiagi</a>, <a href="https://www.linkedin.com/in/haananth/">Hariharan Ananthakrishnan</a>, <a href="https://www.linkedin.com/in/casualjim/">Ivan Porto Carrero</a> and <a href="https://www.linkedin.com/in/joshmachine/">Keerti Lakshminarayan</a></p><p>Netflix has developed a network observability sidecar called <strong>Flow Exporter</strong> that uses eBPF tracepoints to capture TCP flows at near real time. At much less than 1% of CPU and memory on the instance, this highly performant sidecar provides flow data at scale for network insight.</p><h3>Challenges</h3><p>The cloud network infrastructure that Netflix utilizes today consists of AWS services such as VPC, DirectConnect, VPC Peering, Transit Gateways, NAT Gateways, etc and Netflix owned devices. Netflix software infrastructure is a large distributed ecosystem that consists of specialized functional tiers that are operated on the AWS and Netflix owned services. While we strive to keep the ecosystem simple, the inherent nature of leveraging a variety of technologies will lead us to challenges such as:</p><ul><li><strong>App Dependencies and Data Flow Mappings: </strong>With the number of micro services growing by the day without understanding and having visibility into an application’s dependencies and data flows, it is difficult for both service owners and centralized teams to identify systemic issues.</li><li><strong>Pathway Validation: </strong>Netflix velocity of change within the production streaming and studio environment can result in the inability of services to communicate with other resources.</li><li><strong>Service Segmentation: </strong>The ease of the cloud deployments has led to the organic growth of multiple AWS accounts, deployment practices, interconnection practices, etc. Without having network visibility, it’s difficult to improve our reliability, security and capacity posture.</li><li><strong>Network Availability: </strong>The expected continued growth of our ecosystem makes it difficult to understand our network bottlenecks and potential limits we may be reaching.</li></ul><p><strong>Cloud Network Insight </strong>is a suite of solutions that provides both operational and analytical insight into the cloud network infrastructure to address the identified problems. By collecting, accessing and analyzing network data from a variety of sources like <a href="https://netflixtechblog.com/hyper-scale-vpc-flow-logs-enrichment-to-provide-network-insight-e5f1db02910d">VPC Flow Logs</a>, ELB Access Logs, eBPF flow logs on the instances, etc, we can provide network insight to users and central teams through multiple data visualization techniques like <a href="https://netflixtechblog.com/lumen-custom-self-service-dashboarding-for-netflix-8c56b541548c">Lumen</a>, <a href="https://github.com/Netflix/atlas">Atlas</a>, etc.</p><h3>Flow Exporter</h3><p>The Flow Exporter is a sidecar that uses <a href="http://www.brendangregg.com/blog/2018-03-22/tcp-tracepoints.html">eBPF tracepoints</a> to capture TCP flows at near real time on instances that power the Netflix microservices architecture.</p><h4><a href="http://www.brendangregg.com/bpf-performance-tools-book.html">What is BPF?</a></h4><blockquote><em>Berkeley Packet Filter (BPF) is an in-kernel execution engine that processes a virtual instruction set, and has been extended as eBPF for providing a safe way to extend kernel functionality. In some ways, eBPF does to the kernel what JavaScript does to websites: it allows all sorts of new applications to be created.</em></blockquote><p>An eBPF flow log record represents one or more network flows that contain TCP/IP statistics that occur within a variable aggregation interval.</p><p>The sidecar has been implemented by leveraging the highly performant eBPF along with carefully chosen transport protocols to consume <strong>less than 1% of CPU and memory </strong>on any instance in our fleet. The choice of transport protocols like GRPC, HTTPS &amp; UDP is runtime dependent on characteristics of the instance placement.</p><p>The runtime behavior of the Flow Exporter can be dynamically managed by configuration changes via <a href="https://netflixtechblog.com/announcing-archaius-dynamic-properties-in-the-cloud-bc8c51faf675">Fast Properties</a>. The Flow Exporter also publishes various operational metrics to <a href="https://github.com/Netflix/atlas">Atlas</a>. These metrics are visualized using <a href="https://netflixtechblog.com/lumen-custom-self-service-dashboarding-for-netflix-8c56b541548c">Lumen</a>, a self-service dashboarding infrastructure.</p><h3>So how do we ingest and enrich these flows at scale ?</h3><p>Flow Collector is a regional service that ingests and enriches flows. IP addresses within the cloud can move from one EC2 instance or <a href="https://netflix.github.io/titus/">Titus</a> container to another over time. We use <a href="https://www.slideshare.net/AmazonWebServices/a-day-in-the-life-of-a-cloud-network-engineer-at-netflix-net303-reinvent-2017">Sonar</a> to attribute an IP address to a specific application at a particular time. Sonar is an IPv6 and IPv4 address identity tracking service.</p><p>Flow Collector consumes two data streams, the IP address change events from Sonar via Kafka and eBPF flow log data from the Flow Exporter sidecars. It performs real time attribution of flow data with application metadata from Sonar. The attributed flows are pushed to <a href="https://netflixtechblog.com/keystone-real-time-stream-processing-platform-a3ee651812a">Keystone</a> that routes them to the Hive and <a href="https://netflixtechblog.com/how-netflix-uses-druid-for-real-time-insights-to-ensure-a-high-quality-experience-19e1e8568d06">Druid</a> datastores.</p><p>The attributed flow data drives various use cases within Netflix like network monitoring and network usage forecasting available via <a href="https://netflixtechblog.com/lumen-custom-self-service-dashboarding-for-netflix-8c56b541548c">Lumen</a> dashboards and <a href="https://netflixtechblog.com/open-sourcing-metaflow-a-human-centric-framework-for-data-science-fa72e04a5d9">machine learning</a> based network segmentation. The data is also used by security and other partner teams for insight and incident analysis.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*cw0wlZ241Tg71jMQ" /></figure><h3>Summary</h3><p>Providing network insight into the cloud network infrastructure using eBPF flow logs at scale is made possible with eBPF and a highly scalable and efficient flow collection pipeline. After several iterations of the architecture and some tuning, the solution has proven to be able to scale.</p><p>We are currently ingesting and enriching billions of eBPF flow logs per hour and providing visibility into our cloud ecosystem. The enriched data allows us to analyze networks across a variety of dimensions (e.g. availability, performance, and security), to ensure applications can effectively deliver their data payload across a globally dispersed cloud-based ecosystem.</p><h3>Special Thanks To</h3><p><a href="https://www.linkedin.com/in/brendangregg/">Brendan Gregg</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=e3ea997dca96" width="1" height="1" alt=""><hr><p><a href="https://netflixtechblog.com/how-netflix-uses-ebpf-flow-logs-at-scale-for-network-insight-e3ea997dca96">How Netflix uses eBPF flow logs at scale for network insight</a> was originally published in <a href="https://netflixtechblog.com">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Data Engineers of Netflix — Interview with Dhevi Rajendran]]></title>
            <link>https://netflixtechblog.com/data-engineers-of-netflix-interview-with-dhevi-rajendran-a9ab7c7b36e5?source=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/a9ab7c7b36e5</guid>
            <category><![CDATA[culture]]></category>
            <category><![CDATA[data]]></category>
            <category><![CDATA[data-engineering]]></category>
            <category><![CDATA[remote-working]]></category>
            <category><![CDATA[big-data]]></category>
            <dc:creator><![CDATA[Netflix Technology Blog]]></dc:creator>
            <pubDate>Wed, 02 Jun 2021 01:17:13 GMT</pubDate>
            <atom:updated>2021-03-15T18:17:12.746Z</atom:updated>
            <content:encoded><![CDATA[<h3>Data Engineers of Netflix — Interview with Dhevi Rajendran</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/512/0*K7HCBz0U03VAywKr" /><figcaption>Dhevi Rajendran</figcaption></figure><p>This post is part of our <strong>“Data Engineers of Netflix”</strong> interview series, where our very own data engineers talk about their journeys to <strong>Data Engineering @ Netflix</strong>.</p><p><a href="https://www.linkedin.com/in/dhevi-rajendran-7b736b29/"><strong>Dhevi Rajendran</strong></a><strong> is a Data Engineer on the Growth Data Science and Engineering team.</strong> Dhevi joined Netflix in July 2020 and is one of many Data Engineers who have onboarded remotely during the pandemic. In this post, Dhevi talks about her passion for data engineering and taking on a new role during the pandemic.</p><p>Before Netflix, Dhevi was a software engineer at Two Sigma, where she was most recently on a data engineering team responsible for bringing in datasets from a variety of different sources for research and trading purposes. In her free time, she enjoys drawing, doing puzzles, reading, writing, traveling, cooking, and learning new things.</p><p><strong>Her favorite TV shows:</strong> Atlanta, Barry, Better Call Saul, Breaking Bad, Dark, Fargo, Succession, The Killing</p><p><strong>Her favorite movies:</strong> Das Leben der Anderen, Good Will Hunting, Intouchables, Mother, Spirited Away, The Dark Knight, The Truman Show, Up</p><h3>Dhevi, so what got you into data engineering?</h3><p>While my background has mostly been in backend software engineering, I was most recently doing backend work in the data space prior to Netflix. One great thing about working with data is the impact you can create as an engineer.</p><blockquote><strong>At Netflix, the work that data engineers do to produce data in a robust, scalable way is incredibly important to provide the best experience to our members as they interact with our service.</strong></blockquote><p>Beyond the really interesting technical challenges that come with working with big data, there are lots of opportunities to think about higher-level domain challenges as a data engineer. In college, I had done human-computer interaction research on subtitles for the Deaf and hard-of-hearing as well as computational genomics research on Alzheimer’s disease. <strong>I’ve always enjoyed learning about new areas and combining this knowledge with my technical skills to solve real-world problems.</strong></p><h3>What drew you to Netflix?</h3><p>Netflix’s mission and its culture primarily drew me to Netflix. I liked the idea of being a part of a company that brings joy to so many members around the world with an incredibly powerful platform for their stories to be heard. <strong>The blend of creativity and a strong engineering culture at Netflix really appealed to me.</strong></p><p>The culture was also something that piqued my interest. I was pretty skeptical of Netflix’s culture memo at first. Many companies have lofty ideals that don’t necessarily translate into the reality of the company culture, so I was surprised to see how consistently the culture memo aligns with the actual culture at the company. I’ve found the culture of freedom and responsibility empowering.</p><blockquote><strong>Rather than the typical top-down approach many companies use, Netflix trusts each person to make the right decisions for the company by using their deep knowledge of the problems they’re solving along with the context they gather from their leaders and stakeholders.</strong></blockquote><p>This means a lot less red tape, a lot less friction, and a lot more freedom for everyone at the company to do what’s best for the business. I also really appreciate the amount of visibility and input we get into broader strategic decisions that the company makes.</p><p>Finally, I was also really excited about joining the Growth Data Engineering team! My team is responsible for building data products relating to how we connect with our new members around the world, which is high-impact and has far-reaching global significance. I love that I get to help Netflix connect with new members around the world and help shape the first impression we make on them.</p><h3>What is your favorite project or a project that you’re particularly proud of?</h3><p>I have been primarily involved in the payments space. Not a project per se, but one of the things I’ve enjoyed being involved in is the cross-functional meetings with peers and stakeholders who are working in the payments space. These meetings include product managers, designers, consumer insights researchers, software engineers, data scientists, and people in a wide variety of other roles.</p><blockquote><strong>I love that I get to work cross-functionally with such a diverse group of people looking at the same set of problems from a variety of unique perspectives.</strong></blockquote><p>In addition to my day-to-day technical work, these meetings have provided me with the opportunity to be involved in the high-level product, design, and strategic discussions, which I value. Through these cross-functional efforts, I’ve also really gotten to learn and appreciate the nuances of payments. From using credit cards (which are fairly common in the US but not as widely adopted outside the US) to physically paying in person, members in different countries prefer to pay for our subscription in a wide variety of ways. It’s incredible to see the thoughtful and deeply member-driven approach we use to think about something as seemingly routine, straightforward, and often taken for granted as payments.</p><h3>What was it like taking on a new role during the pandemic?</h3><p>First off, I feel very lucky to have found a new role in this very difficult period. With the amount of change and uncertainty, the past year brought, it somehow felt both fitting and imprudent to voluntarily add a career change to the mix. The prospect was daunting at first. I knew there would be a bunch for me to learn coming into Netflix, considering that I hadn’t worked with the technologies my team uses (primarily Scala and Spark). Looking back now, I’m incredibly grateful for the opportunity and glad that I took it. I’ve already learned so much in the past six months and am excited about how much more I can learn and the impact I can make going forward.</p><p>Onboarding remotely has been a unique experience as well. Building relationships and gathering broader context are more difficult right now. I’ve found that I’ve learned to be more proactive and actively seek out opportunities to get to know people and the business, whether through setting up coffee chats, reading memos, or attending meetings covering topics I want to learn more about. I still haven’t met anyone I work with in person, but my teammates, my manager, and people across the company have been really helpful throughout the onboarding process.</p><blockquote><strong>It’s been incredible to see how gracious people are with their time and knowledge. The amount of empathy and understanding people have shown to each other, including to those who are new to the company, has made taking the leap and joining Netflix a positive experience.</strong></blockquote><h3>Learning more</h3><p>Interested in learning more about data roles at Netflix? You’re in the right place! Keep an eye out for our open roles in Data Science and Engineering <a href="https://jobs.netflix.com/search?team=Data%20Science%20and%20Engineering">here</a>. Our culture is key to our impact and growth: read about it <a href="https://jobs.netflix.com/culture">here</a>.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=a9ab7c7b36e5" width="1" height="1" alt=""><hr><p><a href="https://netflixtechblog.com/data-engineers-of-netflix-interview-with-dhevi-rajendran-a9ab7c7b36e5">Data Engineers of Netflix — Interview with Dhevi Rajendran</a> was originally published in <a href="https://netflixtechblog.com">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Data Engineers of Netflix — Interview with Samuel Setegne]]></title>
            <link>https://netflixtechblog.com/data-engineers-of-netflix-interview-with-samuel-setegne-f3027f58c2e2?source=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/f3027f58c2e2</guid>
            <category><![CDATA[data-tools]]></category>
            <category><![CDATA[big-data]]></category>
            <category><![CDATA[developer-productivity]]></category>
            <category><![CDATA[data-engineering]]></category>
            <category><![CDATA[culture]]></category>
            <dc:creator><![CDATA[Netflix Technology Blog]]></dc:creator>
            <pubDate>Wed, 02 Jun 2021 01:16:43 GMT</pubDate>
            <atom:updated>2021-04-26T20:53:39.358Z</atom:updated>
            <content:encoded><![CDATA[<h3>Data Engineers of Netflix — Interview with Samuel Setegne</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Z9YqHqPYhXc1bnyOTfcvjg.jpeg" /><figcaption>Samuel Setegne</figcaption></figure><p>This post is part of our <strong>“Data Engineers of Netflix”</strong> interview series, where our very own data engineers talk about their journeys to <strong>Data Engineering @ Netflix</strong>.</p><p><a href="https://www.linkedin.com/in/samuel-setegne-905448122/"><strong>Samuel Setegne</strong></a><strong> is a Senior Software Engineer on the Core Data Science and Engineering team. Samuel and his team build tools and frameworks that support data engineering teams across Netflix. In this post, Samuel talks about his journey from being a clinical researcher to supporting data engineering teams.</strong></p><p>Samuel comes from West Philadelphia, and he received his Master’s in Biotechnology from Temple University. Before Netflix, Samuel worked at Travelers Insurance in the Data Science &amp; Engineering space, implementing real-time machine learning models to predict severity and complexity at the onset of property claims.</p><p><strong>His favorite TV shows:</strong> <a href="https://www.youtube.com/watch?v=ZOGxOQxXjdo">Bojack Horseman</a>, <a href="https://www.youtube.com/watch?v=OXfgvcJ5T8E">Marco Polo</a>, and <a href="https://www.youtube.com/watch?v=ndl1W4ltcmg&amp;t=6s">The Witcher</a></p><p><strong>His favorite movies:</strong> Scarface, I Am Legend and The Old Guard</p><h3>Sam, what drew you to data engineering?</h3><p>Early in my career, I was headed full speed towards life as a clinical researcher. Many healthcare practitioners had strong hunches and wild theories that were exciting to test against an empirical study. I personally loved looking at raw data and using it to understand patterns in the world through technology. However, most challenges that came with my role were domain-related but not as technically demanding. For example — clinical data was often small enough to fit into memory on an average computer and only in rare cases would its computation require any technical ingenuity or massive computing power. There was not enough scope to explore the distributed and large-scale computing challenges that usually come with big data processing. Furthermore, engineering velocity was often sacrificed owing to rigid processes.</p><blockquote>Moving into pure Data engineering not only offered me the technical challenges I’ve always craved for but also the opportunity to connect the dots through data which was the best of both worlds.</blockquote><h3>What is your favorite project or a project you’re particularly proud of?</h3><p>The very first project I had the opportunity to work on as a Netflix contractor was migrating all of Data Science and Engineering’s <a href="https://netflixtechblog.com/python-at-netflix-bba45dae649e">Python 2 code to Python 3</a>. This was without a doubt, my favorite project that also opened the door for me to join the organization as a full-time employee. It was thrilling to analyze code from various cross-functional teams and learn different coding patterns and styles.</p><blockquote><strong>This kind of exposure opened up opportunities for me to engage with various data engineering teams and advocate for python best practices that helped me drive greater impact at Netflix.</strong></blockquote><h3>What drew you to Netflix?</h3><p>What initially caught my attention about a chance to work at Netflix was the variety and quality of content. My family and friends were always ecstatic about having lively and raucous conversations about Netflix shows or movies they recently watched like Marco Polo and Tiger King.</p><blockquote><strong>Although other great companies play a role in our daily lives, many of them serve as a kind of utility, whereas Netflix is meant to make us live, laugh, and love by enabling us to experience new voices, cultures, and perspectives</strong>.</blockquote><p>After I read Netflix’s culture memo, I was completely sold. It precisely described what I always knew was missing in places I’ve worked before. I found the mantra of <strong>“people over process”</strong> extremely refreshing and eventually learned that it unlocked a bold and creative part of me in my technical designs. For instance, if I feel that a design of an application or a pipeline would benefit from new technology or architecture, I have the freedom to explore and innovate without excessive red tape. Typically in large corporations, you’re tied to strict and redundant processes, causing a lot of fatigue for engineers. <strong>When I landed at Netflix, it was a breath of fresh air to learn that we lean into freedom and responsibility and allow engineers to push the boundaries.</strong></p><h3>Sam, how do you approach building tools/frameworks that can be used across data engineering teams?</h3><p><strong>My team provides generalized solutions for common and repetitive data engineering tasks. This helps provide “paved path” solutions for data engineering teams and reduces the burden of re-inventing the wheel</strong>. When you have many specialized teams composed of highly skilled engineers, the last thing you want for a data engineer is to spend too much time solving small problems that are usually buried inside of the big, broad, and impactful problems. When we extrapolate that to every engineer on every Data Science &amp; Engineering team, it easily adds up and is something worth optimizing.</p><blockquote>Any time you have a data engineer spending cycles working on tasks where the data engineering part of their brain is turned off, that’s an opportunity where better tooling can help.</blockquote><p>For example, many data engineering teams have to orchestrate notification campaigns when they make changes to critical tables that have downstream dependencies. This is achievable by a Data Engineer but it can be very time-consuming, especially having to track the migration of these downstream users over to your new table or table schema to ensure it’s safe to finalize your changes. This problem was tackled by one of my highly skilled team members who built a centralized migration service that lets Data Engineers easily start “migration campaigns” that can automatically identify downstream users and provide notification and status-tracking capabilities by leveraging Jira. The aim is to enable Data Engineers to quickly fire up one of these campaigns and keep an eye out for its completion while using that extra time to focus on other tasks.</p><blockquote>By investing in the right tooling to streamline redundant (yet necessary) tasks, we can drive higher data engineering productivity and efficiency, while accelerating innovation for Netflix.</blockquote><h3>Learning more</h3><p>Interested in learning more about data roles at Netflix? You’re in the right place! <strong>Keep an eye out for our open roles in Data Science and Engineering by visiting our jobs site </strong><a href="https://jobs.netflix.com/search?team=Data%20Science%20and%20Engineering"><strong>here</strong></a><strong>.</strong> Our <a href="https://www.instagram.com/wearenetflix/?hl=en">culture</a> is key to our impact and growth: read about it <a href="https://jobs.netflix.com/culture">here</a>. Check out our chat with Dhevi Rajendran to know more about starting a new role as a Data Engineer during the pandemic <a href="https://netflixtechblog.medium.com/data-engineers-of-netflix-interview-with-dhevi-rajendran-a9ab7c7b36e5">here</a>.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=f3027f58c2e2" width="1" height="1" alt=""><hr><p><a href="https://netflixtechblog.com/data-engineers-of-netflix-interview-with-samuel-setegne-f3027f58c2e2">Data Engineers of Netflix — Interview with Samuel Setegne</a> was originally published in <a href="https://netflixtechblog.com">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[My (Seemingly) Random Walk to Netflix]]></title>
            <link>https://netflixtechblog.com/my-seemingly-random-walk-to-netflix-293d952953fa?source=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/293d952953fa</guid>
            <category><![CDATA[data-science]]></category>
            <category><![CDATA[career-paths]]></category>
            <category><![CDATA[netfli]]></category>
            <category><![CDATA[analytics]]></category>
            <category><![CDATA[academia]]></category>
            <dc:creator><![CDATA[Netflix Technology Blog]]></dc:creator>
            <pubDate>Fri, 28 May 2021 19:55:51 GMT</pubDate>
            <atom:updated>2021-05-28T19:55:51.257Z</atom:updated>
            <content:encoded><![CDATA[<h4>Part of our series on who works in Analytics at Netflix — and what the role entails</h4><p><em>By Sean Barnes, Studio Production Data Science &amp; Engineering</em></p><p>I am going to tell you a story about a person that works for Netflix. That person grew up dreaming of working in the entertainment industry. They attended the University of Southern California, double majored in data science and television &amp; film production, and graduated summa cum laude. Upon graduation, they received an offer from Netflix to become an analytics engineer, and pursue their lifelong dream of orchestrating the beautiful synergy of analytics and entertainment. Pretty straightforward, right?!</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/480/0*jcg5Yc15KKF8LHMa" /></figure><p>Such a linear trajectory would make for a compelling candidate, but in reality, many of us encounter a few twists and turns along the way. I am here to tell you that these twists and turns are OK, and in many cases, they make you better off in the long run. Whether they worked at a <a href="https://netflixtechblog.com/mythbusting-the-analytics-journey-58d692ea707e">manufacturer for very large industrial ventilation systems</a>, or in finance, healthcare, or elsewhere in tech (big or small), most people on my team have unique paths to their current positions at Netflix. I am going to tell you my story, but I will also tell you about how bringing together people with diverse backgrounds can have unexpected benefits.</p><p>When I was growing up, I developed a strong interest in the space program. I went to space camp (nerd alert!), loved space movies (still do!), loved all things astronomy (still do!), and even recall watching a launch or two at school (yes, on those roll-out TV carts). Like any rational person, I set out on a course to pursue a career that would either put me in space or help to put others up there. I decided to attend the Georgia Institute of Technology (Go Jackets!!) and to major in aerospace engineering. I would eventually enroll in the combined BS/MS program, committing to aerospace long-term and to participating in undergraduate and graduate research. In parallel, I also began working as an intern for the U.S. Federal Government as an engineering analyst, which eventually converted into a full-time position. Along the way, I discovered three things that would have a significant impact on my future trajectory:</p><ol><li><strong>No lab for me</strong>: I did not like being in a lab, and I did not like the idea of spending a ton of time trying to improve the efficiency of some engineering part/system.</li><li><strong>Searching for (and not finding) a specialty: </strong>There was not an aerospace engineering discipline that I was really interested in, and trust me, I really tried because I didn’t want to deviate from my linear career trajectory. Structures, dynamics, control systems, fluids, design…pass, pass, pass, pass, and pass!</li><li><strong>Programming joy</strong>: I discovered an aptitude and joy for programming, and in particular, I really liked developing simulation models that could provide meaningful insights and support decision-making without actually building anything or conducting a real-life experiment.</li></ol><p>Given these signals, I made the decision to pivot on my initial plan to work for NASA and designed a new plan more in line with my growing interests. That plan consisted of modifying my MS curriculum to support my newly found enthusiasm for simulation modeling, and transitioning to the Applied Mathematics and Scientific Computation doctoral program at the University of Maryland, College Park. This program was perfect for my interests, and allowed me to develop the interdisciplinary mathematical and computation skills that I have been using ever since. I connected with two advisors who were beginning to explore use cases for operations research in healthcare, which was the perfect opportunity to put my interdisciplinary training to work on meaningful real-world applications. I wrote my dissertation on simulation modeling of infectious disease transmission in healthcare facilities and community populations.</p><blockquote><strong>BOOM, I finally figured out what I was supposed to be doing. End of story, right?!</strong></blockquote><figure><img alt="" src="https://cdn-images-1.medium.com/max/480/0*M0DvEglZhz1a5uR0" /></figure><p>Almost! Hang with me just a smidge longer. After defending my dissertation, I left my position with the U.S. Federal Government to become a tenure-track faculty in the Robert H. Smith School of Business at the University of Maryland, College Park. Yep, I stayed close to home, and worked there for 7 years. I grew a lot during this experience, and really enjoyed working with students and research collaborators. This is also the key period when most of my data science growth occurred, as I was developing my healthcare analytics research program and teaching analytics courses to MS and undergraduate students. Throughout this process, I developed skills in Python programming, data visualization, statistical analysis, machine learning, and optimization, both by doing and by teaching. However, in 2019, I explored several data science opportunities in the tech industry, and I was completely won over by the opportunity to join the <a href="https://netflixtechblog.com/studio-production-data-science-646ee2cc21a1">Studio Production Data Science &amp; Engineering team at Netflix</a>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*hWK2Vl9a4dteSvQNhbTtLQ.png" /></figure><p>There is a mathematical concept called a <a href="https://en.wikipedia.org/wiki/Random_walk"><em>random walk</em></a>, which is essentially a path that is generated via a sequence of (seemingly) random steps. Those steps can be generated in any number of ways (e.g., by flipping a coin, observing changes in the stock market, or using a computer-generated sequence of random numbers), and there are numerous ways to adapt this concept to different applications (e.g., computer science, physics, finance, economics, and more). My (seemingly) random walk to Netflix looks a little something like this:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*GSxc2WZDJJ0gk7XqU22Dgg.png" /><figcaption>Acknowledgment to Ritchie King for graphic design</figcaption></figure><p>Why is my walk only <em>seemingly</em> random? These steps may appear to be random, but what I now realize is that there are some common themes in my experience that align well with core components of Netflix culture. For instance, I am passionate about using data and models to inform decision-making, whether the application is in aerospace, healthcare, or entertainment. I really enjoy building relationships and collaborating with others. I also enjoy bringing analytics and modeling into new spaces for which these practices are relatively new, such as in healthcare and entertainment. Lastly, I’m a learner and an educator, so I love learning new things and helping others learn as well.</p><p>The next observation is also a newly gained perspective. I have recently been reading the book <a href="https://algorithmstoliveby.com/"><em>Algorithms to Live By</em></a>, written by Brian Christian and Tom Griffiths. In the second chapter of the book, the authors describe how the algorithmic tradeoff between exploration and exploitation plays out in real life. Exploration means to seek out new options so that you can learn more about the possibilities, whereas exploitation means to focus on the best option(s) that you have discovered thus far. They provide examples of this tradeoff within the context of how one evaluates which restaurants to visit or which candidate to hire. A lot of my experiences before coming to Netflix were part of my exploration phase, which I now realize is totally OK. I believe this exploration is what is needed to find what truly brings joy, and also eliminate things that do not. And now, I have entered the exploitation phase of my career, where I am fully committed to bringing data science into interdisciplinary spaces.</p><p>OK, I know, it’s time to wrap this up.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/467/0*tv7zHpwacV2-GV9E" /></figure><p>Let me conclude by sharing a quick story about the unexpected benefits of hiring an infectious disease modeler to help accelerate the use of analytics in studio production. According to the U.S. Centers for Disease Control &amp; Prevention, the first known case of COVID-19 was identified in December 2019, which was less than 6 months after my first day at Netflix. By March 2020 — less than 9 months into my tenure — cases of the virus were prevalent across the U.S. and the nation was beginning to shut down.</p><p>At studios across Hollywood, production was halted while executives and frontline workers alike scrambled to learn what they could about the virus and the risks associated with restarting production. Given my background, I emailed the vice president of my group (who hired me), and offered to help in any way that I could. He forwarded my email <em>directly to our CFO</em> [1], which initiated a series of events that included the establishment of a medical advisory board [2], development of a simulation model and risk-scoring framework to help support decisions regarding our safe return to production [3], close collaboration with a truly amazing set of individuals and teams across the company, and even a <a href="https://www.hollywoodreporter.com/news/this-is-how-were-going-to-be-making-movies-at-least-for-another-year-or-two-netflix-execs-talk-filming-amid-the-pandemic">feature article in The Hollywood Reporter</a>. Most of this work continues to this day, as we hopefully approach better times ahead. I never could have imagined such a sequence of events when I first arrived in Los Angeles.</p><p>So for those of you out there who feel like you’re on a (seemingly) random walk…<strong>YOU ARE NOT ALONE!</strong> Many of us have to do the exploration before we find something that we’re willing to exploit over the long-term, and that process does not always follow <a href="https://www.linkedin.com/posts/linkedin_remember-career-paths-are-different-for-activity-6785229605147025408-z8li">the linear trajectory that we imagine</a> when we are taking the first steps away from our origins. Try to find the common themes and skills that you have developed across your diverse experiences, and craft that story for potential employers.</p><p>And to the potential employers out there, <strong>TAKE SOME RISKS!</strong> Think more deeply about what the ‘non-traditional’ candidate may bring to your organization. You never know, some circumstances may arise for which those (seemingly) less-relevant skills and experiences may become more useful than you imagined. By doing so, you’ll be facilitating exploration <em>as an</em> <em>organization</em>, and learning about how to build teams that are truly innovative. So together, employers and employees alike, let’s take our (seemingly) random walks, and explore the possibilities until we find those pockets in space where we can exploit the opportunities and accomplish our greatest goals.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/395/1*C9gRZWX6vL2LDNTLV8W7Rg.jpeg" /><figcaption>Me (several years ago)</figcaption></figure><h4>Footnotes</h4><ol><li>Which, by the way, is a <em>very</em> Netflix thing to do</li><li>Featuring one of my long-time infectious disease research collaborators and mentors</li><li>Embarrassingly named the Barnes Model and the Barnes Scale, respectively, by one of my stunning colleagues</li></ol><p><em>If this post resonates with you and you’d like to explore opportunities with Netflix, check out our </em><a href="http://research.netflix.com/research-area/analytics"><em>analytics site</em></a><em>, search </em><a href="http://jobs.netflix.com/search?team=Data%20Science%20and%20Engineering"><em>open roles</em></a><em>, and learn about our </em><a href="http://jobs.netflix.com/culture"><em>culture</em></a><em>. You can also find more stories like this </em><a href="https://netflixtechblog.com/analytics-at-netflix-who-we-are-and-what-we-do-7d9c08fe6965"><em>here</em></a><em>.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=293d952953fa" width="1" height="1" alt=""><hr><p><a href="https://netflixtechblog.com/my-seemingly-random-walk-to-netflix-293d952953fa">My (Seemingly) Random Walk to Netflix</a> was originally published in <a href="https://netflixtechblog.com">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Achieving observability in async workflows]]></title>
            <link>https://netflixtechblog.com/achieving-observability-in-async-workflows-cd89b923c784?source=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/cd89b923c784</guid>
            <category><![CDATA[netflix-studio]]></category>
            <category><![CDATA[workflow-management]]></category>
            <category><![CDATA[asynchronous]]></category>
            <category><![CDATA[tracing]]></category>
            <category><![CDATA[software-engineering]]></category>
            <dc:creator><![CDATA[Netflix Technology Blog]]></dc:creator>
            <pubDate>Fri, 14 May 2021 17:43:31 GMT</pubDate>
            <atom:updated>2021-05-12T15:55:36.411Z</atom:updated>
            <content:encoded><![CDATA[<p><em>Written by </em><a href="https://www.linkedin.com/in/colbycallahan"><em>Colby Callahan</em></a><em>, </em><a href="https://www.linkedin.com/in/megha-manohara-ba71b09/"><em>Megha Manohara</em></a><em>, and </em><a href="https://www.linkedin.com/in/mike-azar-7064883b/"><em>Mike Azar</em></a><em>.</em></p><p>Managing and operating asynchronous workflows can be difficult without the proper tools and architecture that puts observability, debugging, and tracing at the forefront.</p><p>Imagine getting paged outside normal work hours — users are having trouble with the application you’re responsible for, and you start diving into logs. However, they are scattered across multiple systems, and there isn’t an easy way to tie related messages together. Once you finally find useful identifiers, you may begin writing SQL queries against your production database to find out what went wrong. You’re joining tables, resolving status types, cross-referencing data manually with other systems, and by the end of it all you ask yourself why?</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/304/1*hHyMQuRdHYQp81PsybLhDg.png" /><figcaption>An upset on-call</figcaption></figure><p>This was the experience for us as the backend team on Prodicle Distribution, which is one of the many services offered in the suite of content production-facing applications called Prodicle.</p><p>Prodicle is one of the many applications that is at the exciting intersection of connecting the world of content productions to <a href="https://netflixtechblog.com/netflix-studio-engineering-overview-ed60afcfa0ce">Netflix Studio Engineering</a>. It enables a Production Office Coordinator to keep a Production’s cast, crew, and vendors organized and up to date with the latest information throughout the course of a title’s filming. (e.g. Netflix original series such as La Casa De Papel), as well as with Netflix Studio.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/431/1*8BlwxH4Rj8pztbYACRwWGA.png" /><figcaption>Users of Prodicle: Production Office Coordinator on their job</figcaption></figure><p>As the adoption of Prodicle grew over time, Productions asked for more features, which led to the system quickly evolving in multiple programming languages under different teams. When our team took ownership of Prodicle Distribution, we decided to revamp the service and expand its implementation to multiple UI clients built for web, <a href="https://netflixtechblog.com/netflix-android-and-ios-studio-apps-kotlin-multiplatform-d6d4d8d25d23">Android and iOS</a>.</p><p><strong>Prodicle Distribution</strong></p><p>Prodicle Distribution allows a production office coordinator to send secure, watermarked documents, such as scripts, to crew members as attachments or links, and track delivery. One distribution job might result in several thousand watermarked documents and links being created. If a job has 10 files and 20 recipients, then we have 10 x 20 = 200 unique watermarked documents and (optionally) links associated with them depending on the type of the Distribution job. The recipients of watermarked documents are able to access these documents and links in their email as well as in the Prodicle mobile application.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*nQAmn2xNrp51H89JpyQJqg.gif" /><figcaption>Prodicle Distribution</figcaption></figure><p>Our service is required to be elastic and handle bursty traffic. It also needs to handle third-party integration with Google Drive, making copies of PDFs with watermarks specific to each recipient, adding password protection, creating revocable links, generating thumbnails, and sending emails and push notifications. We are expected to process 1,000 watermarks for a single distribution in a minute, with non-linear latency growth as the number of watermarks increases. The goal is to process these documents as fast as possible and reliably deliver them to recipients while offering strong observability to both our users and internal teams.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/800/1*mLE6HkEYybvZIyoD2ZLLrQ.png" /><figcaption>Prodicle Distribution Requirements</figcaption></figure><p><strong>Asynchronous workflow</strong></p><p>Previously, the Distribution feature of Prodicle was treated as its own unique application. In late 2019, our team started integrating it with the rest of the ecosystem by writing a thin Java <a href="https://netflixtechblog.com/open-sourcing-the-netflix-domain-graph-service-framework-graphql-for-spring-boot-92b9dcecda18">Domain graph service</a> (DGS) to wrap the asynchronous watermarking functionality that was then in Ruby on Rails. The watermarking functionality, at the start, was a simple offering with various Google Drive integrations for storage and links. Our team was responsible for Google integrations, watermarking, bursty traffic management, and on-call support for this application. We had to traverse multiple codebases, and observability systems to debug errors and inefficiencies in the system. Things got hairy. New feature requests were adding to the maintenance burden for the team.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*oFsiKKHtAltaFUHKk_7j0w.png" /><figcaption>Initial offering of Prodicle Distribution backend</figcaption></figure><p>When we decided to migrate the asynchronous workflow to Java, we landed on these additional requirements: 1. We wanted a scalable service that was near real-time, 2. We wanted a workflow orchestrator with good observability for developers, and 3. We wanted to delegate the responsibility of watermarking and bursty traffic management for our asynchronous functions to appropriate teams.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/848/1*_pbBaQmaCNphR2MitJIjDQ.png" /><figcaption>Migration consideration for Prodicle Distribution’s asynchronous workflow</figcaption></figure><p>We evaluated what it would take to do this ourselves or rely on the offerings from our platform teams — <a href="https://netflixtechblog.com/evolution-of-netflix-conductor-16600be36bca">Conductor</a> and one of the new offerings <a href="https://netflixtechblog.com/the-netflix-cosmos-platform-35c14d9351ad">Cosmos</a>. Even though Cosmos was developed for asynchronous media processing, we worked with them to expand to generic file processing and tune their workflow platform for our near real-time use case. Early prototypes and load tests validated that the offering could meet our needs. We leaned into Cosmos because of the low variance in latency through the system, separation of concerns between the API, workflow, and the function systems, ease of load testing, customizable API layer and notifications, support for File I/O abstractions and elastic functions. Another benefit was their observability portal and its capabilities with search. We also migrated the ownership of watermarking to another internal team to focus on developing and supporting additional features.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*-fAoOZ8reMn7qUK3EspZyg.png" /><figcaption>Current architecture of Prodicle Distribution on Cosmos</figcaption></figure><p>With Cosmos, we are well-positioned to expand to future use cases like watermarking on images and videos. The Cosmos team is dedicated to improving features and functionality over the next year to make observations of our async workflows even better. It is great to have a team that will be improving the platform in the background as we continue our application development. We expect the performance and scaling to continue to get better without much effort on our part. We also expect other services to move some of their processing functionality into Cosmos, which makes integrations even easier because services can expose a function within the platform instead of GRPC or REST endpoints. The more services move to Cosmos, the bigger the value proposition becomes.</p><p><strong>Deployed to Production for Productions</strong></p><p>With productions returning to work in the midst of a global pandemic, the adoption of Prodicle Distribution has grown 10x, between June 2020 and April 2021. Starting January 2021 we did an incremental release of Prodicle Distribution on Cosmos and completed the migration in April 2021. We now support hundreds of productions, with tens of thousands of Distribution jobs, and millions of watermarks every month.</p><p>With our migration of Prodicle Distribution to Cosmos, we are able to use their observability portal called Nirvana to debug our workflow and bottlenecks.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Qn5duKIDgu9HSU8O_OL2XQ.png" /><figcaption>Observing Prodicle Distribution on Cosmos in Nirvana</figcaption></figure><p>Now that we have a platform team dedicated to the management of our async infrastructure and watermarking, our team can better maintain and support the distribution of documents. Since our migration, the number of support tickets has decreased. It is now easier for the on-call engineer and the developers to find the associated logs and traces while visualizing the state of the asynchronous workflow and data in the whole system.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/520/1*bmNmCSwr-EQJ67ohBZLOCg.png" /><figcaption>A stress-free on-call</figcaption></figure><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=cd89b923c784" width="1" height="1" alt=""><hr><p><a href="https://netflixtechblog.com/achieving-observability-in-async-workflows-cd89b923c784">Achieving observability in async workflows</a> was originally published in <a href="https://netflixtechblog.com">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Netflix Drive]]></title>
            <link>https://netflixtechblog.com/netflix-drive-a607538c3055?source=rss----2615bd06b42e---4</link>
            <guid isPermaLink="false">https://medium.com/p/a607538c3055</guid>
            <category><![CDATA[storage]]></category>
            <category><![CDATA[s3]]></category>
            <category><![CDATA[studio]]></category>
            <category><![CDATA[infrastructure]]></category>
            <category><![CDATA[netflix]]></category>
            <dc:creator><![CDATA[Netflix Technology Blog]]></dc:creator>
            <pubDate>Wed, 05 May 2021 15:27:29 GMT</pubDate>
            <atom:updated>2021-05-05T17:14:10.253Z</atom:updated>
            <content:encoded><![CDATA[<h4>A file and folder interface for Netflix Cloud Services</h4><p><em>Written by </em><a href="https://www.linkedin.com/in/vikram-krishnamurthy-0883726/"><em>Vikram Krishnamurthy</em></a><em>, </em><a href="https://www.linkedin.com/in/kishore-kasi-8a63165/"><em>Kishore Kasi</em></a><em>, </em><a href="https://www.linkedin.com/in/abhishekkapatkar/"><em>Abhishek Kapatkar</em></a><em>, </em><a href="https://www.linkedin.com/in/chopratejas/"><em>Tejas Chopra</em></a><em>, </em><a href="https://www.linkedin.com/in/prudhviraj9/"><em>Prudhviraj Karumanchi</em></a><em>, </em><a href="https://www.linkedin.com/in/kfrancis/"><em>Kelsey Francis</em></a>, <a href="https://www.linkedin.com/in/shaileshbirari/"><em>Shailesh Birari</em></a></p><p>In this post, we are introducing Netflix Drive, a Cloud drive for media assets and providing a high level overview of some of its features and interfaces. We intend this to be a first post in a series of posts covering Netflix Drive. In the future posts, we will do an architectural deep dive into the several components of Netflix Drive.</p><p>Netflix, and particularly Studio applications (and Studio in the Cloud) produce petabytes of data backed by billions of media assets. Several artists and <a href="https://netflixtechblog.com/production-media-management-transforming-media-workflows-by-leveraging-the-cloud-1174699e4a08">workflows</a> that may be globally distributed, work on different projects, and each of these projects produce content that forms a part of the large corpus of assets.</p><p>Here is an example of globally distributed production where several artists and workflows work in conjunction to create and share assets for one or many projects.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*V8dgMzYoFfbW1Osr" /><figcaption>Fig 1: Globally distributed production with artists working on different assets from different parts of the world</figcaption></figure><p>There are workflows in which these artists may want to view a subset of these assets from this large dataset, for example, pertaining to a specific project. These artists may want to create personal workspaces and work on generating intermediate assets. To support such use cases, access control at the user workspace and project workspace granularity is extremely important for presenting a globally consistent view of pertinent data to these artists.</p><p>Netflix Drive aims to solve this problem of exposing different namespaces and attaching appropriate access control to help build a scalable, performant, globally distributed platform for storing and retrieving pertinent assets.</p><blockquote>Netflix Drive is envisioned to be a Cloud Drive for Studio and Media applications and lends itself to be a generic paved path solution for all content in Netflix.</blockquote><p>It exposes a file/folder interface for applications to save their data and an API interface for control operations. Netflix Drive relies on a data store that will be the persistent storage layer for assets, and a metadata store which will provide a relevant mapping from the file system hierarchy to the data store entities. The major pieces, as shown in <em>Fig. 2</em>, are the <strong>file system interface, the API interface, and the metadata and data stores.</strong> We will delve into these in the following sections.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*hxejhOwc9hxFvizPL5RosA.png" /><figcaption>Fig 2: Netflix Drive components</figcaption></figure><h3><strong>File interface for Netflix Drive</strong></h3><p>Creative applications such as Nuke, Maya, Adobe Photoshop store and retrieve content using files and folders. Netflix Drive relies on FUSE (<a href="https://github.com/libfuse/libfuse">File System In User Space</a>) to provide POSIX files and folders interface to such applications. A FUSE based POSIX interface provides feature customization elasticity, deployment configuration flexibility as well as a standard and seamless file/folder interface. A similar user space abstraction is available for Windows (<a href="http://www.secfs.net/winfsp/">WinFSP</a>) and MacOS (<a href="https://osxfuse.github.io/">MacFUSE</a>)</p><p>The operations that originate from user, application and system actions on files and folders translate to a well defined set of function and system calls which are forwarded by the Linux Virtual File System Layer (or a pass-through/filter driver in Windows) to the FUSE layer in user space. The resulting metadata and data operations will be implemented by appropriate metadata and data adapters in Netflix Drive.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*reyKYgY-g4yeusRabSuVEw.png" /><figcaption>Fig 3: POSIX interface of Netflix Drive</figcaption></figure><p>The POSIX files and folders interface for Netflix Drive is designed as a layered system with the FUSE implementation hooks forming the top layer. This layer will provide entry points for all of the relevant VFS calls that will be implemented. Netflix Drive contains an abstraction layer below FUSE which allows different metadata and data stores to be plugged into the architecture by having their corresponding adapters implement the interface. We will discuss more about the layered architecture in the section below.</p><h3><strong>API Interface for Netflix Drive</strong></h3><p>Along with exposing a file interface which will be a hub of all abstractions, Netflix Drive also exposes API and Polled Task interfaces to allow applications and workflow tools to trigger control operations in Netflix Drive.</p><p>For example, applications can explicitly use REST endpoints to publish files stored in Netflix Drive to cloud, and later use a REST endpoint to retrieve a subset of the published files from cloud. The API interface can also be used to track the transfers of large files and allows other applications to be built on top of Netflix Drive.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*tIftA3wTZblhJmNjghzFbQ.png" /><figcaption>Fig 4: Control interface of Netflix Drive</figcaption></figure><p>The Polled Task interface allows studio and media workflow orchestrators to post or dispatch tasks to Netflix Drive instances on disparate workstations or containers. This allows Netflix Drive to be bootstrapped with an empty namespace when the workstation comes up and dynamically project a specific set of assets relevant to the artists’ work sessions or workflow stages. Further these assets can be projected into a namespace of the artist’s or application’s choosing.</p><p>Alternatively, workstations/containers can be launched with the assets of interest prefetched at startup. These allow artists and applications to obtain a workstation which already contains relevant files and optionally add and delete asset trees during the work session. For example, artists perform transformative work on files, and use Netflix Drive to store/fetch intermediate results as well as the final copy which can be transformed back into a media asset.</p><h3><strong>Bootstrapping Netflix Drive</strong></h3><p>Given the two different modes in which applications can interact with Netflix Drive, now let us discuss how Netflix Drive is bootstrapped.</p><p>On startup, Netflix Drive expects a <em>manifest</em> that contains information about the data store, metadata store, and credentials (tied to a user login) to form an instance of namespace hierarchy. A Netflix Drive mount point may contain multiple Netflix Drive namespaces.</p><p>A <em>dynamic instance </em>allows<em> </em>Netflix Drive to show a user-selected and user-accessible subset of data from a large corpus of assets. A <em>user instance</em> allows it to act like a Cloud Drive, where users can work on content which is automatically synced in the background periodically to Cloud. On restart on a new machine, the same files and folders will be prefetched from the cloud. We will cover the different namespaces of Netflix Drive in more detail in a subsequent blog post.</p><p>Here is an example of a typical bootstrap manifest file.</p><figure><img alt="This image shows a bootstrap manifest json which highlights how Netflix Drive can work with different metadata stores (such as Redis, CockroachDB), and data stores (such as Ceph, S3) and tie them together to provide persistence layer for assets" src="https://cdn-images-1.medium.com/max/1024/1*uuzkImc4XePgXdNcveKaKA.png" /><figcaption>A sample manifest file.</figcaption></figure><p>The manifest is a persistent artifact which renders a user workstation its Netflix Drive personality. It survives instance failures and is able to recreate the same stateful interface on any newly deployed instance.</p><h3>Metadata and Data Store Abstractions</h3><p>In order to allow a variety of different metadata stores and data stores to be easily plugged into the architecture, Netflix Drive exposes abstract interfaces for both metadata and data stores. Here is a high level diagram explaining the different layers of abstractions in Netflix Drive</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*f0snCRo5Kua0gmWeoYhgmA.png" /><figcaption>Fig 5: Layered architecture of Netflix Drive</figcaption></figure><h4><strong>Metadata Store Characteristics</strong></h4><p>Each file in Netflix Drive would have one or many corresponding metadata nodes, corresponding to different versions of the file. The file system hierarchy would be modeled as a tree in the metadata store where the root node is the top level folder for the application.</p><p>Each metadata node will contain several attributes, such as checksum of the file, location of the data, user permissions to access data, file metadata such as size, modification time, etc. A metadata node may also provide support for extended attributes which can be used to model ACLs, symbolic links, or other expressive file system constructs.</p><p>Metadata Store may also expose the concept of workspaces, where each user/application can have several workspaces, and can share workspaces with other users/applications. These are higher level constructs that are very useful to Studio applications.</p><h4>Data Store Characteristics</h4><p>Netflix Drive relies on a data store that allows streaming bytes into files/objects persisted on the storage media. The data store should expose APIs that allow Netflix Drive to perform I/O operations. The transfer mechanism for transport of bytes is a function of the data store.</p><p>In the first manifestation, Netflix Drive is using an object store (such as Amazon S3) as a data store. In order to expose file store-like properties, there were some changes needed in the object store. Each file can be stored as one or more objects. For Studio applications, file sizes may exceed the maximum object size for Cloud Storage, and so, the data store service should have the ability to store multiple parts of a file as separate objects. It is the responsibility of the data store service to tie these objects to a single file and inform the metadata store of the single unique Id for these several object parts. This Data store internally implements the chunking of file into several parts, encrypting of the content, and life cycle management of the data.</p><p><strong>Multi-tiered architecture</strong></p><p>Netflix Drive allows multiple data stores to be a part of the same installation via its bootstrap manifest.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*zUo8OJxV9VVvTxhC7eUBuA.png" /><figcaption>Fig 6: Multiple data stores of Netflix Drive</figcaption></figure><p>Some studio applications such as encoding and transcoding have different I/O characteristics than a typical cloud drive.</p><p>Most of the data produced by these applications is ephemeral in nature, and is read often initially. The final encoded copy needs to be persisted and the ephemeral data can be deleted. To serve such applications, Netflix Drive can persist the ephemeral data in storage tiers which are closer to the application that allow lower read latencies and better economies for read request, since cloud storage reads incur an egress cost. Finally, once the encoded copy is prepared, this copy can be persisted by Netflix Drive to a persistent storage tier in the cloud. A single data store may also choose to archive some subset of content stored in cheaper alternatives.</p><h3><strong>Security</strong></h3><p>Studio applications require strict adherence to security models where only users or applications with specific permissions should be allowed to access specific assets. Security is one of the cornerstones of Netflix Drive design. Netflix Drive dynamic namespace design allows an artist or workflow to access only a small subset of the assets based on the workspace information and access control and is one of the benefits of using Netflix Drive in Studio workflows. Netflix Drive encapsulates the authentication and authorization models in its metadata store. These are translated into POSIX ACLs in Netflix Drive. In the future, Netflix Drive can allow more expressive ACLs by leveraging extended attributes associated with Metadata nodes corresponding to an asset.</p><p>Netflix Drive is currently being used by several Studio teams as the paved path solution for working with assets and is integrated with several media suite applications. As of today, Netflix Drive can be installed on <strong>CentOS, MacOS </strong>and<strong> Windows. </strong>In the future blog posts, we will cover implementation details, learnings, performance analysis of Netflix Drive, and some of the applications and workflows built on top of Netflix Drive.</p><p>If you are passionate about building Storage and Infrastructure solutions for Netflix Data Platform, we are always looking for talented engineers and managers. Please check out our <a href="https://jobs.netflix.com/jobs/67097816">job listings</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=a607538c3055" width="1" height="1" alt=""><hr><p><a href="https://netflixtechblog.com/netflix-drive-a607538c3055">Netflix Drive</a> was originally published in <a href="https://netflixtechblog.com">Netflix TechBlog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
    </channel>
</rss>