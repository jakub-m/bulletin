<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?>
<?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:openSearch="http://a9.com/-/spec/opensearchrss/1.0/" xmlns:blogger="http://schemas.google.com/blogger/2008" xmlns:georss="http://www.georss.org/georss" xmlns:gd="http://schemas.google.com/g/2005" xmlns:thr="http://purl.org/syndication/thread/1.0" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0">
  <id>tag:blogger.com,1999:blog-8474926331452026626</id>
  <updated>2021-07-29T13:05:43.077-07:00</updated>
  <category term="Machine Learning"/>
  <category term="Deep Learning"/>
  <category term="Computer Vision"/>
  <category term="Google Brain"/>
  <category term="open source"/>
  <category term="Natural Language Processing"/>
  <category term="Publications"/>
  <category term="Research"/>
  <category term="TensorFlow"/>
  <category term="Education"/>
  <category term="Machine Perception"/>
  <category term="University Relations"/>
  <category term="Natural Language Understanding"/>
  <category term="Neural Networks"/>
  <category term="conference"/>
  <category term="conferences"/>
  <category term="datasets"/>
  <category term="Reinforcement Learning"/>
  <category term="Health"/>
  <category term="Robotics"/>
  <category term="AI"/>
  <category term="Research Awards"/>
  <category term="CVPR"/>
  <category term="Computer Science"/>
  <category term="MOOC"/>
  <category term="NLP"/>
  <category term="Machine Intelligence"/>
  <category term="Computational Photography"/>
  <category term="Speech"/>
  <category term="Quantum Computing"/>
  <category term="YouTube"/>
  <category term="Algorithms"/>
  <category term="Machine Translation"/>
  <category term="Visualization"/>
  <category term="On-device Learning"/>
  <category term="Pixel"/>
  <category term="Image Classification"/>
  <category term="Android"/>
  <category term="Awards"/>
  <category term="HCI"/>
  <category term="ICLR"/>
  <category term="Information Retrieval"/>
  <category term="accessibility"/>
  <category term="Hardware"/>
  <category term="Image Processing"/>
  <category term="Structured Data"/>
  <category term="TPU"/>
  <category term="optimization"/>
  <category term="Audio"/>
  <category term="AutoML"/>
  <category term="ICML"/>
  <category term="ML Fairness"/>
  <category term="Security and Privacy"/>
  <category term="Speech Recognition"/>
  <category term="ACL"/>
  <category term="Google Accelerated Science"/>
  <category term="Physics"/>
  <category term="Quantum AI"/>
  <category term="Search"/>
  <category term="TTS"/>
  <category term="ACM"/>
  <category term="AI for Social Good"/>
  <category term="Earth Engine"/>
  <category term="Graph Mining"/>
  <category term="K-12"/>
  <category term="ML"/>
  <category term="NeurIPS"/>
  <category term="Collaboration"/>
  <category term="Diversity"/>
  <category term="Google Maps"/>
  <category term="Google Translate"/>
  <category term="Self-Supervised Learning"/>
  <category term="User Experience"/>
  <category term="Voice Search"/>
  <category term="ph.d. fellowship"/>
  <category term="statistics"/>
  <category term="Automatic Speech Recognition"/>
  <category term="Chemistry"/>
  <category term="DeepMind"/>
  <category term="EMNLP"/>
  <category term="NIPS"/>
  <category term="UI"/>
  <category term="Vision Research"/>
  <category term="grants"/>
  <category term="video"/>
  <category term="Faculty Summit"/>
  <category term="Google Cloud Platform"/>
  <category term="Google Genomics"/>
  <category term="Translate"/>
  <category term="Video Analysis"/>
  <category term="crowd-sourcing"/>
  <category term="data science"/>
  <category term="distributed systems"/>
  <category term="market algorithms"/>
  <category term="Art"/>
  <category term="Augmented Reality"/>
  <category term="Compression"/>
  <category term="Course Builder"/>
  <category term="Environment"/>
  <category term="Google Photos"/>
  <category term="Google+"/>
  <category term="Interspeech"/>
  <category term="Multimodal Learning"/>
  <category term="PhD Fellowship"/>
  <category term="Semi-supervised Learning"/>
  <category term="Supervised Learning"/>
  <category term="WWW"/>
  <category term="Cloud Computing"/>
  <category term="Computational Imaging"/>
  <category term="Data Discovery"/>
  <category term="Expander"/>
  <category term="Fusion Tables"/>
  <category term="Google Books"/>
  <category term="Machine Hearing"/>
  <category term="Moore's Law"/>
  <category term="Ngram"/>
  <category term="Social Networks"/>
  <category term="Software"/>
  <category term="Systems"/>
  <category term="Unsupervised Learning"/>
  <category term="renewable energy"/>
  <category term="schema.org"/>
  <category term="API"/>
  <category term="Acoustic Modeling"/>
  <category term="App Engine"/>
  <category term="Europe"/>
  <category term="Gmail"/>
  <category term="Google Play Apps"/>
  <category term="High Dynamic Range Imaging"/>
  <category term="ICCV"/>
  <category term="Image Annotation"/>
  <category term="Internet of Things"/>
  <category term="Networks"/>
  <category term="Optical Character Recognition"/>
  <category term="Recommender Systems"/>
  <category term="Semantic Models"/>
  <category term="Virtual Reality"/>
  <category term="Year in Review"/>
  <category term="economics"/>
  <category term="internationalization"/>
  <category term="publication"/>
  <category term="search ads"/>
  <category term="wikipedia"/>
  <category term="Adaptive Data Analysis"/>
  <category term="Africa"/>
  <category term="App Inventor"/>
  <category term="China"/>
  <category term="DeepDream"/>
  <category term="EMEA"/>
  <category term="Exacycle"/>
  <category term="Google Drive"/>
  <category term="Google Science Fair"/>
  <category term="Graph"/>
  <category term="Inbox"/>
  <category term="India"/>
  <category term="KDD"/>
  <category term="Kaggle"/>
  <category term="Keyboard Input"/>
  <category term="Labs"/>
  <category term="Low-Light Photography"/>
  <category term="MapReduce"/>
  <category term="Policy"/>
  <category term="Proposals"/>
  <category term="Style Transfer"/>
  <category term="TensorBoard"/>
  <category term="VLDB"/>
  <category term="ads"/>
  <category term="electronics"/>
  <category term="resource optimization"/>
  <category term="trends"/>
  <category term="Android Wear"/>
  <category term="April Fools"/>
  <category term="Australia"/>
  <category term="BigQuery"/>
  <category term="Cantonese"/>
  <category term="Chrome"/>
  <category term="Conservation"/>
  <category term="Data Center"/>
  <category term="Electronic Commerce and Algorithms"/>
  <category term="Encryption"/>
  <category term="Entity Salience"/>
  <category term="Faculty Institute"/>
  <category term="Flu Trends"/>
  <category term="Gboard"/>
  <category term="Google Docs"/>
  <category term="Google Sheets"/>
  <category term="Google Trips"/>
  <category term="Google Voice Search"/>
  <category term="Government"/>
  <category term="ICSE"/>
  <category term="IPython"/>
  <category term="Journalism"/>
  <category term="Klingon"/>
  <category term="Korean"/>
  <category term="Linear Optimization"/>
  <category term="Magenta"/>
  <category term="Market Research"/>
  <category term="Mixed Reality"/>
  <category term="NAACL"/>
  <category term="Network Management"/>
  <category term="Nexus"/>
  <category term="Peer Review"/>
  <category term="PhotoScan"/>
  <category term="PiLab"/>
  <category term="Professional Development"/>
  <category term="Public Data Explorer"/>
  <category term="SIGCOMM"/>
  <category term="SIGMOD"/>
  <category term="Site Reliability Engineering"/>
  <category term="Sound Search"/>
  <category term="TV"/>
  <category term="UNIX"/>
  <category term="Visiting Faculty"/>
  <category term="Wiki"/>
  <category term="adsense"/>
  <category term="adwords"/>
  <category term="correlate"/>
  <category term="entities"/>
  <category term="gamification"/>
  <category term="jsm"/>
  <category term="jsm2011"/>
  <category term="localization"/>
  <category term="operating systems"/>
  <category term="osdi"/>
  <category term="osdi10"/>
  <category term="patents"/>
  <title type="text">Google AI Blog</title>
  <subtitle type="html">The latest news from Google AI.</subtitle>
  <link rel="alternate" type="text/html" href="http://ai.googleblog.com/"/>
  <link rel="next" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default?alt=atom&amp;start-index=26&amp;max-results=25&amp;redirect=false"/>
  <author>
    <name>ewood</name>
    <uri>http://www.blogger.com/profile/12341551220176883769</uri>
    <email>noreply@blogger.com</email>
    <gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="16" height="16" src="https://img1.blogblog.com/img/b16-rounded.gif"/>
  </author>
  <generator version="7.00" uri="http://www.blogger.com">Blogger</generator>
  <openSearch:totalResults>969</openSearch:totalResults>
  <openSearch:startIndex>1</openSearch:startIndex>
  <openSearch:itemsPerPage>25</openSearch:itemsPerPage>
  <atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/blogspot/gJZg"/>
  <feedburner:info uri="blogspot/gjzg"/>
  <atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/"/>
  <entry>
    <id>tag:blogger.com,1999:blog-8474926331452026626.post-537064785672594983</id>
    <published>2021-07-28T08:27:00.004-07:00</published>
    <updated>2021-07-29T13:05:10.956-07:00</updated>
    <category scheme="http://www.blogger.com/atom/ns#" term="Africa"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="AI for Social Good"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="Computer Vision"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="datasets"/>
    <title type="text">Mapping Africa’s Buildings with Satellite Imagery</title>
    <content type="html">&lt;span class="byline-author"&gt;Posted by John Quinn, Software Engineer, Google Research, Ghana&lt;/span&gt; &lt;p&gt;An accurate record of building footprints is important for a range of applications, from population estimation and urban planning to humanitarian response and environmental science. After a disaster, such as a flood or an earthquake, authorities need to estimate how many households have been affected. Ideally there would be up-to-date census information for this, but in practice such records may be out of date or unavailable. Instead, data on the locations and density of buildings can be a valuable alternative source of information. &lt;/p&gt;&lt;p&gt;A good way to collect such data is through satellite imagery, which can map the distribution of buildings across the world, particularly in areas that are isolated or difficult to access. However, detecting buildings with computer vision methods in some environments can be a challenging task. Because satellite imaging involves photographing the earth from several hundred kilometres above the ground, even at high resolution (30–50 cm per pixel), a small building or tent shelter occupies only a few pixels. The task is even more difficult for informal settlements, or rural areas where buildings constructed with natural materials can visually blend into the surroundings. There are also many types of natural and artificial features that can be easily confused with buildings in overhead imagery. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-8FOuLSKgG-s/YQFy5MkCCGI/AAAAAAAAH8s/4c2uBwe_cWYpSe6_x7-rkIQD1k1i8yxJgCLcBGAsYHQ/s901/ConfoundingObjects.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="901" data-original-width="890" height="400" src="https://1.bp.blogspot.com/-8FOuLSKgG-s/YQFy5MkCCGI/AAAAAAAAH8s/4c2uBwe_cWYpSe6_x7-rkIQD1k1i8yxJgCLcBGAsYHQ/w395-h400/ConfoundingObjects.png" width="395" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Objects that can confuse computer vision models for building identification (clockwise from top left) pools, rocks, enclosure walls and shipping containers.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;In “&lt;a href="https://arxiv.org/abs/2107.12283"&gt;Continental-Scale Building Detection from High-Resolution Satellite Imagery&lt;/a&gt;”, we address these challenges, using new methods for detecting buildings that work in rural and urban settings across different terrains, such as savannah, desert, and forest, as well as informal settlements and refugee facilities. We use this building detection model to create the &lt;a href="https://sites.research.google/open-buildings"&gt;Open Buildings dataset&lt;/a&gt;, a new open-access data resource containing the locations and footprints of 516 million buildings with coverage across most of the African continent. The dataset will support several practical, scientific and humanitarian applications, ranging from disaster response or population mapping to planning services such as new medical facilities or studying human impact on the natural environment.  &lt;/p&gt;&lt;p&gt;&lt;b&gt;Model Development&lt;/b&gt;&lt;br /&gt;We built a training dataset for the building detection model by manually labelling 1.75 million buildings in 100k images. The figure below shows some examples of how we labelled images in the training data, taking into account confounding characteristics of different areas across the African continent. In rural areas, for example, it was necessary to identify different types of dwelling places and to disambiguate them from natural features, while in urban areas we needed to develop labelling policies for dense and contiguous structures. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-N5eiVvwK_Gk/YQFzITyXhtI/AAAAAAAAH8w/F2djNAFXCTUDj7x-SdmeC4pDfCqkoLy8ACLcBGAsYHQ/s745/image1.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="240" data-original-width="745" height="206" src="https://1.bp.blogspot.com/-N5eiVvwK_Gk/YQFzITyXhtI/AAAAAAAAH8w/F2djNAFXCTUDj7x-SdmeC4pDfCqkoLy8ACLcBGAsYHQ/w640-h206/image1.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;&lt;b&gt;(1)&lt;/b&gt; Example of a compound containing both dwelling places as well as smaller outbuildings such as grain stores. &lt;b&gt;(2)&lt;/b&gt; Example of a round, thatched-roof structure that can be difficult for a model to distinguish from trees, and where it is necessary to use cues from pathways, clearings and shadows to disambiguate. &lt;b&gt;(3)&lt;/b&gt; Example of several contiguous buildings for which the boundaries cannot be easily distinguished.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;We trained the model to detect buildings in a bottom-up way, first by classifying each pixel as building or non-building, and then grouping these pixels together into individual instances. The detection pipeline was based on the &lt;a href="https://arxiv.org/abs/1505.04597"&gt;U-Net&lt;/a&gt; model, which is commonly used in satellite image analysis. One advantage of U-Net is that it is a relatively compact architecture, and so can be applied to large quantities of imaging data without a heavy compute burden. This is critical, because the final task of applying this to continental-scale satellite imagery means running the model on many billions of image tiles.  &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-GOkhVuEYtbM/YQFzZ_CqulI/AAAAAAAAH88/TH1V6nURowY9fimNmYqXQpGYG27GpjJUgCLcBGAsYHQ/s1685/image5.jpg" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="498" data-original-width="1685" height="190" src="https://1.bp.blogspot.com/-GOkhVuEYtbM/YQFzZ_CqulI/AAAAAAAAH88/TH1V6nURowY9fimNmYqXQpGYG27GpjJUgCLcBGAsYHQ/w640-h190/image5.jpg" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Example of segmenting buildings in satellite imagery.&amp;nbsp;&lt;b&gt;Left:&lt;/b&gt;&amp;nbsp;Source image;&amp;nbsp;&lt;b&gt;Center:&lt;/b&gt;&amp;nbsp;Semantic segmentation, with each pixel assigned a confidence score that it is a building vs. non-building;&amp;nbsp;&lt;b&gt;Right:&lt;/b&gt;&amp;nbsp;Instance segmentation, obtained by thresholding and grouping together connected components.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Initial experiments with the basic model had low &lt;a href="https://en.wikipedia.org/wiki/Precision_and_recall"&gt;precision and recall&lt;/a&gt;, for example due to the variety of natural and artificial features with building-like appearance. We found a number of methods that improved performance. One was the use of &lt;a href="https://arxiv.org/abs/1710.09412"&gt;mixup&lt;/a&gt; as a regularisation method, where random training images are blended together by taking a weighted average. Though mixup was originally proposed for image classification, we modified it to be used for semantic segmentation. Regularisation is important in general for this building segmentation task, because even with 100k training images, the training data do not capture the full variation of terrain, atmospheric and lighting conditions that the model is presented with at test time, and hence, there is a tendency to overfit. This is mitigated by mixup as well as random augmentation of training images. &lt;/p&gt;&lt;p&gt;Another method that we found to be effective was the use of unsupervised self-training. We prepared a set of 100 million satellite images from across Africa, and filtered these to a subset of 8.7 million images that mostly contained buildings. This dataset was used for self-training using the &lt;a href="https://arxiv.org/abs/1911.04252"&gt;Noisy Student&lt;/a&gt; method, in which the output of the best building detection model from the previous stage is used as a ‘teacher’ to then train a ‘student’ model that makes similar predictions from augmented images. In practice, we found that this reduced false positives and sharpened the detection output. The student model gave higher confidence to buildings and lower confidence to background. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-kFv8TDjiGOg/YQFziW7dZWI/AAAAAAAAH9A/RuuF9oumsDgC8BZy-K7vRLXJUEqCE2_VQCLcBGAsYHQ/s1051/image4.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="302" data-original-width="1051" height="184" src="https://1.bp.blogspot.com/-kFv8TDjiGOg/YQFziW7dZWI/AAAAAAAAH9A/RuuF9oumsDgC8BZy-K7vRLXJUEqCE2_VQCLcBGAsYHQ/w640-h184/image4.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Difference in model output between the student and teacher models for a typical image. In panel (d), red areas are those that the student model finds more likely to be buildings than the teacher model, and blue areas more likely to be background.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;One problem that we faced initially was that our model had a tendency to create “blobby” detections, without clearly delineated edges and with a tendency for neighbouring buildings to be merged together. To address this, we applied another idea from the original &lt;a href="https://arxiv.org/abs/1505.04597"&gt;U-Net paper&lt;/a&gt;, which is to use distance weighting to adapt the loss function to emphasise the importance of making correct predictions near boundaries. During training, distance weighting places greater emphasis at the edges by adding weight to the loss — particularly where there are instances that nearly touch. For building detection, this encourages the model to correctly identify the gaps in between buildings, which is important so that many close structures are not merged together. We found that the original U-Net distance weighting formulation was helpful but slow to compute. So, we developed an alternative based on Gaussian convolution of edges, which was both faster and more effective. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-eJYbpIYg0ms/YQFz9s46igI/AAAAAAAAH9I/DUt9_kPc3domDQDFAE9gUE0lmWxHlZntwCLcBGAsYHQ/s1013/Edges.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="508" data-original-width="1013" height="200" src="https://1.bp.blogspot.com/-eJYbpIYg0ms/YQFz9s46igI/AAAAAAAAH9I/DUt9_kPc3domDQDFAE9gUE0lmWxHlZntwCLcBGAsYHQ/w400-h200/Edges.png" width="400" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Distance weighting schemes to emphasise nearby edges: U-Net&amp;nbsp;&lt;b&gt;(left)&lt;/b&gt;&amp;nbsp;and Gaussian convolution of edges&amp;nbsp;&lt;b&gt;(right)&lt;/b&gt;.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Our &lt;a href="https://arxiv.org/abs/2107.12283"&gt;technical report&lt;/a&gt; has more details on each of these methods. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Results&lt;/b&gt;&lt;br /&gt;We evaluated the performance of the model on several different regions across the continent, in different categories: urban, rural, and medium-density. In addition, with the goal of preparing for potential humanitarian applications, we tested the model on regions with displaced persons and refugee settlements. Precision and recall did vary between regions, so achieving consistent performance across the continent is an ongoing challenge. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-Q329VaudvO4/YQF0f1AXO8I/AAAAAAAAH9Q/dfml_Xjue7Y_h1f5NhYylPr6_5HB1mCfACLcBGAsYHQ/s604/PrecisionRecall.jpg" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="604" data-original-width="553" height="640" src="https://1.bp.blogspot.com/-Q329VaudvO4/YQF0f1AXO8I/AAAAAAAAH9Q/dfml_Xjue7Y_h1f5NhYylPr6_5HB1mCfACLcBGAsYHQ/w586-h640/PrecisionRecall.jpg" width="586" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Precision-recall curves, measured at 0.5&amp;nbsp;&lt;a href="https://en.wikipedia.org/wiki/Jaccard_index"&gt;intersection-over-union&lt;/a&gt;&amp;nbsp;threshold.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;When visually inspecting the detections for low-scoring regions, we noted various causes. In rural areas, label errors were problematic. For example, single buildings within a mostly-empty area can be difficult for labellers to spot. In urban areas, the model had a tendency to split large buildings into separate instances. The model also underperformed in desert terrain, where buildings were hard to distinguish against the background. &lt;/p&gt;&lt;p&gt;We carried out an ablation study to understand which methods contributed most to the final performance, measured in &lt;a href="https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Mean_average_precision"&gt;mean average precision&lt;/a&gt; (mAP). Distance weighting, mixup and the use of &lt;a href="https://image-net.org/"&gt;ImageNet&lt;/a&gt; pre-training were the biggest factors for the performance of the supervised learning baseline. The ablated models that did not use these methods had a mAP difference of -0.33, -0.12 and -0.07 respectively. Unsupervised self-training gave a further significant boost of +0.06 mAP. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-MES2HEmsCKc/YQGXrO09syI/AAAAAAAAH-I/Gfx8zbXdipcEf5R_YVTGR7WBr2nhe9cYgCLcBGAsYHQ/s1350/ablations.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="545" data-original-width="1350" height="258" src="https://1.bp.blogspot.com/-MES2HEmsCKc/YQGXrO09syI/AAAAAAAAH-I/Gfx8zbXdipcEf5R_YVTGR7WBr2nhe9cYgCLcBGAsYHQ/w640-h258/ablations.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Ablation study of training methods. The first row shows the mAP performance of the best model combined with self-training, and the second row shows the best model with supervised learning only (the baseline). By disabling each training optimization from the baseline in turn, we observe the impact on mAP test performance. Distance weighting has the most significant effect.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;!--&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-u-WegC4oZ4E/YQF0z95UlnI/AAAAAAAAH9c/_ZHIfHJjJXUJU5ZOWl7LyVZoww7fakszQCLcBGAsYHQ/s648/Ablation.jpg" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="262" data-original-width="648" height="258" src="https://1.bp.blogspot.com/-u-WegC4oZ4E/YQF0z95UlnI/AAAAAAAAH9c/_ZHIfHJjJXUJU5ZOWl7LyVZoww7fakszQCLcBGAsYHQ/w640-h258/Ablation.jpg" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Ablation study of training methods. The first row shows the mAP performance of the best model combined with self-training, and the second row shows the best model with supervised learning only (the baseline). By disabling each training optimization from the baseline in turn, we observe the impact on mAP test performance. Distance weighting has the most significant effect.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;--&gt;&lt;p&gt;&lt;b&gt;Generating the Open Buildings Dataset&lt;/b&gt;&lt;br /&gt;To create the final dataset, we applied our best building detection model to satellite imagery across the African continent (8.6 billion image tiles covering 19.4 million km&lt;sup&gt;2&lt;/sup&gt;, 64% of the continent), which resulted in the detection of 516M distinct structures. &lt;/p&gt;&lt;p&gt;Each building’s outline was simplified as a polygon and associated with a &lt;a href="https://maps.google.com/pluscodes/"&gt;Plus Code&lt;/a&gt;, which is a geographic identifier made up of numbers and letters, akin to a street address, and useful for identifying buildings in areas that don’t have formal addressing systems. We also include confidence scores and guidance on suggested thresholds to achieve particular precision levels. &lt;/p&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-nM7u9-jNRB0/YQF083zsIFI/AAAAAAAAH9k/3i33jLnVbxY7c1PJE1kTT_WXX4a5fnERwCLcBGAsYHQ/s801/image2.png" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="585" data-original-width="801" height="293" src="https://1.bp.blogspot.com/-nM7u9-jNRB0/YQF083zsIFI/AAAAAAAAH9k/3i33jLnVbxY7c1PJE1kTT_WXX4a5fnERwCLcBGAsYHQ/w400-h293/image2.png" width="400" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;p&gt;  The sizes of the structures vary as shown below, tending towards small footprints. The inclusion of small structures is important, for example, to support analyses of informal settlements or refugee facilities. &lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-jVxTGydW6yI/YQF1NpCMBrI/AAAAAAAAH90/622jzhqrTzIN2g5PNTw39Kr523JIXK8vwCLcBGAsYHQ/s538/FootprintDistribution.jpg" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="498" data-original-width="538" height="370" src="https://1.bp.blogspot.com/-jVxTGydW6yI/YQF1NpCMBrI/AAAAAAAAH90/622jzhqrTzIN2g5PNTw39Kr523JIXK8vwCLcBGAsYHQ/w400-h370/FootprintDistribution.jpg" width="400" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Distribution of building footprint sizes.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;The data is freely available and we look forward to hearing how it is used. In the future, we may add new features and regions, depending on usage and feedback. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Acknowledgements&lt;/b&gt;&lt;br /&gt;&lt;em&gt;This work is part of our &lt;a href="https://ai.google/social-good/"&gt;AI for Social Good&lt;/a&gt; efforts and was led by Google Research, Ghana.  Thanks to the co-authors of this work: Wojciech Sirko, Sergii Kashubin, Marvin Ritter, Abigail Annkah, Yasser Salah Eddine Bouchareb, Yann Dauphin, Daniel Keysers, Maxim Neumann and Moustapha Cisse. We are grateful to Abdoulaye Diack, Sean Askay, Ruth Alcantara and Francisco Moneo for help with coordination. Rob Litzke, Brian Shucker, Yan Mayster and Michelina Pallone provided valuable assistance with geo infrastructure.  &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/blogspot/gJZg?a=bcEqeVSMnBQ:_yCO0jy9YNI:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/blogspot/gJZg/~4/bcEqeVSMnBQ" height="1" width="1" alt=""/&gt;</content>
    <link rel="replies" type="application/atom+xml" href="http://ai.googleblog.com/feeds/537064785672594983/comments/default" title="Post Comments"/>
    <link rel="replies" type="text/html" href="http://ai.googleblog.com/2021/07/mapping-africas-buildings-with.html#comment-form" title="0 Comments"/>
    <link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/537064785672594983"/>
    <link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/537064785672594983"/>
    <link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/blogspot/gJZg/~3/bcEqeVSMnBQ/mapping-africas-buildings-with.html" title="Mapping Africa’s Buildings with Satellite Imagery"/>
    <author>
      <name>Google AI</name>
      <uri>http://www.blogger.com/profile/12098626514775266161</uri>
      <email>noreply@blogger.com</email>
      <gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="16" height="16" src="https://img1.blogblog.com/img/b16-rounded.gif"/>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://1.bp.blogspot.com/-8FOuLSKgG-s/YQFy5MkCCGI/AAAAAAAAH8s/4c2uBwe_cWYpSe6_x7-rkIQD1k1i8yxJgCLcBGAsYHQ/s72-w395-h400-c/ConfoundingObjects.png" height="72" width="72"/>
    <thr:total>0</thr:total>
    <feedburner:origLink>http://ai.googleblog.com/2021/07/mapping-africas-buildings-with.html</feedburner:origLink>
  </entry>
  <entry>
    <id>tag:blogger.com,1999:blog-8474926331452026626.post-3366307881061046058</id>
    <published>2021-07-27T09:49:00.001-07:00</published>
    <updated>2021-07-28T12:56:14.486-07:00</updated>
    <category scheme="http://www.blogger.com/atom/ns#" term="AI"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="Information Retrieval"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="open source"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="TensorFlow"/>
    <title type="text">Advances in TF-Ranking</title>
    <content type="html">&lt;span class="byline-author"&gt;Posted by Michael Bendersky and Xuanhui Wang, Software Engineers, Google Research&lt;/span&gt; &lt;p&gt;In December 2018, &lt;a href="https://ai.googleblog.com/2018/12/tf-ranking-scalable-tensorflow-library.html"&gt;we&lt;/a&gt; &lt;a href="https://youtu.be/Un0JDL3i5Hg"&gt;introduced&lt;/a&gt; &lt;a href="https://github.com/tensorflow/ranking"&gt;TF-Ranking&lt;/a&gt;,&amp;nbsp;an open-source TensorFlow-based library for developing scalable neural &lt;a href="https://en.wikipedia.org/wiki/Learning_to_rank"&gt;learning-to-rank&lt;/a&gt; (LTR) models, which are useful in settings where users expect to receive an ordered list of items in response to their query. LTR models — unlike standard classification models that classify one item at a time — receive an entire list of items as an input, and learn an ordering that maximizes the utility of the entire list. While search and recommendation systems are the most common applications of LTR models, since its release, we have seen TF-Ranking being applied in diverse domains beyond search, including &lt;a href="https://dl.acm.org/doi/abs/10.1145/3308560.3316603"&gt;e-commerce&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/1904.12084"&gt;SAT solvers&lt;/a&gt;, and &lt;a href="https://dl.acm.org/doi/abs/10.1145/3450267.3450538"&gt;smart city planning&lt;/a&gt;.  &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-IQTHStDjBVk/YQG1FVsdgNI/AAAAAAAAH-Q/WpWEWkgot483yRy6RhUsDvhaiRmPo2MzwCLcBGAsYHQ/s769/image1.jpg" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="350" data-original-width="769" height="183" src="https://1.bp.blogspot.com/-IQTHStDjBVk/YQG1FVsdgNI/AAAAAAAAH-Q/WpWEWkgot483yRy6RhUsDvhaiRmPo2MzwCLcBGAsYHQ/w400-h183/image1.jpg" width="400" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;The goal of learning-to-rank (LTR) is to learn a function f() that takes as an input a list of items (documents, products, movies, etc.) and outputs the list of items in the optimal order (descending order of relevance). Here, green shade indicates item relevance level, and the red item marked with 'x' is non-relevant.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;In May 2021, we published a &lt;a href="https://github.com/tensorflow/ranking/releases/tag/v0.4.0"&gt;major release&lt;/a&gt; of TF-Ranking that enables full support for natively building LTR models using &lt;a href="https://keras.io/about/"&gt;Keras&lt;/a&gt;, a high-level API of &lt;a href="https://www.tensorflow.org/"&gt;TensorFlow 2&lt;/a&gt;. Our native Keras ranking model has a brand-new workflow design, including a flexible &lt;em&gt;ModelBuilder&lt;/em&gt;, a &lt;em&gt;DatasetBuilder&lt;/em&gt; to set up training data, and a &lt;em&gt;Pipeline&lt;/em&gt; to train the model with the provided dataset. These components make building a customized LTR model easier than ever, and facilitate rapid exploration of new model structures for production and research. If &lt;a href="https://blog.tensorflow.org/2018/12/introducing-ragged-tensors.html"&gt;RaggedTensors&lt;/a&gt; are your tool of choice, TF-Ranking is now working with them &lt;a href="https://github.com/tensorflow/ranking/blob/master/tensorflow_ranking/examples/keras/antique_ragged.py"&gt;as well&lt;/a&gt;. In addition, our &lt;a href="https://github.com/tensorflow/ranking/releases/tag/v0.4.2"&gt;most recent release&lt;/a&gt;, which incorporates the &lt;a href="https://github.com/tensorflow/models/tree/master/orbit"&gt;Orbit&lt;/a&gt; training library, contains a long list of advances — the culmination of two and half years of neural LTR research. Below we share a few of the key improvements available in the latest TF-Ranking version. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-bL294WB0B6c/YQG1MepjFMI/AAAAAAAAH-U/oh9jrFOSzKAEuAL-r52v87Okb901AT3VgCLcBGAsYHQ/s339/image3.jpg" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="248" data-original-width="339" height="293" src="https://1.bp.blogspot.com/-bL294WB0B6c/YQG1MepjFMI/AAAAAAAAH-U/oh9jrFOSzKAEuAL-r52v87Okb901AT3VgCLcBGAsYHQ/w400-h293/image3.jpg" width="400" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Workflow to build and train a native Keras ranking model. Blue modules are provided by TF-Ranking, and green modules are customizable.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;      &lt;p&gt;&lt;b&gt;Learning-to-Rank with TFR-BERT&lt;/b&gt;&lt;br /&gt;Recently, pretrained language models like &lt;a href="https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html"&gt;BERT&lt;/a&gt; have achieved state-of-the-art performance on various language understanding tasks. To capture the expressiveness of these models, TF-Ranking implements a novel TFR-BERT architecture that couples BERT with the power of LTR to optimize the ordering of list inputs. As an example, consider a query and a list of &lt;em&gt;n&lt;/em&gt; documents that one might like to rank in response to this query. Instead of learning an independent BERT representation for each &lt;em&gt;&amp;lt;query, document&amp;gt;&lt;/em&gt; pair, LTR models apply a &lt;a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2007-40.pdf"&gt;ranking loss&lt;/a&gt; to jointly learn a BERT representation that maximizes the utility of the entire ranked list with respect to the ground-truth labels. &lt;/p&gt;&lt;p&gt;The figure below illustrates this process. First, we flatten a list of &lt;em&gt;n&lt;/em&gt; documents to rank in response to a query into a list &lt;em&gt;&amp;lt;query, document&amp;gt;&lt;/em&gt; tuples. These tuples are fed into a pre-trained language model (e.g., BERT). The pooled BERT outputs for the entire document list are then jointly fine-tuned with one of the specialized &lt;a href="https://github.com/tensorflow/ranking/blob/master/tensorflow_ranking/python/losses.py"&gt;ranking losses&lt;/a&gt; available in TF-Ranking. Our experience shows that this TFR-BERT architecture delivers significant improvements in pretrained language model performance, leading to state-of-the-art performance for &lt;a href="https://arxiv.org/abs/2004.08476"&gt;several&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2010.00200"&gt;popular&lt;/a&gt; ranking tasks, especially when multiple pretrained language models are ensembled. Our users can now get started with TFR-BERT using this &lt;a href="https://github.com/tensorflow/ranking/blob/master/tensorflow_ranking/examples/keras/tfrbert_antique_train.py"&gt;simple example&lt;/a&gt;. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-3Y-7M__j9XM/YQG1RMl5hTI/AAAAAAAAH-Y/i-tZ7G-FTKoSoGnDsCLOYsU1OrWQvEwYgCLcBGAsYHQ/s907/image4.jpg" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="285" data-original-width="907" height="202" src="https://1.bp.blogspot.com/-3Y-7M__j9XM/YQG1RMl5hTI/AAAAAAAAH-Y/i-tZ7G-FTKoSoGnDsCLOYsU1OrWQvEwYgCLcBGAsYHQ/s16000/image4.jpg" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;An illustration of the TFR-BERT architecture, in which a joint LTR model over a list of n documents is constructed using BERT representations of individual &amp;lt;query, document&amp;gt; pairs.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;&lt;b&gt;Interpretable Learning-to-Rank&lt;/b&gt;&lt;br /&gt;Transparency and interpretability are important factors in deploying LTR models in ranking systems that can be involved in determining the outcomes of processes such as loan eligibility assessment, advertisement targeting, or guiding medical treatment decisions. In such cases, the contribution of each individual feature to the final ranking should be examinable and understandable to ensure transparency, accountability and fairness of the outcomes.  &lt;/p&gt;&lt;p&gt;One possible way to achieve this is using &lt;a href="https://en.wikipedia.org/wiki/Generalized_additive_model"&gt;generalized additive models&lt;/a&gt; (GAMs) — intrinsically interpretable machine learning models that are linearly composed of smooth functions of individual features. However, while GAMs have been extensively studied on &lt;a href="https://en.wikipedia.org/wiki/Regression_analysis"&gt;regression&lt;/a&gt; and classification tasks, it is less clear how to apply them in a ranking setting. For instance, while GAMs can be straightforwardly applied to model each individual item in the list, modeling both item interactions and the context in which these items are ranked is a more challenging research problem. To this end, we have developed a &lt;a href="https://arxiv.org/abs/2005.02553"&gt;neural ranking GAM&lt;/a&gt; — an extension of generalized additive models to ranking problems. &lt;/p&gt;&lt;p&gt;Unlike standard GAMs, a neural ranking GAM can take into account both the features of the ranked items and the context features (e.g., query or user profile) to derive an interpretable, compact model. This ensures that not only the contribution of each item-level feature is interpretable, but also the contribution of the context features. For example, in the figure below, using a neural ranking GAM makes visible how distance, price, and relevance, in the context of a given user device, contribute to the final ranking of the hotel. Neural ranking GAMs are now &lt;a href="https://github.com/tensorflow/ranking/issues/202"&gt;available&lt;/a&gt; as a part of TF-Ranking, &lt;/p&gt;   &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-wLnimKYmhbA/YPrnHWh4jiI/AAAAAAAAH8c/AndggKFG4_YTKo7MeZ6m5E67ZA-Z4wQfgCLcBGAsYHQ/s1270/image2.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="940" data-original-width="1270" src="https://1.bp.blogspot.com/-wLnimKYmhbA/YPrnHWh4jiI/AAAAAAAAH8c/AndggKFG4_YTKo7MeZ6m5E67ZA-Z4wQfgCLcBGAsYHQ/s16000/image2.png" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;An example of applying neural ranking GAM for local search. For each input feature (e.g., price, distance), a sub-model produces a sub-score that can be examined, providing transparency. Context features (e.g., user device type) can be utilized to derive importance weights of submodels.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;p&gt;&lt;b&gt;Neural Ranking or Gradient Boosting?&lt;/b&gt;&lt;br /&gt;While neural models have achieved state of the art performance in multiple domains, specialized &lt;a href="https://en.wikipedia.org/wiki/Gradient_boosting#Gradient_tree_boosting"&gt;gradient boosted decision trees&lt;/a&gt; (GBDTs) like &lt;a href="https://www.microsoft.com/en-us/research/uploads/prod/2016/02/MSR-TR-2010-82.pdf"&gt;LambdaMART&lt;/a&gt; remained the baseline to beat in a variety of open LTR datasets. The success of GBDTs in open datasets is due to several reasons. First, due to their relatively small size, neural models are prone to &lt;a href="https://en.wikipedia.org/wiki/Overfitting"&gt;overfitting&lt;/a&gt; on these datasets. Second, since GBDTs partition their input feature space using decision trees, they are naturally more resilient to variations in numerical scales in ranking data, which often contain features with &lt;a href="https://en.wikipedia.org/wiki/Zipf%27s_law"&gt;Zipfian&lt;/a&gt; or otherwise skewed distributions. However, GBDTs do have their limitations in &lt;a href="https://dl.acm.org/doi/pdf/10.1145/3292500.3330676"&gt;more realistic&lt;/a&gt; ranking scenarios, which often combine both textual and numerical features. For instance, GBDTs cannot be directly applied to large discrete feature spaces, such as raw document text. They are also, in general, less scalable than neural ranking models. &lt;/p&gt;&lt;p&gt;Therefore, since the TF-Ranking release, our team has significantly deepened the understanding of how best to leverage neural models in ranking with numerical features. This culminated in a Data Augmented Self-Attentive Latent Cross (DASALC) model, described in an &lt;a href="https://research.google/pubs/pub50030/"&gt;ICLR 2021 paper&lt;/a&gt;, which is the first to establish parity, and in some cases statistically significant improvements, of neural ranking models over strong LambdaMART baselines on open LTR datasets. This achievement is made possible through a combination of techniques, which include data augmentation, &lt;a href="https://research.google/pubs/pub49171/"&gt;neural feature transformation&lt;/a&gt;, self-attention for &lt;a href="https://research.google/pubs/pub49364/"&gt;modeling&lt;/a&gt; document interactions, &lt;a href="https://research.google/pubs/pub48321/"&gt;listwise ranking loss&lt;/a&gt;, and model ensembling similar to boosting in GBDTs. The architecture of the DASALC model was entirely implemented using the TF-Ranking library. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Conclusion&lt;/b&gt;&lt;br /&gt;All in all, we believe that the new Keras-based TF-Ranking version will make it easier to conduct neural LTR research and deploy production-grade ranking systems. We encourage everyone to try out the &lt;a href="https://github.com/tensorflow/ranking/releases/tag/v0.4.0"&gt;latest version&lt;/a&gt; and follow &lt;a href="https://github.com/tensorflow/ranking/blob/master/tensorflow_ranking/examples/keras/keras_dnn_tfrecord.py"&gt;this introductory example&lt;/a&gt; for a hands-on experience. While we are very excited about this new release, our research and development journey is far from over, so we will continue to advance our understanding of learning-to-rank problems and share these advances with our users. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Acknowledgements&lt;/b&gt;&lt;br /&gt;&lt;em&gt;This project was only possible thanks to the current and past members of the TF-Ranking team: Honglei Zhuang, ‎Le Yan, Rama Pasumarthi, Rolf Jagerman, Zhen Qin, Shuguang Han, Sebastian Bruch, Nathan Cordeiro, Marc Najork and Patrick McGregor. We also extend special thanks to our collaborators from the Tensorflow team: Zhenyu Tan, Goldie Gadde, Rick Chao, Yuefeng Zhou‎, Hongkun Yu, and Jing Li.&lt;/em&gt;&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/blogspot/gJZg?a=bzF_yMoyjxE:hL_Y6nR_WqQ:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/blogspot/gJZg/~4/bzF_yMoyjxE" height="1" width="1" alt=""/&gt;</content>
    <link rel="replies" type="application/atom+xml" href="http://ai.googleblog.com/feeds/3366307881061046058/comments/default" title="Post Comments"/>
    <link rel="replies" type="text/html" href="http://ai.googleblog.com/2021/07/advances-in-tf-ranking.html#comment-form" title="0 Comments"/>
    <link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/3366307881061046058"/>
    <link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/3366307881061046058"/>
    <link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/blogspot/gJZg/~3/bzF_yMoyjxE/advances-in-tf-ranking.html" title="Advances in TF-Ranking"/>
    <author>
      <name>Google AI</name>
      <uri>http://www.blogger.com/profile/12098626514775266161</uri>
      <email>noreply@blogger.com</email>
      <gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="16" height="16" src="https://img1.blogblog.com/img/b16-rounded.gif"/>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://1.bp.blogspot.com/-IQTHStDjBVk/YQG1FVsdgNI/AAAAAAAAH-Q/WpWEWkgot483yRy6RhUsDvhaiRmPo2MzwCLcBGAsYHQ/s72-w400-h183-c/image1.jpg" height="72" width="72"/>
    <thr:total>0</thr:total>
    <feedburner:origLink>http://ai.googleblog.com/2021/07/advances-in-tf-ranking.html</feedburner:origLink>
  </entry>
  <entry>
    <id>tag:blogger.com,1999:blog-8474926331452026626.post-791697466172075100</id>
    <published>2021-07-23T12:12:00.000-07:00</published>
    <updated>2021-07-23T12:12:34.579-07:00</updated>
    <category scheme="http://www.blogger.com/atom/ns#" term="accessibility"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="Audio"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="Deep Learning"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="Health"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="Research"/>
    <title type="text">Applying Advanced Speech Enhancement in Cochlear Implants</title>
    <content type="html">&lt;span class="byline-author"&gt;Posted by Samuel J. Yang, Research Scientist and Dick Lyon, Principal Scientist, Google Research&lt;/span&gt; &lt;p&gt;For the &lt;a href="https://www.who.int/news-room/fact-sheets/detail/deafness-and-hearing-loss"&gt;~466 million people in the world&lt;/a&gt; who are deaf or hard of hearing, the lack of easy access to accessibility services can be a barrier to participating in spoken conversations encountered daily. While hearing aids can help alleviate this, simply amplifying sound is insufficient for many. One additional option that may be available is the &lt;a href="https://en.wikipedia.org/wiki/Cochlear_implant"&gt;cochlear implant&lt;/a&gt; (CI), which is an electronic device that is surgically inserted into a part of the inner ear, called the &lt;a href="https://simple.wikipedia.org/wiki/Cochlea"&gt;cochlea&lt;/a&gt;, and stimulates the auditory nerve electrically via external sound processors. While many individuals with these cochlear implants can learn to interpret these electrical stimulations as audible speech, the listening experience can be quite varied and particularly challenging in noisy environments.  &lt;/p&gt;&lt;p&gt;Modern cochlear implants drive electrodes with pulsatile signals (i.e., discrete stimulation pulses) that are computed by external sound processors. The main challenge still facing the CI field is how to best process sounds — to convert sounds to pulses on electrodes — in a way that makes them more intelligible to users. Recently, to stimulate progress on this problem, scientists in industry and academia organized a &lt;a href="https://cihackathon.com/"&gt;CI Hackathon&lt;/a&gt; to open the problem up to a wider range of ideas. &lt;/p&gt;&lt;p&gt;In this post, we share exploratory research demonstrating that a speech enhancement preprocessor — specifically, a noise suppressor — can be used at the input of a CI’s processor to enhance users’ understanding of speech in noisy environments. We also discuss how we built on this work in &lt;a href="https://github.com/google-research/google-research/tree/master/cochlear_implant"&gt;our entry&lt;/a&gt; for the CI Hackathon and how we will continue developing this work. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Improving CIs with Noise Suppression&lt;/b&gt;&lt;br/&gt;In 2019, a small internal project demonstrated the benefits of noise suppression at the input of a CI’s processor. In this project, participants listened to 60 pre-recorded and pre-processed audio samples and ranked them by their listening comfort. CI users listened to the audio using their devices' existing strategy for generating electrical pulses. &lt;/p&gt;  &lt;table align="center" style="margin-left: auto; margin-right: auto; text-align: center; width: 60%;"&gt;&lt;tbody&gt;       &lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;em&gt;Audio without background noise&lt;/em&gt;&lt;/td&gt;&lt;/tr&gt;  &lt;tr&gt;&lt;td align="center"&gt;&lt;audio controls="controls" src="https://github.com/google-research/google-research/raw/77439a33a5c2234a1f4aec9e4e986fed4af5bd94/cochlear_implant/sample_audio/clean.wav"&gt;&lt;/audio&gt;&lt;/td&gt;&lt;/tr&gt;        &lt;tr&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;        &lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;em&gt;Audio with background noise&lt;/em&gt;&lt;/td&gt;&lt;/tr&gt;  &lt;tr&gt;&lt;td align="center"&gt;&lt;audio controls="controls" src="https://github.com/google-research/google-research/raw/master/cochlear_implant/sample_audio/noisy.wav"&gt;&lt;/audio&gt;&lt;/td&gt;&lt;/tr&gt;      &lt;tr&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;/tr&gt;     &lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;em&gt;Audio with background noise + noise suppression&lt;/em&gt;&lt;/td&gt;&lt;/tr&gt;  &lt;tr&gt;&lt;td align="center"&gt;&lt;audio controls="controls" src="https://github.com/google-research/google-research/raw/master/cochlear_implant/sample_audio/enhanced.wav"&gt;&lt;/audio&gt;&lt;/td&gt;&lt;/tr&gt;     &lt;/tbody&gt;&lt;/table&gt;&lt;br/&gt;   &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Background audio clip from &lt;a href="https://www.flickr.com/photos/39881443@N06/4160025701/"&gt;“IMG_0991.MOV” by Kenny MacCarthy&lt;/a&gt;, license: &lt;a href="https://creativecommons.org/licenses/by/2.0/"&gt;CC-BY 2.0&lt;/a&gt;.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;     &lt;p&gt;As shown below, both listening comfort and intelligibility usually increased, sometimes dramatically, when speech with noise (the lightest bar) was processed with noise suppression. &lt;/p&gt;   &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/--_N9quZ9cBU/YPrSHh7uklI/AAAAAAAAH74/IGCBSmDqzT0Q_rIFfTFkRmxRx7EQtUnNwCLcBGAsYHQ/s1358/image1.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="1358" data-original-width="1258" height="400" src="https://1.bp.blogspot.com/--_N9quZ9cBU/YPrSHh7uklI/AAAAAAAAH74/IGCBSmDqzT0Q_rIFfTFkRmxRx7EQtUnNwCLcBGAsYHQ/w370-h400/image1.png" width="370" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;CI users in an early research study have improved listening comfort — qualitatively scored from "very poor" (0.0) to "OK" (0.5) to "very good" (1.0) — and speech intelligibility (i.e., the fraction of words in a sentence correctly transcribed) when trying to listen to noisy audio samples of speech with noise suppression applied.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;p&gt;For the CI Hackathon, we &lt;a href="https://github.com/google-research/google-research/tree/master/cochlear_implant"&gt;built on the project&lt;/a&gt; above, continuing to leverage our use of a noise suppressor while additionally exploring an approach to compute the pulses too &lt;/p&gt;&lt;p&gt;&lt;b&gt;Overview of the Processing Approach&lt;/b&gt;&lt;br/&gt;The hackathon considered a CI with 16 electrodes. Our approach decomposes the audio into 16 overlapping frequency bands, corresponding to the positions of the electrodes in the cochlea.  Next, because the dynamic range of sound easily spans multiple orders of magnitude more than what we expect the electrodes to represent, we aggressively compress the dynamic range of the signal by applying &lt;a href="https://doi.org/10.1109/ICASSP.2017.7953242"&gt;"per-channel energy normalization"&lt;/a&gt; (PCEN). Finally, the range-compressed signals are used to create the electrodogram (i.e., what the CI displays on the electrodes). &lt;/p&gt;&lt;p&gt;In addition, the hackathon required a submission be evaluated in multiple audio categories, including music, which is an important but notoriously difficult category of sounds for CI users to enjoy. However, the speech enhancement network was trained to suppress non-speech sounds, including both noise and music, so we needed to take extra measures to avoid suppressing instrumental music (note that in general, music suppression might be preferred by some users in certain contexts). To do this, we created a “mix” of the original audio with the noise-suppressed audio so that enough of the music would pass through to remain audible. We varied in real-time the fraction of original audio mixed from 0% to 40% (0% if all of the input is estimated as speech, up to 40% as more of the input is estimated as non-speech) based on the estimate from the open-source &lt;a href="https://www.tensorflow.org/hub/tutorials/yamnet"&gt;YAMNet&lt;/a&gt; classifier on every ~1 second window of audio of whether the input is speech or non-speech. &lt;/p&gt;&lt;p&gt;&lt;b&gt;The Conv-TasNet Speech Enhancement Model&lt;/b&gt;&lt;br/&gt;To implement a speech enhancement module that suppresses non-speech sounds, such as noise and music, we use the &lt;a href="https://doi.org/10.1109/TASLP.2019.2915167"&gt;Conv-TasNet&lt;/a&gt; model, which can separate different kinds of sounds. To start, the raw audio waveforms are transformed and processed into a form that can be used by a neural network. The model transforms short, 2.5 millisecond frames of input audio with a learnable analysis transform to generate features optimized for sound separation. The network then produces two “masks” from those features: one mask for speech and one mask for noise. These masks indicate the degree to which each feature corresponds to either speech or noise. Separated speech and noise are reconstructed back to the audio domain by multiplying the masks with the analysis features, applying a synthesis transform back to audio-domain frames, and stitching the resulting short frames together. As a final step, the speech and noise estimates are processed by a &lt;a href="https://research.google/pubs/pub47816/"&gt;mixture consistency layer&lt;/a&gt;, which improves the quality of the estimated waveforms by ensuring that they sum up to the original input mixture waveform.  &lt;/p&gt;&lt;p&gt;  &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-ZV96R5C1OVk/YPrSTU1EWDI/AAAAAAAAH78/CDaBE7sdn2ckUZgrUg32Ohv-VXnuog2SQCLcBGAsYHQ/s1999/image2.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="427" data-original-width="1999" height="136" src="https://1.bp.blogspot.com/-ZV96R5C1OVk/YPrSTU1EWDI/AAAAAAAAH78/CDaBE7sdn2ckUZgrUg32Ohv-VXnuog2SQCLcBGAsYHQ/w640-h136/image2.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Block diagram of the speech enhancement system, which is based on Conv-TasNet.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;p&gt;The model is both &lt;a href="https://en.wikipedia.org/wiki/Causal_system"&gt;causal&lt;/a&gt; and low latency: for each 2.5 milliseconds of input audio, the model produces estimates of separated speech and noise, and thus could be used in real-time. For the hackathon, to demonstrate what could be possible with increased compute power in future hardware, we chose to use a model variant with 2.9 million parameters. This model size is too large to be practically implemented in a CI today, but demonstrates what kind of performance would be possible with more capable hardware in the future. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Listening to the Results&lt;/b&gt;&lt;br/&gt;As we optimized our models and overall solution, we used the hackathon-provided &lt;a href="https://en.wikipedia.org/wiki/Vocoder"&gt;vocoder&lt;/a&gt; (which required a fixed temporal spacing of electrical pulses) to produce audio simulating what CI users might perceive. We then conducted blind A-B listening tests as typical hearing users.  &lt;/p&gt;&lt;p&gt;Listening to the vocoder simulations below, the speech in the reconstructed sounds — from the vocoder processing the electrodograms — is reasonably intelligible when the input sound doesn't contain too much background noise, however there is still room to improve the clarity of the speech. Our submission performed well in the speech-in-noise category and achieved second place overall. &lt;/p&gt;   &lt;table align="center" style="margin-left: auto; margin-right: auto; text-align: center; width: 60%;"&gt;&lt;tbody&gt;       &lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;em&gt;Simulated audio with fixed temporal spacing&lt;/em&gt;&lt;/td&gt;&lt;/tr&gt;  &lt;tr&gt;&lt;td align="center"&gt;&lt;audio controls="controls" src="https://github.com/google-research/google-research/raw/7b9d963a56f58ae7e036243e6a5b75d3294175ea/cochlear_implant/sample_audio/enhanced_vocoded.wav"&gt;&lt;/audio&gt;&lt;/td&gt;&lt;/tr&gt;            &lt;/tbody&gt;&lt;/table&gt;   &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Vocoder simulation of what CI users might perceive from audio from an electrodogram with fixed temporal spacing, with background noise and noise suppression applied.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;p&gt;A bottleneck on quality is that the fixed temporal spacing of stimulation pulses sacrifices fine-time structure in the audio. A change to the processing to produce pulses timed to peaks in the filtered sound waveforms captures more information about the pitch and structure of sound than is conventionally represented in implant stimulation patterns. &lt;/p&gt;   &lt;table align="center" style="margin-left: auto; margin-right: auto; text-align: center; width: 60%;"&gt;&lt;tbody&gt;       &lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;em&gt;Simulated audio with adaptive spacing and fine time structure&lt;/em&gt;&lt;/td&gt;&lt;/tr&gt;  &lt;tr&gt;&lt;td align="center"&gt;&lt;audio controls="controls" src="https://github.com/google-research/google-research/raw/7b9d963a56f58ae7e036243e6a5b75d3294175ea/cochlear_implant/sample_audio/enhanced_vocoded_fine_time.wav"&gt;&lt;/audio&gt;&lt;/td&gt;&lt;/tr&gt;            &lt;/tbody&gt;&lt;/table&gt;   &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Vocoder simulation, using the same vocoder as above, but on an electrodogram from the modified processing that synchronizes stimulation pulses to peaks of the sound waveform.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;It's important to note that this second vocoder output is overly optimistic about how well it might sound to a real CI user. For instance, the simple vocoder used here does not model how current spread in the cochlea blurs the stimulus, making it harder to resolve different frequencies. But this at least suggests that preserving fine-time structure is valuable and that the electrodogram itself is not the bottleneck. &lt;/p&gt;&lt;p&gt;Ideally, all processing approaches would be evaluated by a broad range of CI users, with the electrodograms implemented directly on their CIs rather than relying upon vocoder simulations.  &lt;/p&gt;&lt;p&gt;&lt;b&gt;Conclusion and a Call to Collaborate&lt;/b&gt;&lt;br/&gt;We are planning to follow up on this experience in two main directions. First, we plan to explore the application of noise suppression to other hearing-accessibility modalities, including hearing aids, transcription, and vibrotactile sensory substitution. Second, we'll take a deeper dive into the creation of electrodogram patterns for cochlear implants, exploiting fine temporal structure that is not accommodated in the usual CIS (continous interleaved sampling) patterns that are standard in the industry. According to &lt;a href="https://doi.org/10.1109/79.708543"&gt;Louizou&lt;/a&gt;: “It remains a puzzle how some single-channel patients can perform so well given the limited spectral information they receive''. Therefore, using &lt;a href="https://doi.org/10.1109/TBCAS.2012.2219530"&gt;fine temporal structure&lt;/a&gt; might be a &lt;a href="http://www.machinehearing.org/"&gt;critical step&lt;/a&gt; towards achieving an improved CI experience. &lt;/p&gt;&lt;p&gt;Google is committed to building technology &lt;a href="http://g.co/pwd"&gt;with and for people with disabilities&lt;/a&gt;. If you are interested in collaborating to improve the state of the art in cochlear implants (or hearing aids), please reach out to &lt;a href="mailto:ci-collaborators@googlegroups.com"&gt;ci-collaborators@googlegroups.com&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Acknowledgements&lt;/b&gt;&lt;br/&gt;&lt;em&gt;We would like to thank the Cochlear Impact hackathon organizers for giving us this opportunity and partnering with us. The participating team within Google is Samuel J. Yang, Scott Wisdom, Pascal Getreuer, Chet Gnegy, Mihajlo Velimirović, Sagar Savla, and Richard F. Lyon with guidance from Dan Ellis and Manoj Plakal.&lt;/em&gt;&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/blogspot/gJZg?a=mxT65-EVcFU:53DYr2CCzJo:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/blogspot/gJZg/~4/mxT65-EVcFU" height="1" width="1" alt=""/&gt;</content>
    <link rel="replies" type="application/atom+xml" href="http://ai.googleblog.com/feeds/791697466172075100/comments/default" title="Post Comments"/>
    <link rel="replies" type="text/html" href="http://ai.googleblog.com/2021/07/applying-advanced-speech-enhancement-in.html#comment-form" title="0 Comments"/>
    <link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/791697466172075100"/>
    <link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/791697466172075100"/>
    <link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/blogspot/gJZg/~3/mxT65-EVcFU/applying-advanced-speech-enhancement-in.html" title="Applying Advanced Speech Enhancement in Cochlear Implants"/>
    <author>
      <name>Google AI</name>
      <uri>http://www.blogger.com/profile/12098626514775266161</uri>
      <email>noreply@blogger.com</email>
      <gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="16" height="16" src="https://img1.blogblog.com/img/b16-rounded.gif"/>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://1.bp.blogspot.com/--_N9quZ9cBU/YPrSHh7uklI/AAAAAAAAH74/IGCBSmDqzT0Q_rIFfTFkRmxRx7EQtUnNwCLcBGAsYHQ/s72-w370-h400-c/image1.png" height="72" width="72"/>
    <thr:total>0</thr:total>
    <feedburner:origLink>http://ai.googleblog.com/2021/07/applying-advanced-speech-enhancement-in.html</feedburner:origLink>
  </entry>
  <entry>
    <id>tag:blogger.com,1999:blog-8474926331452026626.post-8328077619605477836</id>
    <published>2021-07-22T09:58:00.000-07:00</published>
    <updated>2021-07-22T09:58:47.986-07:00</updated>
    <category scheme="http://www.blogger.com/atom/ns#" term="Deep Learning"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="Health"/>
    <title type="text">Multi-task Prediction of Organ Dysfunction in ICUs</title>
    <content type="html">&lt;span class="byline-author"&gt;Posted by Subhrajit Roy, Research Scientist and Diana Mincu, Research Software Engineer, Google Research&lt;/span&gt; &lt;p&gt;The intensive care unit (ICU) of a hospital looks after the most medically vulnerable patients, many of whom require organ support, such as &lt;a href="https://en.wikipedia.org/wiki/Mechanical_ventilation"&gt;mechanical ventilation&lt;/a&gt; or &lt;a href="https://en.wikipedia.org/wiki/Dialysis"&gt;dialysis&lt;/a&gt;. While always critical, the demand on ICU services during the COVID-19 pandemic has further underscored the importance of data-driven decision-making in healthcare. Furthermore, the ability to accurately predict the clinical outcomes of ICU patients has the potential  to guide therapy and may inform decisions about most effective care, including staffing and triage support.  &lt;/p&gt;&lt;p&gt;Applying machine learning (ML) to electronic health records (EHRs) has shown promise in predicting clinical outcomes. However, many of these ML models are based on &lt;em&gt;single-task&lt;/em&gt; learning (ST), where the models are trained only to predict a specific adverse event, such as an organ dysfunction or the need for a life-support intervention. Of greater benefit would be to train &lt;em&gt;multi-task&lt;/em&gt; models, which take into account a variety of competing risks along with the interdependencies between organ systems that factor into patient outcomes in a realistic setting. &lt;/p&gt;&lt;p&gt;In “&lt;a href="https://academic.oup.com/jamia/advance-article/doi/10.1093/jamia/ocab101/6307184"&gt;Multi-task prediction of organ dysfunction in the ICU using sequential sub-network routing&lt;/a&gt;”, we propose a multi-task learning (MTL) architecture, called Sequential Sub-Network Routing (SeqSNR), that better captures the complexity of a realistic setting. Inspired by a clinician's holistic approach to diagnosing problems, SeqSNR is designed to use flexible parameter sharing and routing to find related tasks and encourage cross-learning between them. We successfully applied SeqSNR to the task of continuous adverse event prediction in an ICU setting and showed advantages over single-task and naïve multi-tasking, especially in low training data scenarios.  &lt;/p&gt;&lt;p&gt;&lt;b&gt;Data and Labels&lt;/b&gt;&lt;br /&gt;In this study, we used the freely available, open access, de-identified &lt;a href="https://mimic.mit.edu/"&gt;MIMIC-III&lt;/a&gt; EHR dataset, which includes a patient cohort consisting of 36,498 adults across 52,038 critical care admissions at the Beth Israel Deaconess Medical Center between 2001 and 2012. Similar to our &lt;a href="https://ai.googleblog.com/2018/05/deep-learning-for-electronic-health.html"&gt;previous&lt;/a&gt; &lt;a href="https://ai.googleblog.com/2020/04/a-step-towards-protecting-patients-from.html"&gt;studies&lt;/a&gt;, we employed a &lt;a href="https://www.nature.com/articles/s41746-018-0029-1"&gt;version&lt;/a&gt; of the MIMIC-III dataset that was mapped to the &lt;a href="https://www.hl7.org/fhir/overview.html"&gt;Fast Healthcare Interoperability Resource&lt;/a&gt; (FHIR) standard and used a comprehensive set of features, including a sequence of vital signs, laboratory results, past medications, procedures, diagnoses, and more.  &lt;/p&gt;&lt;p&gt;The MIMIC-III database contains multi-modal recordings from ICU patients. Unlike most datasets in ML, the input and targets are often not explicitly defined and must be inferred from the data. So, using a combination of automated rule-based methods and clinical review, we defined a suite of diverse endpoints, including critical care interventions, specific organ dysfunctions, and overall patient outcomes.  &lt;/p&gt;&lt;p&gt;The task given to the model was to predict the onset of a selection of adverse events within 24–48 hours for every hour after a patient’s admission into the ICU. The defined adverse events included &lt;a href="https://en.wikipedia.org/wiki/Acute_kidney_injury"&gt;acute kidney injury&lt;/a&gt; (AKI), &lt;a href="https://en.wikipedia.org/wiki/Renal_replacement_therapy#Continuous_Renal_Replacement_Therapy_(CRRT)"&gt;continuous renal replacement therapy&lt;/a&gt; (CRRT) dialysis, administration of &lt;a href="https://en.wikipedia.org/wiki/Antihypotensive_agent"&gt;vasopressors&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/Inotrope"&gt;inotropes&lt;/a&gt;, mechanical ventilation (MV), mortality, and remaining length of stay (LoS).  &lt;/p&gt;&lt;p&gt;&lt;b&gt;The SeqSNR Algorithm&lt;/b&gt;&lt;br /&gt;While multi-task learning captures the interdependencies between organ systems and balances competing risks, it can be challenging to implement successfully. In practice, jointly-trained tasks often impair one another, an effect called “&lt;a href="https://arxiv.org/abs/2007.10185"&gt;negative transfer&lt;/a&gt;”. The intuition behind SeqSNR was that modular ‘sub-networks’ would mitigate this issue by automatically optimizing how information is shared across multiple tasks.  &lt;/p&gt;&lt;p&gt;SeqSNR is a time series adaptation of the &lt;a href="https://ojs.aaai.org//index.php/AAAI/article/view/3788"&gt;SNR architecture&lt;/a&gt; and is a combination of a deep embedding layer followed by stacked &lt;a href="https://en.wikipedia.org/wiki/Recurrent_neural_network"&gt;recurrent neural network&lt;/a&gt; (RNN) layers. Modularisation is achieved by splitting  both the embedding layer and the RNN stack into multiple modules connected by routing variables that are learned during the training phase. The routing connections are always created between blocks in one layer and the next. This approach minimizes negative transfer by ensuring that data of low relevance to a particular task layer is filtered out. In essence, this means that each task utilizes a different path through the model.  &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-dSlDDAcwWy4/YPhGmmI7rYI/AAAAAAAAH7Y/wmhlt332d5U0p-shL8a1I-PGQrU_mQkiACLcBGAsYHQ/s1054/image5.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="520" data-original-width="1054" height="316" src="https://1.bp.blogspot.com/-dSlDDAcwWy4/YPhGmmI7rYI/AAAAAAAAH7Y/wmhlt332d5U0p-shL8a1I-PGQrU_mQkiACLcBGAsYHQ/w640-h316/image5.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;A high-level overview of the SeqSNR architecture.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;b&gt;Findings&lt;/b&gt;&lt;br /&gt;SeqSNR shows a modest improvement in discriminative performance overall relative to single-task and naïve multitasking. However, it's performance improvement is more significant in scenarios with few training labels. &lt;/p&gt;&lt;p&gt;Because the prevalence of different outcomes varied widely in the dataset (e.g. ~38% of patients had MV, but CRRT dialysis is present for only ~3%), many accuracy metrics are not suitable. Instead, we report the area under the &lt;a href="https://en.wikipedia.org/wiki/Precision_and_recall"&gt;precision recall&lt;/a&gt; curve (AU PRC), which is more reliable given imbalanced data. Moreover, we performed the &lt;a href="https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test"&gt;Wilcoxon Signed Rank Tests&lt;/a&gt; to draw statistically significant conclusions for pairwise comparisons of ST learning, &lt;a href="https://link.springer.com/chapter/10.1007/978-1-4615-5529-2_5"&gt;shared-bottom&lt;/a&gt; (SB) multi-task learning (i.e., naïve multi-task learning), and SeqSNR across &lt;a href="https://www.taylorfrancis.com/books/mono/10.1201/9780429246593/introduction-bootstrap-bradley-efron-tibshirani"&gt;bootstrapped samples&lt;/a&gt; from the held-out test set. The performance differences between the three architectures were modest, but SeqSNR outperformed both ST and SB in four out of six tasks (p-values are reported in the &lt;a href="https://academic.oup.com/jamia/advance-article/doi/10.1093/jamia/ocab101/6307184"&gt;paper&lt;/a&gt;). &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-5IBcZIbKSvM/YPhGypVHCmI/AAAAAAAAH7c/hNpBFUQDx0Mg7FRzWjxo8VH_RMX4bBuxACLcBGAsYHQ/s1532/image4.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="992" data-original-width="1532" height="414" src="https://1.bp.blogspot.com/-5IBcZIbKSvM/YPhGypVHCmI/AAAAAAAAH7c/hNpBFUQDx0Mg7FRzWjxo8VH_RMX4bBuxACLcBGAsYHQ/w640-h414/image4.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Comparison of single task (ST), shared bottom (SB) and SeqSNR performance on the MIMIC-III dataset.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;b&gt;Label Efficiency&lt;/b&gt;&lt;br /&gt;We hypothesized that multi-task learning could assist in low-data scenarios by using easy-to-label auxiliary tasks to boost the performance of the main tasks. We formulated prediction tasks with only a portion of the training labels available for the primary prediction task, but kept the entire dataset for the “helper tasks”. The latter are chosen because they are reliably encoded in the EHR and are straightforward to timestamp. An example of such a helper task is length of stay,  since the start and end of admissions are accurately timestamped in MIMIC-III. On the other hand, the start and end of mechanical ventilation events are not reliably timestamped. So, we defined a set of rules based on expert-defined heuristics to determine the ventilation times using multiple sources of mechanical ventilator–related settings along with physiological measurements in the EHR dataset that are indicative of MV. &lt;/p&gt;&lt;p&gt;The development of these rules for a new clinical endpoint was time-consuming and involved manual review of the dataset by experts. The difficulty in exhaustively labeling the dataset led us to test the model performance with only 1–10% of the data labeled, which resulted in a decline in model performance. The “helper tasks” are useful in this scenario since they are 100% labeled and can be used with the primary tasks (1–10% labeled) to jointly train the multi-task model for improved overall performance.  &lt;/p&gt;&lt;p&gt;We chose AKI, mechanical ventilation, CRRT Dialysis, and vasoactive medications as primary endpoints using 1%, 5%, and 10% of the training labels, along with 100% of labels for the helper tasks — labs and vitals, mortality, and LoS. Performance of both ST and SeqSNR decreased as the percentage of labels for the primary endpoint was reduced, but SeqSNR outperformed ST across all tasks and all training data reduction percentages, with a statistically significant boost in performance for all cases. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-u60t3oAhlzg/YPhHQLNGopI/AAAAAAAAH7o/BeR1I4a3ITkrXj7W4-mpOTACPab3cQDaACLcBGAsYHQ/s1240/LabelEfficiency.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="986" data-original-width="1240" height="508" src="https://1.bp.blogspot.com/-u60t3oAhlzg/YPhHQLNGopI/AAAAAAAAH7o/BeR1I4a3ITkrXj7W4-mpOTACPab3cQDaACLcBGAsYHQ/w640-h508/LabelEfficiency.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Label efficiency results showing the discriminative performance when the training dataset for the primary endpoint is reduced to 1%, 5% and 10% while the helper tasks have access to all training labels.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;This is a useful finding, given the difficulties of annotating endpoint labels in EHR datasets, which frequently necessitates human evaluation by doctors. The ability to use numerous endpoints, some of which may be easier to label (like duration of stay or mortality), could lessen the need for manual curation on more difficult endpoints that are annotated differently (like  mechanical ventilation).  &lt;/p&gt;&lt;p&gt;&lt;b&gt;Subgroup Performance&lt;/b&gt;&lt;br /&gt;While the version of the MIMIC-III dataset used contained labels for gender and age, it did not contain information on race and the information on ethnicity was limited. We computed the performance of all selected models across age and gender subgroups. We observed that in the scenarios with few instances in the dataset, the MTL models (both SB models and SeqSNR) often outperform ST. Even though there are exceptions, on average all models seem to be relatively balanced across age and gender subgroups. We invite the reader to refer to the &lt;a href="https://academic.oup.com/jamia/advance-article/doi/10.1093/jamia/ocab101/6307184#265336640"&gt;supplemental section&lt;/a&gt; of our paper for a detailed performance breakdown. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Next Steps&lt;/b&gt;&lt;br /&gt;This work is a proof of concept for SeqSNR on a set of canonical EHR prediction tasks. The code for this architecture is publicly available &lt;a href="https://github.com/google/ehr-predictions"&gt;here&lt;/a&gt;. And will hopefully stimulate further research in EHR multi-tasking and other deep learning architectures inspired by clinical reasoning.   &lt;/p&gt;&lt;p&gt;In future, it will be important to evaluate the  performance of SeqSNR on different combinations of tasks, different time horizons and different datasets. One other area of potential growth in this project is to expand subgroup analysis by including datasets with additional population information, race, ethnicity, etc.&amp;nbsp;Another area we are exploring is expanding subgroup analysis by including datasets with additional population information, such as race, ethnicity, etc. We also emphasize that these are prototype models designed to showcase methodologies, and more rigorous evaluation would be needed to bring these tools into deployment.  &lt;/p&gt;&lt;p&gt;&lt;b&gt;Acknowledgements&lt;/b&gt;&lt;br /&gt;&lt;em&gt;This work involved collaborative efforts from a multidisciplinary team of researchers, software engineers, clinicians, and cross-functional contributors. We thank our co-authors:  Eric Loreaux, Anne Mottram, Ivan Protsyuk, Natalie Harris, Sebastien Baur, Yuan Xue, Jessica Schrouff, Ali Connell, Alan Karthikesalingam, Martin Seneviratne from Google, Nenad Tomasev from Deepmind, and Hugh Montgomery from University College London. We also thank Zhe Zhao from Google Research and Kathryn Rough, Cian Hughes, Megumi Morigami and Doris Wong from Google Health for their input and review, and the MIMIC team for curating this open access dataset for the research community. &lt;/em&gt;&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/blogspot/gJZg?a=4G4KoZEbE9Y:PB9MFTFbdts:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/blogspot/gJZg/~4/4G4KoZEbE9Y" height="1" width="1" alt=""/&gt;</content>
    <link rel="replies" type="application/atom+xml" href="http://ai.googleblog.com/feeds/8328077619605477836/comments/default" title="Post Comments"/>
    <link rel="replies" type="text/html" href="http://ai.googleblog.com/2021/07/multi-task-prediction-of-organ.html#comment-form" title="0 Comments"/>
    <link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/8328077619605477836"/>
    <link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/8328077619605477836"/>
    <link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/blogspot/gJZg/~3/4G4KoZEbE9Y/multi-task-prediction-of-organ.html" title="Multi-task Prediction of Organ Dysfunction in ICUs"/>
    <author>
      <name>Google AI</name>
      <uri>http://www.blogger.com/profile/12098626514775266161</uri>
      <email>noreply@blogger.com</email>
      <gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="16" height="16" src="https://img1.blogblog.com/img/b16-rounded.gif"/>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://1.bp.blogspot.com/-dSlDDAcwWy4/YPhGmmI7rYI/AAAAAAAAH7Y/wmhlt332d5U0p-shL8a1I-PGQrU_mQkiACLcBGAsYHQ/s72-w640-h316-c/image5.png" height="72" width="72"/>
    <thr:total>0</thr:total>
    <feedburner:origLink>http://ai.googleblog.com/2021/07/multi-task-prediction-of-organ.html</feedburner:origLink>
  </entry>
  <entry>
    <id>tag:blogger.com,1999:blog-8474926331452026626.post-4133940225737160804</id>
    <published>2021-07-19T11:40:00.005-07:00</published>
    <updated>2021-07-20T11:39:32.632-07:00</updated>
    <category scheme="http://www.blogger.com/atom/ns#" term="conferences"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="ICML"/>
    <title type="text">Google at ICML 2021</title>
    <content type="html">&lt;span class="byline-author"&gt;Posted by Cat Armato and Jaqui Herman, Program Managers&lt;/span&gt; &lt;p&gt;Groups across Google are actively pursuing research across the field of machine learning, ranging from theory to application. With scalable tools and architectures, we build machine learning systems to solve deep scientific and engineering challenges in areas of language, music, visual processing, and more. &lt;/p&gt;&lt;p&gt;Google is proud to be a &lt;a href="https://icml.cc/Conferences/2021/Sponsors"&gt;Platinum Sponsor&lt;/a&gt; of the thirty-eighth &lt;a href="https://icml.cc/Conferences/2021"&gt;International Conference on Machine Learning&lt;/a&gt; (ICML 2021), a premier annual event happening this week. As a leader in machine learning research — with over 100 accepted publications and Googlers participating in workshops — we look forward to our continued partnership with the broader machine learning research community. &lt;/p&gt;&lt;p&gt;Registered for ICML 2021? We hope you’ll visit the Google virtual booth to learn more about the exciting work, creativity, and fun that goes into solving a portion of the field’s most interesting challenges. Take a look below to learn more about the Google research being presented at ICML 2021 &lt;b&gt;(Google affiliations in bold).&lt;/b&gt;&lt;/p&gt;&lt;p&gt;     &lt;b&gt;&lt;span style="text-decoration:underline;"&gt;Organizing Committee&lt;/span&gt;&lt;/b&gt;&lt;br/&gt;  ICML Board Members include: &lt;b&gt;&lt;em&gt;Corinna Cortes, Hugo Larochelle, Shakir Mohamed&lt;/em&gt;&lt;/b&gt;&lt;br/&gt;    ICML Emeritus Board includes: &lt;b&gt;&lt;em&gt;William Cohen, Andrew McCallum&lt;/em&gt;&lt;/b&gt;&lt;br/&gt;    Tutorial Co-Chair member: &lt;b&gt;&lt;em&gt;Quoc Lee&lt;/em&gt;&lt;/b&gt;&lt;/p&gt;  &lt;p&gt;&lt;b&gt;&lt;span style="text-decoration:underline;"&gt;Publications&lt;/span&gt;&lt;/b&gt;&lt;br/&gt;  &lt;a href="http://proceedings.mlr.press/v139/dong21a/dong21a.pdf"&gt;Attention Is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Yihe Dong, Jean-Baptiste Cordonnier, Andreas Loukas&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/leibo21a/leibo21a.pdf"&gt;Scalable Evaluation of Multi-agent Reinforcement Learning with Melting Pot&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Joel Z. Leibo, Edgar Duéñez-Guzmán, Alexander Sasha Vezhnevets, John P. Agapiou, Peter Sunehag, Raphael Koster, Jayd Matyas, Charles Beattie,&lt;b&gt; Igor Mordatch&lt;/b&gt;, Thore Graepel       &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/xiao21b/xiao21b.pdf"&gt;On the Optimality of Batch Policy Optimization Algorithms&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Chenjun Xiao, &lt;/b&gt;Yifan Wu, Tor Lattimore,&lt;b&gt; Bo Dai, Jincheng Mei&lt;/b&gt;, Lihong Li*, Csaba Szepesvari, &lt;b&gt;Dale Schuurmans&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/scetbon21a/scetbon21a.pdf"&gt;Low-Rank Sinkhorn Factorization&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Meyer Scetbon, &lt;b&gt;Marco Cuturi&lt;/b&gt;, Gabriel Peyré   &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/grathwohl21a/grathwohl21a.pdf"&gt;Oops I Took A Gradient: Scalable Sampling for Discrete Distributions&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Will Grathwohl, Kevin Swersky, Milad Hashemi, David Duvenaud&lt;/b&gt;, Chris J. Maddison   &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/farahmand21a/farahmand21a.pdf"&gt;PID Accelerated Value Iteration Algorithm&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Amir-Massoud Farahmand&lt;b&gt;, Mohammad Ghavamzadeh&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/saha21b/saha21b.pdf"&gt;Dueling Convex Optimization&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Aadirupa Saha, &lt;b&gt;Tomer Koren, Yishay Mansour&lt;/b&gt;  &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/izmailov21a/izmailov21a.pdf"&gt;What Are Bayesian Neural Network Posteriors Really Like?&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Pavel Izmailov, &lt;b&gt;Sharad Vikram, Matthew D. Hoffman&lt;/b&gt;, Andrew Gordon Wilson   &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/dadashi21a/dadashi21a.pdf"&gt;Offline Reinforcement Learning with Pseudometric Learning&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Robert Dadashi&lt;/b&gt;, Shideh Rezaeifar,&lt;b&gt; Nino Vieillard, Léonard Hussenot, Olivier Pietquin, Matthieu Geist&lt;/b&gt;  &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/ceron21a/ceron21a.pdf"&gt;Revisiting Rainbow: Promoting More Insightful and Inclusive Deep Reinforcement Learning Research&lt;/a&gt; &lt;em&gt;(see &lt;a href="https://ai.googleblog.com/2021/07/reducing-computational-cost-of-deep.html"&gt;blog post&lt;/a&gt;)&lt;/em&gt;&lt;br/&gt;&lt;em&gt;  Johan S. Obando-Ceron, &lt;b&gt;Pablo Samuel Castro&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/ghasemipour21a/ghasemipour21a.pdf"&gt;EMaQ: Expected-Max Q-Learning Operator for Simple Yet Effective Offline and Online RL&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Seyed Kamyar Seyed Ghasemipour*, &lt;b&gt;Dale Schuurmans, Shixiang Shane Gu&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/frerix21a/frerix21a.pdf"&gt;Variational Data Assimilation with a Learned Inverse Observation Operator&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Thomas Frerix, Dmitrii Kochkov, Jamie A. Smith&lt;/b&gt;, Daniel Cremers, &lt;b&gt;Michael P. Brenner, Stephan Hoyer&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/ruiz-garcia21a/ruiz-garcia21a.pdf"&gt;Tilting the Playing Field: Dynamical Loss Functions for Machine Learning&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Miguel Ruiz-Garcia, Ge Zhang, &lt;b&gt;Samuel S. Schoenholz&lt;/b&gt;, Andrea J. Liu       &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/rybkin21b/rybkin21b.pdf"&gt;Model-Based Reinforcement Learning via Latent-Space Collocation&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Oleh Rybkin, Chuning Zhu, Anusha Nagabandi, Kostas Daniilidis, &lt;b&gt;Igor Mordatch&lt;/b&gt;, Sergey Levine       &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/sander21a/sander21a.pdf"&gt;Momentum Residual Neural Networks&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Michael E. Sander, Pierre Ablin, &lt;b&gt;Mathieu Blondel&lt;/b&gt;, Gabriel Peyré       &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/tay21b/tay21b.pdf"&gt;OmniNet: Omnidirectional Representations from Transformers&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Yi Tay, Mostafa Dehghani, Vamsi Aribandi, Jai Gupta, Philip Pham, Zhen Qin, Dara Bahri, Da-Cheng Juan, Donald Metzler&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/tay21a/tay21a.pdf"&gt;Synthesizer: Rethinking Self-Attention for Transformer Models&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, Che Zheng&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/verma21a/verma21a.pdf"&gt;Towards Domain-Agnostic Contrastive Learning&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Vikas Verma, Minh-Thang Luong&lt;/b&gt;, Kenji Kawaguchi,&lt;b&gt; Hieu Pham, Quoc V. Le&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2006.04222.pdf"&gt;Randomized Entity-wise Factorization for Multi-agent Reinforcement Learning&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Shariq Iqbal, Christian A. Schroeder de Witt, Bei Peng, Wendelin Böhmer, Shimon Whiteson, &lt;b&gt;Fei Sha&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/wu21c/wu21c.pdf"&gt;LIME: Learning Inductive Bias for Primitives of Mathematical Reasoning&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Yuhuai Wu, &lt;b&gt;Markus Rabe,&lt;/b&gt; Wenda Li, Jimmy Ba, Roger Grosse, &lt;b&gt;Christian Szegedy&lt;/b&gt;   &lt;span style="text-decoration:underline;"&gt; &lt;/span&gt;  &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/ndousse21a/ndousse21a.pdf"&gt;Emergent Social Learning via Multi-agent Reinforcement Learning&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Kamal Ndousse, &lt;b&gt;Douglas Eck, Sergey Levine, Natasha Jaques&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/du21b/du21b.pdf"&gt;Improved Contrastive Divergence Training of Energy-Based Models&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Yilun Du, Shuang Li, Joshua Tenenbaum, &lt;b&gt;Igor Mordatch&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/jiang21k/jiang21k.pdf"&gt;Characterizing Structural Regularities of Labeled Data in Overparameterized Models&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Ziheng Jiang*, &lt;b&gt;Chiyuan Zhang, Kunal Talwar, Michael Mozer&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/chebotar21a/chebotar21a.pdf"&gt;Actionable Models: Unsupervised Offline Reinforcement Learning of Robotic Skills&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Yevgen Chebotar, Karol Hausman, Yao Lu, Ted Xiao, Dmitry Kalashnikov, Jake Varley, Alex Irpan, Benjamin Eysenbach, Ryan Julian, Chelsea Finn, Sergey Levine&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/filos21a/filos21a.pdf"&gt;PsiPhi-Learning: Reinforcement Learning with Demonstrations using Successor Features and Inverse Temporal Difference Learning&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Angelos Filos, Clare Lyle, Yarin Gal, Sergey Levine, &lt;b&gt;Natasha Jaques&lt;/b&gt;, Gregory Farquhar       &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/tan21a/tan21a.pdf"&gt;EfficientNetV2: Smaller Models and Faster Training&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Mingxing Tan, Quoc V. Le&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/vicol21a/vicol21a.pdf"&gt;Unbiased Gradient Estimation in Unrolled Computation Graphs with Persistent Evolution Strategies&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Paul Vicol, Luke Metz, Jascha Sohl-Dickstein&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/yuan21d/yuan21d.pdf"&gt;Federated Composite Optimization&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Honglin Yuan*, &lt;b&gt;Manzil Zaheer, Sashank Reddi&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/chierichetti21a/chierichetti21a.pdf"&gt;Light RUMs&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Flavio Chierichetti, &lt;b&gt;Ravi Kumar, Andrew Tomkins&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/davis21a/davis21a.pdf"&gt;Catformer: Designing Stable Transformers via Sensitivity Analysis&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Jared Quincy Davis, Albert Gu,&lt;b&gt; Krzysztof Choromanski&lt;/b&gt;, Tri Dao, Christopher Re, &lt;b&gt;Chelsea Finn&lt;/b&gt;, Percy Liang       &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/yang21h/yang21h.pdf"&gt;Representation Matters: Offline Pretraining for Sequential Decision Making&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Mengjiao Yang, Ofir Nachum&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/choi21b/choi21b.pdf"&gt;Variational Empowerment as Representation Learning for Goal-Conditioned Reinforcement Learning&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Jongwook Choi*, Archit Sharma*, Honglak Lee, &lt;b&gt;Sergey Levine, Shixiang Shane Gu&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/chung21a/chung21a.pdf"&gt;Beyond Variance Reduction: Understanding the True Impact of Baselines on Policy Optimization&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Wesley Chung,&lt;b&gt; &lt;/b&gt;Valentin Thomas, &lt;b&gt;Marlos C. Machado, Nicolas Le Roux&lt;/b&gt;          &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/wadia21a/wadia21a.pdf"&gt;Whitening and Second Order Optimization Both Make Information in the Dataset Unusable During Training, and Can Reduce or Prevent Generalization&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Neha S. Wadia,&lt;b&gt; Daniel Duckworth, Samuel S. Schoenholz, Ethan Dyer, Jascha Sohl-Dickstein&lt;/b&gt;  &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/teterwak21a/teterwak21a.pdf"&gt;Understanding Invariance via Feedforward Inversion of Discriminatively Trained Classifiers&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Piotr Teterwak*, &lt;b&gt;Chiyuan Zhang, Dilip Krishnan, Michael C. Mozer &lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/furuta21a/furuta21a.pdf"&gt;Policy Information Capacity: Information-Theoretic Measure for Task Complexity in Deep Reinforcement Learning&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Hiroki Furuta, Tatsuya Matsushima, Tadashi Kozuno, Yutaka Matsuo, &lt;b&gt;Sergey Levine, Ofir Nachum, Shixiang Shane Gu&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/hussenot21a/hussenot21a.pdf"&gt;Hyperparameter Selection for Imitation Learning&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Leonard Hussenot, Marcin Andrychowicz, Damien Vincent, Robert Dadashi, Anton Raichuk, Lukasz Stafiniak, Sertan Girgin, Raphael Marinier, Nikola Momchev, Sabela Ramos, Manu Orsini, Olivier Bachem, Matthieu Geist, Olivier Pietquin&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/rawat21a/rawat21a.pdf"&gt;Disentangling Sampling and Labeling Bias for Learning in Large-Output Spaces&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Ankit Singh Rawat, Aditya Krishna Menon, Wittawat Jitkrittum, Sadeep Jayasumana, Felix X. Yu,&lt;/b&gt;  &lt;b&gt;Sashank J. Reddi, Sanjiv Kumar &lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/deng21c/deng21c.pdf"&gt;Revenue-Incentive Tradeoffs in Dynamic Reserve Pricing&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Yuan Deng, Sebastien Lahaie, Vahab Mirrokni, Song Zuo&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/likhosherstov21a/likhosherstov21a.pdf"&gt;Debiasing a First-Order Heuristic for Approximate Bi-Level Optimization&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Valerii Likhosherstov, &lt;b&gt;Xingyou Song, Krzysztof Choromanski&lt;/b&gt;, Jared Davis, Adrian Weller   &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/wen21b/wen21b.pdf"&gt;Characterizing the Gap Between Actor-Critic and Policy Gradient&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Junfeng Wen, Saurabh Kumar, &lt;b&gt;Ramki Gummadi, Dale Schuurmans&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/whang21b/whang21b.pdf"&gt;Composing Normalizing Flows for Inverse Problems&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Jay Whang, &lt;b&gt;Erik Lindgren&lt;/b&gt;, Alexandros Dimakis        &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/cassel21a/cassel21a.pdf"&gt;Online Policy Gradient for Model Free Learning of Linear Quadratic Regulators with √T Regret&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Asaf Cassel, &lt;b&gt;Tomer Koren &lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/leme21a/leme21a.pdf"&gt;Learning to Price Against a Moving Target&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Renato Paes Leme, Balasubramanian Sivan&lt;/b&gt;, Yifeng Teng,&lt;b&gt; Pratik Worah&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/correa21a/correa21a.pdf"&gt;Fairness and Bias in Online Selection&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Jose Correa, Andres Cristi, &lt;b&gt;Paul Duetting, Ashkan Norouzi-Fard&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/nock21a/nock21a.pdf"&gt;The Impact of Record Linkage on Learning from Feature Partitioned Data&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Richard Nock&lt;/b&gt;, Stephen Hardy, Wilko Henecka, Hamish Ivey-Law, Jakub Nabaglo, Giorgio Patrini,   Guillaume Smith, Brian Thorne       &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/feng21b/feng21b.pdf"&gt;Reserve Price Optimization for First Price Auctions in Display Advertising&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Zhe Feng*,&lt;b&gt; Sébastien Lahaie, Jon Schneider, Jinchao Ye&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/agarwal21b/agarwal21b.pdf"&gt;A Regret Minimization Approach to Iterative Learning Control&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Naman Agarwal, Elad Hazan, Anirudha Majumdar&lt;/b&gt;, Karan Singh       &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/menon21a/menon21a.pdf"&gt;A Statistical Perspective on Distillation&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Aditya Krishna Menon, Ankit Singh Rawat, Sashank J. Reddi, Seungyeon Kim, Sanjiv Kumar&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/cella21a/cella21a.pdf"&gt;Best Model Identification: A Rested Bandit Formulation&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Leonardo Cella, Massimiliano Pontil, &lt;b&gt;Claudio Gentile&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/cranko21a/cranko21a.pdf"&gt;Generalised Lipschitz Regularisation Equals Distributional Robustness&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Zac Cranko, Zhan Shi, Xinhua Zhang, &lt;b&gt;Richard Nock&lt;/b&gt;, &lt;b&gt;Simon Kornblith&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/lancewicki21a/lancewicki21a.pdf"&gt;Stochastic Multi-armed Bandits with Unrestricted Delay Distributions&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Tal Lancewicki, Shahar Segal,&lt;b&gt; Tomer Koren, Yishay Mansour&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/balseiro21a/balseiro21a.pdf"&gt;Regularized Online Allocation Problems: Fairness and Beyond&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Santiago Balseiro, Haihao Lu, Vahab Mirrokni&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/kumar21b/kumar21b.pdf"&gt;Implicit Rate-Constrained Optimization of Non-decomposable Objectives&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Abhishek Kumar, Harikrishna Narasimhan, Andrew Cotter&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/mei21a/mei21a.pdf"&gt;Leveraging Non-uniformity in First-Order Non-Convex Optimization&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Jincheng Mei&lt;/b&gt;, Yue Gao, &lt;b&gt;Bo Dai&lt;/b&gt;, Csaba Szepesvari, &lt;b&gt;Dale Schuurmans &lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/cutkosky21a/cutkosky21a.pdf"&gt;Dynamic Balancing for Model Selection in Bandits and RL&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Ashok Cutkosky&lt;b&gt;, Christoph Dann, Abhimanyu Das, Claudio Gentile, &lt;/b&gt;Aldo Pacchiano, &lt;b&gt;Manish Purohit&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/saha21a/saha21a.pdf"&gt;Adversarial Dueling Bandits&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Aadirupa Saha, &lt;b&gt;Tomer Koren, Yishay Mansour&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/hiranandani21a/hiranandani21a.pdf"&gt;Optimizing Black-Box Metrics with Iterative Example Weighting&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Gaurush Hiranandani*, Jatin Mathur,&lt;b&gt; Harikrishna Narasimhan, Mahdi Milani Fard, Oluwasanmi Koyejo&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/cortes21a/cortes21a.pdf"&gt;Relative Deviation Margin Bounds&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Corinna Cortes, Mehryar Mohri, Ananda Theertha Suresh&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/hoedt21a/hoedt21a.pdf"&gt;MC-LSTM: Mass-Conserving LSTM&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Pieter-Jan Hoedt, Frederik Kratzert, Daniel Klotz, Christina Halmich, Markus Holzleitner, &lt;b&gt;Grey Nearing&lt;/b&gt;, Sepp Hochreiter, Günter Klambauer       &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/golany21a/golany21a.pdf"&gt;12-Lead ECG Reconstruction via Koopman Operators&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Authors:Tomer Golany, Kira Radinsky, &lt;b&gt;Daniel Freedman&lt;/b&gt;, Saar Minha       &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/heidari21a/heidari21a.pdf"&gt;Finding Relevant Information via a Discrete Fourier Expansion&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Mohsen Heidari, Jithin Sreedharan&lt;b&gt;, Gil Shamir, &lt;/b&gt;Wojciech Szpankowski       &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/ren21a/ren21a.pdf"&gt;LEGO: Latent Execution-Guided Reasoning for Multi-hop Question Answering on Knowledge Graphs&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Hongyu Ren,&lt;b&gt; Hanjun Dai, Bo Dai&lt;/b&gt;, Xinyun Chen, Michihiro Yasunaga, Haitian Sun, &lt;b&gt;Dale Schuurmans&lt;/b&gt;, Jure Leskovec,&lt;b&gt; Denny Zhou&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/chen21m/chen21m.pdf"&gt;SpreadsheetCoder: Formula Prediction from Semi-structured Context&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Xinyun Chen,&lt;b&gt; Petros Maniatis, Rishabh Singh, Charles Sutton, Hanjun Dai, Max Lin, Denny Zhou&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/atsidakou21a/atsidakou21a.pdf"&gt;Combinatorial Blocking Bandits with Stochastic Delays&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Alexia Atsidakou, Orestis Papadigenopoulos, &lt;b&gt;Soumya Basu&lt;/b&gt;, Constantine Caramani, Sanjay Shakkottai       &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/basu21a/basu21a.pdf"&gt;Beyond log2(T) Regret for Decentralized Bandits in Matching Markets&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Soumya Basu&lt;/b&gt;, Karthik Abinav Sankararaman, Abishek Sankararaman       &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/alieva21a/alieva21a.pdf"&gt;Robust Pure Exploration in Linear Bandits with Limited Budget&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Ayya Alieva, Ashok Cutkosky,&lt;b&gt; Abhimanyu Das&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/hong21a/hong21a.pdf"&gt;Latent Programmer: Discrete Latent Codes for Program Synthesis&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Joey Hong, David Dohan, Rishabh Singh, Charles Sutton, Manzil Zaheer&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/jia21b/jia21b.pdf"&gt;Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision&lt;/a&gt; &lt;em&gt;(see &lt;a href="https://ai.googleblog.com/2021/05/align-scaling-up-visual-and-vision.html"&gt;blog post&lt;/a&gt;)&lt;/em&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, Tom Duerig&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/roeder21a/roeder21a.pdf"&gt;On Linear Identifiability of Learned Representations&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Geoffrey Roeder, &lt;b&gt;Luke Metz, Diederik P. Kingma&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/rajagopalan21a/rajagopalan21a.pdf"&gt;Hierarchical Clustering of Data Streams: Scalable Algorithms and Approximation Guarantees&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Anand Rajagopalan&lt;/b&gt;, Fabio Vitale, Danny Vainstein,&lt;b&gt; Gui Citovsky, Cecilia M Procopiuc, Claudio Gentile&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/gillenwater21a/gillenwater21a.pdf"&gt;Differentially Private Quantiles&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Jennifer Gillenwater, Matthew Joseph, Alex Kulesza&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/jiang21i/jiang21i.pdf"&gt;Active Covering&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Heinrich Jiang, Afshin Rostamizadeh&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/rematas21a/rematas21a.pdf"&gt;Sharf: Shape-Conditioned Radiance Fields from a Single View&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Konstantinos Rematas, Ricardo Martin-Brualla, Vittorio Ferrari&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/triantafillou21a/triantafillou21a.pdf"&gt;Learning a Universal Template for Few-Shot Dataset Generalization&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Eleni Triantafillou*, &lt;b&gt;Hugo Larochelle&lt;/b&gt;, Richard Zemel, &lt;b&gt;Vincent Dumoulin&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/chien21a/chien21a.pdf"&gt;Private Alternating Least Squares: Practical Private Matrix Completion with Tighter Rates&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Steve Chien, Prateek Jain, Walid Krichene, Steffen Rendle, Shuang Song, Abhradeep Thakurta, Li Zhang&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/cohen21c/cohen21c.pdf"&gt;Differentially-Private Clustering of Easy Instances&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Edith Cohen, Haim Kaplan, Yishay Mansour,  Uri Stemmer, Eliad Tsfadia&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2007.14321.pdf"&gt;Label-Only Membership Inference Attacks&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Christopher A. Choquette-Choo, Florian Tramèr, &lt;b&gt;Nicholas Carlini&lt;/b&gt;, Nicolas Papernot       &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/chen21f/chen21f.pdf"&gt;Neural Feature Matching in Implicit 3D Representations&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Yunlu Chen, Basura Fernando, Hakan Bilen&lt;b&gt;, Thomas Mensink,&lt;/b&gt; Efstratios Gavves       &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/chang21a/chang21a.pdf"&gt;Locally Private k-Means in One Round&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Alisa Chang, Badih Ghazi, Ravi Kumar, Pasin Manurangsi&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/shin21a/shin21a.pdf"&gt;Large-Scale Meta-learning with Continual Trajectory Shifting&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Jaewoong Shin, Hae Beom Lee,&lt;b&gt; Boqing Gong&lt;/b&gt;, Sung Ju Hwang       &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/kandiros21a/kandiros21a.pdf"&gt;Statistical Estimation from Dependent Data&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Vardis Kandiros, Yuval Dagan,&lt;b&gt; Nishanth Dikkala&lt;/b&gt;, Surbhi Goel, Constantinos Daskalakis       &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/qiao21b/qiao21b.pdf"&gt;Oneshot Differentially Private Top-k Selection&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Gang Qiao, Weijie J. Su, &lt;b&gt;Li Zhang&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/sabour21a/sabour21a.pdf"&gt;Unsupervised Part Representation by Flow Capsules&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Sara Sabour, Andrea Tagliasacchi, Soroosh Yazdani, Geoffrey E. Hinton, David J. Fleet&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/asi21b/asi21b.pdf"&gt;Private Stochastic Convex Optimization: Optimal Rates in L1 Geometry&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Hilal Asi, Vitaly Feldman,&lt;b&gt; Tomer Koren,&lt;/b&gt; Kunal Talwar       &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/kairouz21b/kairouz21b.pdf"&gt;Practical and Private (Deep) Learning Without Sampling or Shuffling&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Peter Kairouz, Brendan McMahan, Shuang Song, Om Thakkar, Abhradeep Thakurta, Zheng Xu&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/ghazi21a/ghazi21a.pdf"&gt;Differentially Private Aggregation in the Shuffle Model: Almost Central Accuracy in Almost a Single Message&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Badih Ghazi, Ravi Kumar, Pasin Manurangsi, Rasmus Pagh, Amer Sinha &lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/liu21w/liu21w.pdf"&gt;Leveraging Public Data for Practical Private Query Release&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Terrance Liu, Giuseppe Vietri, &lt;b&gt;Thomas Steinke&lt;/b&gt;, Jonathan Ullman, Zhiwei Steven Wu       &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/kveton21a/kveton21a.pdf"&gt;Meta-Thompson Sampling&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Branislav Kveton, &lt;/b&gt;Mikhail Konobeev,&lt;b&gt; Manzil Zaheer, Chih-wei Hsu, Martin Mladenov, Craig Boutilier, &lt;/b&gt;Csaba Szepesvári       &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/murphy21a/murphy21a.pdf"&gt;Implicit-PDF: Non-parametric Representation of Probability Distributions on the Rotation Manifold&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Kieran A Murphy, Carlos Esteves, Varun Jampani, Srikumar Ramalingam, Ameesh Makadia&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/cohen-addad21a/cohen-addad21a.pdf"&gt;Improving Ultrametrics Embeddings Through Coresets&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Vincent Cohen-Addad&lt;/b&gt;, Rémi de Joannis de Verclos, Guillaume Lagarde       &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/cortes21b/cortes21b.pdf"&gt;A Discriminative Technique for Multiple-Source Adaptation&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Corinna Cortes, Mehryar Mohri, Ananda Theertha Suresh&lt;/b&gt;, Ningshan Zhang       &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/cheng21b/cheng21b.pdf"&gt;Self-Supervised and Supervised Joint Training for Resource-Rich Machine Translation&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Yong Cheng&lt;/b&gt;, Wei Wang*,&lt;b&gt; Lu Jiang, Wolfgang Macherey&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2106.08448.pdf"&gt;Correlation Clustering in Constant Many Parallel Rounds&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Vincent Cohen-Addad, Silvio Lattanzi, &lt;/b&gt;Slobodan Mitrović&lt;b&gt;, Ashkan Norouzi-Fard, Nikos Parotsidis,&lt;/b&gt;  Jakub Tarnawski       &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/dhulipala21a/dhulipala21a.pdf"&gt;Hierarchical Agglomerative Graph Clustering in Nearly-Linear Time&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Laxman Dhulipala, &lt;b&gt;David Eisenstat, Jakub Łącki, Vahab Mirrokni&lt;/b&gt;, Jessica Shi    &lt;span style="text-decoration:underline;"&gt; &lt;/span&gt;  &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/sandler21a/sandler21a.pdf"&gt;Meta-learning Bidirectional Update Rules&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Mark Sandler, Max Vladymyrov, Andrey Zhmoginov, Nolan Miller, Andrew Jackson, Tom Madams, Blaise Aguera y Arcas&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/rosca21a/rosca21a.pdf"&gt;Discretization Drift in Two-Player Games&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Mihaela Rosca, Yan Wu, &lt;b&gt;Benoit Dherin&lt;/b&gt;, David G.T. Barrett       &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/sun21e/sun21e.pdf"&gt;Reasoning Over Virtual Knowledge Bases With Open Predicate Relations&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  Haitian Sun*&lt;b&gt;, Pat Verga, Bhuwan Dhingra, &lt;/b&gt;Ruslan Salakhutdinov,&lt;b&gt; William W. Cohen&lt;/b&gt;      &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/merchant21a/merchant21a.pdf"&gt;Learn2Hop: Learned Optimization on Rough Landscapes&lt;/a&gt;&lt;br/&gt;&lt;em&gt;  &lt;b&gt;Amil Merchant, Luke Metz, Samuel Schoenholz, Ekin Cubuk&lt;/b&gt;       &lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/bahri21a/bahri21a.pdf"&gt;Locally Adaptive Label Smoothing Improves Predictive Churn&lt;/a&gt;&lt;br/&gt;    &lt;em&gt;&lt;b&gt;Dara Bahri, Heinrich Jiang&lt;/b&gt;&lt;/em&gt;      &lt;/p&gt;&lt;p&gt;&lt;a href="http://proceedings.mlr.press/v139/chen21v/chen21v.pdf"&gt;Overcoming Catastrophic Forgetting by Bayesian Generative Regularization&lt;/a&gt;&lt;br/&gt;    &lt;em&gt;Patrick H. Chen,&lt;b&gt; Wei Wei&lt;/b&gt;, Cho-jui Hsieh, &lt;b&gt;Bo Dai&lt;/b&gt;&lt;/em&gt;      &lt;p&gt;&lt;b&gt;&lt;span style="text-decoration:underline;"&gt;Workshops&lt;/span&gt;&lt;/b&gt; (&lt;em&gt;only Google affiliations are noted&lt;/em&gt;)&lt;br/&gt;  &lt;a href="https://www.latinxinai.org/icml-2021-about"&gt;LatinX in AI (LXAI) Research at ICML 2021&lt;/a&gt;&lt;br/&gt;  Hosts: &lt;em&gt;&lt;b&gt;Been Kim, Natasha Jaques&lt;/b&gt;&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://sites.google.com/corp/view/udlworkshop2021/home"&gt;Uncertainty and Robustness in Deep Learning&lt;/a&gt;&lt;br/&gt;    Organizers:&lt;em&gt;&lt;b&gt; Balaji Lakshminarayanan, Jasper Snoek&lt;/b&gt;&lt;/em&gt;  Invited Speaker: &lt;em&gt;&lt;b&gt;Dustin Tran&lt;/b&gt;&lt;/em&gt;      &lt;/p&gt;&lt;p&gt;&lt;a href="https://sites.google.com/view/RL4RealLife"&gt;Reinforcement Learning for Real Life&lt;/a&gt;&lt;br/&gt;  Organizers:&lt;em&gt;&lt;b&gt; Minmin Chen, Lihong Li&lt;/b&gt;&lt;/em&gt;  Invited Speaker: &lt;em&gt;&lt;b&gt;Ed Chi&lt;/b&gt;&lt;/em&gt;      &lt;/p&gt;&lt;p&gt;&lt;a href="https://sites.google.com/view/imlh2021/"&gt;Interpretable Machine Learning in Healthcare&lt;/a&gt;&lt;br/&gt;  Organizers:&lt;em&gt;&lt;b&gt; Alan Karthikesalingam&lt;/b&gt;&lt;/em&gt;  Invited Speakers: &lt;em&gt;&lt;b&gt;Abhijit Guha Roy, Jim Winkens&lt;/b&gt;&lt;/em&gt;      &lt;/p&gt;&lt;p&gt;&lt;a href="https://sites.google.com/view/naci2021"&gt;The Neglected Assumptions in Causal Inference&lt;/a&gt;&lt;br/&gt;  Organizer:&lt;em&gt;&lt;b&gt; Alexander D'Amour&lt;/b&gt;&lt;/em&gt;      &lt;/p&gt;&lt;p&gt;&lt;a href="https://sites.google.com/view/recourse21"&gt;ICML Workshop on Algorithmic Recourse&lt;/a&gt;&lt;br/&gt;  Invited Speakers: &lt;em&gt;&lt;b&gt;Been Kim, Berk Ustun&lt;/b&gt;&lt;/em&gt;      &lt;/p&gt;&lt;p&gt;&lt;a href="https://advml-workshop.github.io/icml2021/"&gt;A Blessing in Disguise: The Prospects and Perils of Adversarial Machine Learning&lt;/a&gt;&lt;br/&gt;  Invited Speaker: &lt;em&gt;&lt;b&gt;Nicholas Carlini&lt;/b&gt;&lt;/em&gt;      &lt;/p&gt;&lt;p&gt;&lt;a href="https://sites.google.com/view/icml2021oppo"&gt;Overparameterization: Pitfalls and Opportunities&lt;/a&gt;&lt;br/&gt;  Organizers: &lt;em&gt;&lt;b&gt;Yasaman Bahri,&lt;/b&gt; &lt;b&gt;Hanie Sedghi&lt;/b&gt;&lt;/em&gt;      &lt;/p&gt;&lt;p&gt;&lt;a href="https://sites.google.com/corp/view/itr3"&gt;Information-Theoretic Methods for Rigorous, Responsible, and Reliable Machine Learning (ITR3)&lt;/a&gt;&lt;br/&gt;  Invited Speaker: &lt;em&gt;&lt;b&gt;Thomas Steinke&lt;/b&gt;&lt;/em&gt;      &lt;/p&gt;&lt;p&gt;&lt;a href="https://sites.google.com/view/optml-icml2021"&gt;Beyond First-Order Methods in Machine Learning Systems&lt;/a&gt;&lt;br/&gt;  Invited Speaker: &lt;em&gt;&lt;b&gt;Courtney Paquette&lt;/b&gt;&lt;/em&gt;      &lt;/p&gt;&lt;p&gt;&lt;a href="https://icml21ssl.github.io/"&gt;ICML 2021 Workshop: Self-Supervised Learning for Reasoning and Perception&lt;/a&gt;&lt;br/&gt;  Invited Speaker: &lt;em&gt;&lt;b&gt;Chelsea Finn&lt;/b&gt;&lt;/em&gt;      &lt;/p&gt;&lt;p&gt;&lt;a href="https://lyang36.github.io/icml2021_rltheory/"&gt;Workshop on Reinforcement Learning Theory&lt;/a&gt;&lt;br/&gt;    Invited Speaker: &lt;em&gt;&lt;b&gt;Bo Dai&lt;/b&gt;&lt;/em&gt;&lt;/p&gt;  &lt;p&gt;&lt;b&gt;&lt;span style="text-decoration:underline;"&gt;Tutorials&lt;/span&gt;&lt;/b&gt; (&lt;em&gt;only Google affiliations are noted&lt;/em&gt;)&lt;br/&gt;  &lt;a href="https://sites.google.com/view/ResponsibleAITutorial"&gt;Responsible AI in Industry: Practical Challenges and Lessons Learned&lt;/a&gt;&lt;br/&gt;  Organizers: &lt;em&gt;&lt;b&gt;Ben Packer&lt;/b&gt;&lt;/em&gt;      &lt;/p&gt;&lt;p&gt;&lt;a href="https://sites.google.com/corp/view/nsc-tutorial/home"&gt;Online and Non-stochastic Control&lt;/a&gt;&lt;br/&gt;  Organizers: &lt;em&gt;&lt;b&gt;Elad Hazan&lt;/b&gt;&lt;/em&gt;      &lt;/p&gt;&lt;p&gt;&lt;a href="https://random-matrix-learning.github.io/"&gt;Random Matrix Theory and ML (RMT +ML)&lt;/a&gt;&lt;br/&gt;  Organizers:&lt;em&gt; &lt;b&gt;Fabian Pedregosa, Jeffrey Pennington&lt;/b&gt;, &lt;b&gt;Courntey Paquette&lt;/b&gt;&lt;/em&gt;      Self-Attention for Computer Vision   Organizers: &lt;em&gt;&lt;b&gt;Prajit Ramachandran, Ashish Vaswani&lt;/b&gt;&lt;/em&gt;  &lt;p&gt;&lt;em&gt;* Indicates work done while at Google&lt;/em&gt;&lt;/em&gt;&lt;/p&gt; &lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/blogspot/gJZg?a=8690zr_BdnY:1RVAYLqxM_U:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/blogspot/gJZg/~4/8690zr_BdnY" height="1" width="1" alt=""/&gt;</content>
    <link rel="replies" type="application/atom+xml" href="http://ai.googleblog.com/feeds/4133940225737160804/comments/default" title="Post Comments"/>
    <link rel="replies" type="text/html" href="http://ai.googleblog.com/2021/07/google-at-icml-2021.html#comment-form" title="0 Comments"/>
    <link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/4133940225737160804"/>
    <link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/4133940225737160804"/>
    <link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/blogspot/gJZg/~3/8690zr_BdnY/google-at-icml-2021.html" title="Google at ICML 2021"/>
    <author>
      <name>Google AI</name>
      <uri>http://www.blogger.com/profile/12098626514775266161</uri>
      <email>noreply@blogger.com</email>
      <gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="16" height="16" src="https://img1.blogblog.com/img/b16-rounded.gif"/>
    </author>
    <thr:total>0</thr:total>
    <feedburner:origLink>http://ai.googleblog.com/2021/07/google-at-icml-2021.html</feedburner:origLink>
  </entry>
  <entry>
    <id>tag:blogger.com,1999:blog-8474926331452026626.post-8564855857063590663</id>
    <published>2021-07-16T10:22:00.020-07:00</published>
    <updated>2021-07-16T12:24:07.211-07:00</updated>
    <category scheme="http://www.blogger.com/atom/ns#" term="Computer Vision"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="Machine Learning"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="Research"/>
    <title type="text">High Fidelity Image Generation Using Diffusion Models</title>
    <content type="html">&lt;span class="byline-author"&gt;Posted by Jonathan Ho, Research Scientist and Chitwan Saharia, Software Engineer, Google Research, Brain Team&lt;/span&gt; &lt;p&gt;Natural image synthesis is a broad class of machine learning (ML) tasks with wide-ranging applications that pose a number of design challenges. One example is image super-resolution, in which a model is trained to transform a low resolution image into a detailed high resolution image (e.g., &lt;a href="https://ai.googleblog.com/2016/11/enhance-raisr-sharp-images-with-machine.html"&gt;RAISR&lt;/a&gt;). Super-resolution has many applications that can range from restoring old family portraits to &lt;a href="https://en.wikipedia.org/wiki/Super-resolution_imaging"&gt;improving medical imaging systems&lt;/a&gt;. Another such image synthesis task is class-conditional image generation, in which a model is trained to generate a sample image from an input class label. The resulting generated sample images can be used to improve performance of downstream models for image classification, segmentation, and more.&lt;/p&gt;&lt;p&gt;Generally, these image synthesis tasks are performed by deep generative models, such as &lt;a href="https://arxiv.org/abs/1406.2661"&gt;GANs&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/1312.6114"&gt;VAEs&lt;/a&gt;, and &lt;a href="https://arxiv.org/abs/1601.06759"&gt;autoregressive models&lt;/a&gt;. Yet each of these generative models has its downsides when trained to synthesize high quality samples on difficult, high resolution datasets. For example, GANs often suffer from &lt;a href="https://developers.google.com/machine-learning/gan/problems"&gt;unstable training and mode collapse&lt;/a&gt;, and autoregressive models typically suffer from slow synthesis speed. &lt;/p&gt;&lt;p&gt;Alternatively, &lt;a href="https://arxiv.org/abs/1503.03585"&gt;diffusion models&lt;/a&gt;, originally proposed in 2015, have seen a recent revival in interest due to their training stability and their promising sample quality results on &lt;a href="https://arxiv.org/abs/2006.11239"&gt;image&lt;/a&gt; &lt;a href="https://arxiv.org/abs/1907.05600"&gt;and&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2009.00713"&gt;audio&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2009.09761"&gt;generation&lt;/a&gt;. Thus, they offer potentially favorable trade-offs compared to other types of deep generative models. Diffusion models work by corrupting the training data by progressively adding Gaussian noise, slowly wiping out details in the data until it becomes pure noise, and then training a neural network to reverse this corruption process. Running this reversed corruption process synthesizes data from pure noise by gradually denoising it until a clean sample is produced. This synthesis procedure &lt;a href="https://arxiv.org/abs/2006.11239"&gt;can be interpreted&lt;/a&gt; as an optimization algorithm that &lt;a href="https://arxiv.org/abs/1907.05600"&gt;follows the gradient of the data density&lt;/a&gt; to produce likely samples. &lt;/p&gt;&lt;p&gt;Today we present two connected approaches that push the boundaries of the image synthesis quality for diffusion models — &lt;a href="https://iterative-refinement.github.io/"&gt;Super-Resolution via Repeated Refinements&lt;/a&gt; (SR3) and a model for class-conditioned synthesis, called &lt;a href="https://cascaded-diffusion.github.io/"&gt;Cascaded Diffusion Models&lt;/a&gt; (CDM). We show that by scaling up diffusion models and with carefully selected data augmentation techniques, we can outperform existing approaches. Specifically, SR3 attains strong image super-resolution results that surpass GANs in human evaluations. CDM generates high fidelity ImageNet samples that surpass &lt;a href="https://arxiv.org/abs/1809.11096"&gt;BigGAN-deep&lt;/a&gt; and&lt;a href="https://arxiv.org/abs/1906.00446"&gt; VQ-VAE2&lt;/a&gt; on both &lt;a href="https://en.wikipedia.org/wiki/Fr%C3%A9chet_inception_distance"&gt;FID score&lt;/a&gt; and &lt;a href="https://arxiv.org/abs/1905.10887"&gt;Classification Accuracy Score&lt;/a&gt; by a large margin.  &lt;/p&gt;&lt;p&gt;&lt;b&gt;SR3: Image Super-Resolution&lt;/b&gt;&lt;br /&gt;&lt;a href="http://iterative-refinement.github.io"&gt;SR3&lt;/a&gt; is a super-resolution diffusion model that takes as input a low-resolution image, and builds a corresponding high resolution image from pure noise. The model is trained on an image corruption process in which noise is progressively added to a high-resolution image until only pure noise remains. It then learns to reverse this process, beginning from pure noise and progressively removing noise to reach a target distribution through the guidance of the input low-resolution image..  &lt;/p&gt;&lt;video controls="" autoplay="" loop="" muted="" width="95%"&gt;&lt;source src="https://iterative-refinement.github.io/assets/cascade_movie2_mp4.mp4" type="video/mp4"&gt;&lt;/source&gt;&lt;/video&gt;&lt;p&gt;With large scale training, SR3 achieves strong benchmark results on the super-resolution task for face and natural images when scaling to resolutions 4x–8x that of the input low-resolution  image. These super-resolution models can further be cascaded together to increase the effective super-resolution scale factor, e.g., stacking a 64x64 → 256x256 and a 256x256 → 1024x1024 face super-resolution model together in order to perform a 64x64 → 1024x1024 super-resolution task.  &lt;/p&gt;&lt;p&gt;We compare SR3 with existing methods using human evaluation study. We conduct a &lt;a href="https://en.wikipedia.org/wiki/Two-alternative_forced_choice"&gt;Two-Alternative Forced Choice Experiment&lt;/a&gt; where subjects are asked to choose between the reference high resolution image, and the model output when asked the question, “&lt;em&gt;Which image would you guess is from a camera?&lt;/em&gt;” We measure the performance of the model through confusion rates (% of time raters choose the model outputs over reference images, where a perfect algorithm would achieve a 50% confusion rate). The results of this study are shown in the figure below. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-MRJn88KECNA/YPG5dmVIpYI/AAAAAAAAH6s/cNP2QlhlZz0xJ8yWNbpgr1JJVIXwXJ-3QCLcBGAsYHQ/s1462/image4.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="744" data-original-width="1462" height="326" src="https://1.bp.blogspot.com/-MRJn88KECNA/YPG5dmVIpYI/AAAAAAAAH6s/cNP2QlhlZz0xJ8yWNbpgr1JJVIXwXJ-3QCLcBGAsYHQ/w640-h326/image4.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;&lt;b&gt;Above:&lt;/b&gt; We achieve close to 50% confusion rate on the task of 16x16 → 128x128 faces, outperforming state-of-the-art face super-resolution methods&amp;nbsp;&lt;a href="https://arxiv.org/abs/2003.03808"&gt;PULSE&lt;/a&gt;&amp;nbsp;and&amp;nbsp;&lt;a href="https://arxiv.org/abs/1711.10703"&gt;FSRGAN&lt;/a&gt;. &lt;b&gt;Below:&lt;/b&gt; We also achieve a 40% confusion rate on the much more difficult task of 64x64 → 256x256 natural images, outperforming the regression baseline by a large margin.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;b&gt;CDM: Class-Conditional ImageNet Generation&lt;/b&gt;&lt;br /&gt;Having shown the effectiveness of SR3 in performing natural image super-resolution, we go a step further and use these SR3 models for class-conditional image generation. &lt;a href="https://cascaded-diffusion.github.io/"&gt;CDM&lt;/a&gt; is a class-conditional diffusion model trained on ImageNet data to generate high-resolution natural images. Since ImageNet is a difficult, high-entropy dataset, we built CDM as a cascade of multiple diffusion models. This cascade approach involves chaining together multiple generative models over several spatial resolutions: one diffusion model that generates data at a low resolution, followed by a sequence of SR3 super-resolution diffusion models that gradually increase the resolution of the generated image to the highest resolution. It is well known that cascading improves quality and training speed for high resolution data, as shown by previous studies (for example in &lt;a href="https://arxiv.org/abs/1812.01608"&gt;autoregressive models&lt;/a&gt; and &lt;a href="https://arxiv.org/abs/1906.00446"&gt;VQ-VAE-2&lt;/a&gt;) and in &lt;a href="https://arxiv.org/abs/2102.09672"&gt;concurrent&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2105.05233"&gt;work&lt;/a&gt; for diffusion models. As demonstrated by our quantitative results below, CDM further highlights the effectiveness of cascading in diffusion models for sample quality and usefulness in downstream tasks, such as image classification. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-fZUmX3WWlJ8/YPG5qDDvVmI/AAAAAAAAH6w/nK7NMZzGZno-t_rBeuhkPHBq4k8Z32kYQCLcBGAsYHQ/s800/image3.gif" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="295" data-original-width="800" height="236" src="https://1.bp.blogspot.com/-fZUmX3WWlJ8/YPG5qDDvVmI/AAAAAAAAH6w/nK7NMZzGZno-t_rBeuhkPHBq4k8Z32kYQCLcBGAsYHQ/w640-h236/image3.gif" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Example of the cascading pipeline that includes a sequence of diffusion models: the first generates a low resolution image, and the rest perform upsampling to the final high resolution image. Here the pipeline is for class-conditional ImageNet generation, which begins with a class-conditional diffusion model at 32x32 resolution, followed by 2x and 4x class-conditional super-resolution using SR3.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-sevsNGJ66P4/YPG53LmbXAI/AAAAAAAAH64/MPy1jbpdbbY1CQ7fIWjl_PZXzKJ38rCXwCLcBGAsYHQ/s1521/image2.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="1141" data-original-width="1521" height="480" src="https://1.bp.blogspot.com/-sevsNGJ66P4/YPG53LmbXAI/AAAAAAAAH64/MPy1jbpdbbY1CQ7fIWjl_PZXzKJ38rCXwCLcBGAsYHQ/w640-h480/image2.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Selected generated images from our 256x256 cascaded class-conditional ImageNet model.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Along with including the SR3 model in the cascading pipeline, we also introduce a new data augmentation technique, which we call &lt;em&gt;conditioning augmentation&lt;/em&gt;, that further improves the sample quality results of CDM. While the super-resolution models in CDM are trained on original images from the dataset, during generation they need to perform super-resolution on the images generated by a low-resolution base model, which may not be of sufficiently high quality in comparison to the original images. This leads to a train-test mismatch for the super-resolution models. Conditioning augmentation refers to applying data augmentation to the low-resolution input image of each super-resolution model in the cascading pipeline. These augmentations, which in our case include Gaussian noise and Gaussian blur, prevents each super-resolution model from overfitting to its lower resolution conditioning input, eventually leading to better higher resolution sample quality for CDM. &lt;/p&gt;&lt;p&gt;Altogether, CDM generates high fidelity samples superior to BigGAN-deep and VQ-VAE-2 in terms of both &lt;a href="https://en.wikipedia.org/wiki/Fr%C3%A9chet_inception_distance"&gt;FID score&lt;/a&gt; and &lt;a href="https://arxiv.org/abs/1905.10887"&gt;Classification Accuracy Score&lt;/a&gt; on class-conditional ImageNet generation. CDM is a pure generative model that does not use a classifier to boost sample quality, unlike other models such as &lt;a href="https://arxiv.org/abs/2105.05233"&gt;ADM&lt;/a&gt; and VQ-VAE-2. See below for quantitative results on sample quality. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-0QCpdlDZTyU/YPG6GJId-tI/AAAAAAAAH7A/Bp2OK_PTrqQBo_1-P2E7KftCCJr7yPa2ACLcBGAsYHQ/s1999/image5.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="487" data-original-width="1999" height="156" src="https://1.bp.blogspot.com/-0QCpdlDZTyU/YPG6GJId-tI/AAAAAAAAH7A/Bp2OK_PTrqQBo_1-P2E7KftCCJr7yPa2ACLcBGAsYHQ/w640-h156/image5.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Class-conditional ImageNet FID scores at the 256x256 resolution for methods that do not use extra classifiers to boost sample quality. BigGAN-deep is reported at its best truncation value. (Lower is better.)&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-fxFnIOObOFY/YPG6Q2nld6I/AAAAAAAAH7I/mnBOaoQv3IYLS8meip7XrAqpUceBtARLQCLcBGAsYHQ/s1999/image1.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="626" data-original-width="1999" height="200" src="https://1.bp.blogspot.com/-fxFnIOObOFY/YPG6Q2nld6I/AAAAAAAAH7I/mnBOaoQv3IYLS8meip7XrAqpUceBtARLQCLcBGAsYHQ/w640-h200/image1.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;ImageNet classification accuracy scores at the 256x256 resolution, measuring the validation set accuracy of a classifier trained on generated data. CDM generated data attains significant gains over existing methods, closing the gap in classification accuracy between real and generated data. (Higher is better.)&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;b&gt;Conclusion&lt;/b&gt;&lt;br /&gt;With SR3 and CDM, we have pushed the performance of diffusion models to state-of-the-art on super-resolution and class-conditional ImageNet generation benchmarks. We are excited to further test the limits of diffusion models for a wide variety of generative modeling problems. For more information on our work, please visit &lt;a href="http://iterative-refinement.github.io"&gt;Image Super-Resolution via Iterative Refinement&lt;/a&gt; and &lt;a href="http://cascaded-diffusion.github.io"&gt;Cascaded Diffusion Models for High Fidelity Image Generation&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Acknowledgements:&lt;/b&gt;&lt;br /&gt;&lt;em&gt;We thank our co-authors William Chan, Mohammad Norouzi, Tim Salimans, and David Fleet, and we are grateful for research discussions and assistance from Ben Poole, Jascha Sohl-Dickstein, Doug Eck, and the rest of the Google Research, Brain Team. Thanks to Tom Small for helping us with the animations.&lt;/em&gt;&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/blogspot/gJZg?a=m7_gu_XWeyA:g_NcGYP94oY:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/blogspot/gJZg/~4/m7_gu_XWeyA" height="1" width="1" alt=""/&gt;</content>
    <link rel="replies" type="application/atom+xml" href="http://ai.googleblog.com/feeds/8564855857063590663/comments/default" title="Post Comments"/>
    <link rel="replies" type="text/html" href="http://ai.googleblog.com/2021/07/high-fidelity-image-generation-using.html#comment-form" title="0 Comments"/>
    <link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/8564855857063590663"/>
    <link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/8564855857063590663"/>
    <link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/blogspot/gJZg/~3/m7_gu_XWeyA/high-fidelity-image-generation-using.html" title="High Fidelity Image Generation Using Diffusion Models"/>
    <author>
      <name>Google AI</name>
      <uri>http://www.blogger.com/profile/12098626514775266161</uri>
      <email>noreply@blogger.com</email>
      <gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="16" height="16" src="https://img1.blogblog.com/img/b16-rounded.gif"/>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://1.bp.blogspot.com/-MRJn88KECNA/YPG5dmVIpYI/AAAAAAAAH6s/cNP2QlhlZz0xJ8yWNbpgr1JJVIXwXJ-3QCLcBGAsYHQ/s72-w640-h326-c/image4.png" height="72" width="72"/>
    <thr:total>0</thr:total>
    <feedburner:origLink>http://ai.googleblog.com/2021/07/high-fidelity-image-generation-using.html</feedburner:origLink>
  </entry>
  <entry>
    <id>tag:blogger.com,1999:blog-8474926331452026626.post-6520074264803989229</id>
    <published>2021-07-15T10:34:00.000-07:00</published>
    <updated>2021-07-15T10:34:24.759-07:00</updated>
    <category scheme="http://www.blogger.com/atom/ns#" term="Reinforcement Learning"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="Robotics"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="TPU"/>
    <title type="text">Speeding Up Reinforcement Learning with a New Physics Simulation Engine</title>
    <content type="html">&lt;span class="byline-author"&gt;Posted by C. Daniel Freeman, Senior Software Engineer and Erik Frey, Staff Software Engineer, Google Research&lt;/span&gt;&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Reinforcement_learning"&gt;Reinforcement learning&lt;/a&gt; (RL) is a &lt;a href="https://ai.googleblog.com/2019/02/long-range-robotic-navigation-via.html"&gt;popular method&lt;/a&gt; for &lt;a href="https://ai.googleblog.com/2020/05/agile-and-intelligent-locomotion-via.html"&gt;teaching robots&lt;/a&gt; to navigate and &lt;a href="https://ai.googleblog.com/2018/06/scalable-deep-reinforcement-learning.html"&gt;manipulate&lt;/a&gt; the physical world, which itself can be simplified and expressed as interactions between &lt;em&gt;rigid bodies&lt;/em&gt;&lt;sup id="fnref1"&gt;&lt;a href="#fn1" rel="footnote"&gt;&lt;span style="font-size: x-small;"&gt;1&lt;/span&gt;&lt;/a&gt;&lt;/sup&gt; (i.e., solid physical objects that do not deform when a force is applied to them). In order to facilitate the collection of training data in a practical amount of time, RL usually leverages simulation, where approximations of any number of complex objects are composed of many rigid bodies connected by joints and powered by actuators. But this poses a challenge: it frequently takes millions to billions of simulation frames for an RL agent to become proficient at even simple tasks, such as walking, using tools, or assembling toy blocks. &lt;/p&gt;  &lt;p&gt;While progress has been made to &lt;a href="https://ai.googleblog.com/2020/08/tackling-open-challenges-in-offline.html"&gt;improve training efficiency by recycling simulation frames&lt;/a&gt;, some RL tools instead sidestep this problem by distributing the generation of simulation frames across many simulators. These distributed simulation platforms yield impressive results that train very quickly, but they must run on compute clusters with thousands of CPUs or GPUs which are inaccessible to most researchers. &lt;/p&gt;&lt;p&gt;In “&lt;a href="https://arxiv.org/abs/2106.13281"&gt;Brax - A Differentiable Physics Engine for Large Scale Rigid Body Simulation&lt;/a&gt;”, we present a new physics simulation engine that matches the performance of a large compute cluster with just a single TPU or GPU. The engine is designed to both efficiently run thousands of parallel physics simulations alongside a machine learning (ML) algorithm on a single accelerator and scale millions of simulations seamlessly across pods of interconnected accelerators. We’ve &lt;a href="https://github.com/google/brax"&gt;open sourced the engine&lt;/a&gt; along with reference RL algorithms and simulation environments that are all accessible via &lt;a href="https://colab.sandbox.google.com/github/google/brax/blob/main/notebooks/training.ipynb"&gt;Colab&lt;/a&gt;. Using this new platform, we demonstrate 100-1000x faster training compared to a traditional workstation setup. &lt;/p&gt;  &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-Za-dyrqXP24/YPBB_LLLRHI/AAAAAAAAH48/2R922TQkSwsh38UEPztNA86DqAZqAMBfACLcBGAsYHQ/s1600/image1.gif" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="817" data-original-width="1600" height="326" src="https://1.bp.blogspot.com/-Za-dyrqXP24/YPBB_LLLRHI/AAAAAAAAH48/2R922TQkSwsh38UEPztNA86DqAZqAMBfACLcBGAsYHQ/w640-h326/image1.gif" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Three typical RL workflows. The&lt;b&gt; left&lt;/b&gt; shows a typical workstation flow: on a single machine, with the environment on CPU, training takes hours or days. The&lt;b&gt; middle&lt;/b&gt; shows a typical distributed simulation flow: training takes minutes by farming simulation out to thousands of machines. The&lt;b&gt; right&lt;/b&gt; shows the Brax flow: learning and large batch simulation occur side by side on a single CPU/GPU chip.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;b&gt;Physics Simulation Engine Design Opportunities&lt;/b&gt;&lt;br/&gt;Rigid body physics are used in video games, robotics, molecular dynamics, biomechanics, graphics and animation, and other domains. In order to accurately model such systems, simulators integrate forces from gravity, motor actuation, joint constraints, object collisions, and others to simulate the motion of a physical system across time. &lt;/p&gt;   &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-xzTE4RTKkHg/YPBCTEmN4CI/AAAAAAAAH5E/vgQLCCI-eKw93j46VtHAyVtgLAde7M0MgCLcBGAsYHQ/s782/image9.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="538" data-original-width="782" height="440" src="https://1.bp.blogspot.com/-xzTE4RTKkHg/YPBCTEmN4CI/AAAAAAAAH5E/vgQLCCI-eKw93j46VtHAyVtgLAde7M0MgCLcBGAsYHQ/w640-h440/image9.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Simulation of three spherical bodies, a wall, two joints, and one actuator. For each simulation timestep, forces and torques are integrated together to update the positions, rotations, and velocities of each physical body.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;Taking a closer look at how most physics simulation engines are designed today, there are a few large opportunities to improve efficiency. As we noted above, a typical robotics learning pipeline places a single learner in a tight feedback with many simulations in parallel, but upon analyzing this architecture, one finds that: &lt;/p&gt;&lt;ol&gt; &lt;li&gt;This layout imposes an enormous latency bottleneck. Because the data must travel over the network within a datacenter, the learner must wait for 10,000+ nanoseconds to fetch experience from the simulator. Were this experience instead already on the same device as the learner’s neural network, latency would drop to &amp;lt;1 nanosecond.  &lt;/li&gt;&lt;li&gt;The computation necessary for training the agent (one simulation step, followed by one update of the agent’s neural network) is overshadowed by the computation spent packaging the data (i.e., marshalling data within the engine, then into a wire format such as &lt;a href="https://developers.google.com/protocol-buffers"&gt;protobuf&lt;/a&gt;, then into &lt;a href="https://en.wikipedia.org/wiki/Transmission_Control_Protocol"&gt;TCP&lt;/a&gt; buffers, and then undoing all these steps on the learner side).  &lt;/li&gt;&lt;li&gt;The computations happening within each simulator are remarkably similar, but not exactly the same. &lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;b&gt;Brax Design&lt;/b&gt;&lt;br/&gt;In response to these observations, Brax is designed so that its physics calculations are exactly the same across each of its thousands of parallel environments by ensuring that the simulation is free of &lt;em&gt;branches&lt;/em&gt; (i.e., simulation “&lt;em&gt;if”&lt;/em&gt; logic that diverges as a result of the environment state). An example of a branch in a physics engine is the application of a contact force between a ball and a wall: different code paths will execute depending on whether the ball is touching the wall. That is, &lt;em&gt;if &lt;/em&gt;the ball contacts the wall, separate code for simulating the ball’s bounce off the wall will execute. Brax employs a mix of the following three strategies to avoid branching: &lt;/p&gt;&lt;ul&gt; &lt;li&gt;&lt;em&gt;Replace the discrete branching logic with a continuous function&lt;/em&gt;, such as approximating the ball-wall contact force using a &lt;a href="https://en.wikipedia.org/wiki/Signed_distance_function"&gt;signed distance function&lt;/a&gt;. This approach results in the most efficiency gains.  &lt;li&gt;&lt;em&gt;Evaluate the branch during JAX’s &lt;a href="https://jax.readthedocs.io/en/latest/jax.html?highlight=jit#jax.jit"&gt;just-in-time compile&lt;/a&gt;.&lt;/em&gt; Many branches based on static properties of the environment, such as whether it’s even possible for two objects to collide, may be evaluated prior to simulation time.  &lt;li&gt;&lt;em&gt;Run both sides of the branch during simulation but then select only the required results.&lt;/em&gt; Because this executes some code that isn’t ultimately used, it wastes operations compared to the above.  &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Once the calculations are guaranteed to be exactly uniform, the entire training architecture can be reduced in complexity to be executed on a single TPU or GPU. Doing so removes the computational overhead and latency of cross-machine communication.  In practice, these changes lower the cost of training by 100x-1000x for comparable workloads. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Brax Environments&lt;/b&gt;&lt;br/&gt;Environments are tiny packaged worlds that define a task for an RL agent to learn. Environments contain not only the means to simulate a world, but also functions, such as how to observe the world and the definition of the goal in that world. &lt;/p&gt;&lt;p&gt;A few standard benchmark environments have emerged in recent years for testing new RL algorithms and for evaluating the impact of those algorithms using metrics commonly understood by research scientists. Brax includes four such ready-to-use environments that come from the popular &lt;a href="https://gym.openai.com/"&gt;OpenAI gym:&lt;/a&gt; &lt;a href="https://gym.openai.com/envs/Ant-v2/"&gt;Ant&lt;/a&gt;, &lt;a href="https://gym.openai.com/envs/HalfCheetah-v2/"&gt;HalfCheetah&lt;/a&gt;, &lt;a href="https://gym.openai.com/envs/Humanoid-v2/"&gt;Humanoid&lt;/a&gt;, and &lt;a href="https://gym.openai.com/envs/Reacher-v2/"&gt;Reacher&lt;/a&gt;. &lt;/p&gt;      &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;  &lt;tbody&gt;  &lt;tr&gt;    &lt;td&gt;            &lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-w1fngfkPou0/YPBCq9tV5kI/AAAAAAAAH5M/27Xzefx15NYf0TXIcOj47pXl6lDYozIdACLcBGAsYHQ/s414/image5.gif" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="322" data-original-width="414" src="https://1.bp.blogspot.com/-w1fngfkPou0/YPBCq9tV5kI/AAAAAAAAH5M/27Xzefx15NYf0TXIcOj47pXl6lDYozIdACLcBGAsYHQ/s320/image5.gif" width="320" /&gt;&lt;/a&gt;&lt;/div&gt;         &lt;/td&gt;    &lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;    &lt;td&gt;           &lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-ynkWxgmLucE/YPBCqz_bzII/AAAAAAAAH5U/VnKz2ru6yZAgO8C0IphooJ4zup4G0JUcwCLcBGAsYHQ/s414/image4.gif" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="322" data-original-width="414" src="https://1.bp.blogspot.com/-ynkWxgmLucE/YPBCqz_bzII/AAAAAAAAH5U/VnKz2ru6yZAgO8C0IphooJ4zup4G0JUcwCLcBGAsYHQ/s320/image4.gif" width="320" /&gt;&lt;/a&gt;&lt;/div&gt;         &lt;/td&gt;    &lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;     &lt;td&gt;           &lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-1aPtvNcWAGk/YPBCqxzL8qI/AAAAAAAAH5Q/036dqbtKAJkVE_G7Yc8rxVc4A_bfJsILgCLcBGAsYHQ/s414/image12.gif" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="322" data-original-width="414" src="https://1.bp.blogspot.com/-1aPtvNcWAGk/YPBCqxzL8qI/AAAAAAAAH5Q/036dqbtKAJkVE_G7Yc8rxVc4A_bfJsILgCLcBGAsYHQ/s320/image12.gif" width="320" /&gt;&lt;/a&gt;&lt;/div&gt;         &lt;/div&gt;&lt;/td&gt;    &lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;     &lt;td&gt;           &lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-HiwPUacRKM0/YPBCrAVMpPI/AAAAAAAAH5Y/XThQkNR82poh0HMrTd1_FZ1rzNhBthd1QCLcBGAsYHQ/s414/image8.gif" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="322" data-original-width="414" src="https://1.bp.blogspot.com/-HiwPUacRKM0/YPBCrAVMpPI/AAAAAAAAH5Y/XThQkNR82poh0HMrTd1_FZ1rzNhBthd1QCLcBGAsYHQ/s320/image8.gif" width="320" /&gt;&lt;/a&gt;&lt;/div&gt;         &lt;/td&gt;  &lt;/tr&gt;     &lt;/tbody&gt;&lt;/table&gt;  &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;From left to right: Ant, HalfCheetah, Humanoid, and Reacher are popular baseline environments for RL research.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;Brax also includes three novel environments: dexterous manipulation of an object (a popular challenge in robotics), generalized locomotion (an agent that goes to a target placed anywhere around it), and a simulation of an industrial robot arm. &lt;/p&gt; &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;  &lt;tbody&gt;  &lt;tr&gt;    &lt;td&gt;           &lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-W3lXKYhaOJo/YPBDRhqXwQI/AAAAAAAAH5s/em_t2NhCBR0FY8Z9HAeOHn8NXcyqfoU-ACLcBGAsYHQ/s348/image2.gif" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="249" data-original-width="348" src="https://1.bp.blogspot.com/-W3lXKYhaOJo/YPBDRhqXwQI/AAAAAAAAH5s/em_t2NhCBR0FY8Z9HAeOHn8NXcyqfoU-ACLcBGAsYHQ/s320/image2.gif" width="320" /&gt;&lt;/a&gt;&lt;/div&gt;         &lt;/td&gt;    &lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;    &lt;td&gt;                    &lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-H9lTyVstDjc/YPBGkE6izeI/AAAAAAAAH6g/7dkkJmXV7-EQEWYPzZZ_KkxsOU6GSprTACLcBGAsYHQ/s348/image13.gif" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="249" data-original-width="348" src="https://1.bp.blogspot.com/-H9lTyVstDjc/YPBGkE6izeI/AAAAAAAAH6g/7dkkJmXV7-EQEWYPzZZ_KkxsOU6GSprTACLcBGAsYHQ/s320/image13.gif" width="320" /&gt;&lt;/a&gt;&lt;/div&gt;                  &lt;/td&gt;    &lt;td&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/td&gt;     &lt;td&gt;           &lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-RqdH8dLe84Q/YPBDRjNS5TI/AAAAAAAAH5o/ryEmGdS030UeRkD8V7gkUuMLtAMpr0ZZQCLcBGAsYHQ/s348/image6.gif" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="249" data-original-width="348" src="https://1.bp.blogspot.com/-RqdH8dLe84Q/YPBDRjNS5TI/AAAAAAAAH5o/ryEmGdS030UeRkD8V7gkUuMLtAMpr0ZZQCLcBGAsYHQ/s320/image6.gif" width="320" /&gt;&lt;/a&gt;&lt;/div&gt;            &lt;/td&gt;       &lt;/tr&gt;  &lt;/tbody&gt;&lt;/table&gt; &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;&lt;b&gt;Left:&lt;/b&gt; Grasp, a claw hand that learns dexterous manipulation. &lt;b&gt;Middle:&lt;/b&gt; Fetch, a toy, box-like dog learns a general goal-based locomotion policy. &lt;b&gt;Right:&lt;/b&gt; Simulation of &lt;a href="https://www.universal-robots.com/products/ur5-robot/"&gt;UR5e&lt;/a&gt;, an industrial robot arm.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;b&gt;Performance Benchmarks&lt;/b&gt;&lt;br/&gt;The first step for analyzing Brax’s performance is to measure the speed at which it can simulate large batches of environments, because this is the critical bottleneck to overcome in order for the learner to consume enough experience to learn quickly. &lt;/p&gt;&lt;p&gt;These two graphs below show how many physics steps (updates to the state of the environment) Brax can produce as it is tasked with simulating more and more environments in parallel. The graph on the left shows that Brax scales the number of steps per second linearly with the number of parallel environments, only hitting memory bandwidth bottlenecks at 10,000 environments, which is not only enough for training single agents, but also suitable for &lt;a href="https://deepmind.com/blog/article/population-based-training-neural-networks"&gt;training entire populations of agents&lt;/a&gt;. The graph on the right shows two things: first, that Brax performs well not only on TPU, but also on high-end GPUs (see the &lt;a href="https://www.nvidia.com/en-us/data-center/v100/"&gt;V100&lt;/a&gt; and &lt;a href="https://www.nvidia.com/en-us/data-center/tesla-p100/"&gt;P100&lt;/a&gt; curves), and second, that by leveraging JAX’s &lt;a href="https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap"&gt;device parallelism primitives&lt;/a&gt;, Brax scales seamlessly across multiple devices, reaching hundreds of millions of physics steps per second (see the &lt;em&gt;TPUv3 8x8&lt;/em&gt; curve, which is 64 &lt;a href="https://cloud.google.com/tpu"&gt;TPUv3&lt;/a&gt; chips directly connected to each other over a high speed interconnect) . &lt;/p&gt;  &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-2032wGgcsk0/YPBDjRPJaWI/AAAAAAAAH6A/MsDBJTrMH4c2klChA6WEgR1POsSsqqHfgCLcBGAsYHQ/s1418/image3.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="654" data-original-width="1418" height="296" src="https://1.bp.blogspot.com/-2032wGgcsk0/YPBDjRPJaWI/AAAAAAAAH6A/MsDBJTrMH4c2klChA6WEgR1POsSsqqHfgCLcBGAsYHQ/w640-h296/image3.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;&lt;b&gt;Left:&lt;/b&gt; Scaling of the simulation steps per second for each Brax environment on a 4x2 TPU v3. &lt;b&gt;Right:&lt;/b&gt; Scaling of the simulation steps per second for several accelerators on the Ant environment.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;Another way to analyze Brax’s performance is to measure its impact on the time it takes to run a reinforcement learning experiment on a single workstation. Here we compare Brax training the popular &lt;a href="https://github.com/google/brax/blob/main/brax/envs/ant.py"&gt;Ant&lt;/a&gt; benchmark environment to its &lt;a href="https://github.com/openai/gym/blob/master/gym/envs/mujoco/ant.py"&gt;OpenAI counterpart&lt;/a&gt;, powered by the &lt;a href="http://www.mujoco.org/"&gt;MuJoCo physics engine&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;In the graph below, the blue line represents a standard workstation setup, where a learner runs on the GPU and the simulator runs on the CPU. We see that the time it takes to train an ant to run with reasonable proficiency (a score of 4000 on the y axis) drops from about 3 hours for the blue line, to about 10 seconds using Brax on accelerator hardware. It’s interesting to note that even on CPU alone (the grey line), Brax performs more than an order of magnitude faster, benefitting from learner and simulator both sitting in the same process. &lt;/p&gt;   &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-KU3l-v2fqf0/YPBDwKsUneI/AAAAAAAAH6E/QjXVk96xT24esu0U7HfsM2Y8XlVx2w8FACLcBGAsYHQ/s1394/image10.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="646" data-original-width="1394" height="296" src="https://1.bp.blogspot.com/-KU3l-v2fqf0/YPBDwKsUneI/AAAAAAAAH6E/QjXVk96xT24esu0U7HfsM2Y8XlVx2w8FACLcBGAsYHQ/w640-h296/image10.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Brax’s optimized &lt;a href="https://arxiv.org/abs/1707.06347"&gt;PPO&lt;/a&gt; versus a standard GPU-backed PPO learning the MuJoCo-Ant-v2 environment, evaluated for 10 million steps. Note the x-axis is log-wallclock-time in seconds. Shaded region indicates lowest and highest performing seeds over 5 replicas, and solid line indicates mean.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;&lt;b&gt;Physics Fidelity&lt;/b&gt;&lt;br/&gt;Designing a &lt;a href="https://ai.googleblog.com/2017/10/closing-simulation-to-reality-gap-for.html"&gt;simulator that matches the behavior of the real world&lt;/a&gt; is a known hard problem that this work does not address. Nevertheless, it is useful to compare Brax to a reference simulator to ensure it is producing output that is at least as valid. In this case, we again compare Brax to &lt;a href="http://www.mujoco.org/"&gt;MuJoCo&lt;/a&gt;, which is well-regarded for its simulation quality. We expect to see that, all else being equal, a policy has a similar reward trajectory whether trained in MuJoCo or Brax. &lt;/p&gt;   &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-EtkcA1VQsN4/YPBD3uYpd4I/AAAAAAAAH6M/VaaiMN0IH0k4JpIfDdWtsCki7wdJ8CEnwCLcBGAsYHQ/s394/image7.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="265" data-original-width="394" height="430" src="https://1.bp.blogspot.com/-EtkcA1VQsN4/YPBD3uYpd4I/AAAAAAAAH6M/VaaiMN0IH0k4JpIfDdWtsCki7wdJ8CEnwCLcBGAsYHQ/w640-h430/image7.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;MuJoCo-Ant-v2 vs. Brax Ant, showing the number of environment steps plotted against the average episode score achieved for the environment. Both environments were trained with the same standard implementation of &lt;a href="https://arxiv.org/pdf/1812.05905.pdf"&gt;SAC&lt;/a&gt;. Shaded region indicates lowest and highest performing seeds over five runs, and solid line indicates the mean.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;    &lt;p&gt;These curves show that as the reward rises at about the same rate for both simulators, both engines compute physics with a comparable level of complexity or difficulty to solve. And as both curves top out at about the same reward, we have confidence that the same general physical limits apply to agents operating to the best of their ability in either simulation. &lt;/p&gt;&lt;p&gt;We can also measure Brax’s ability to conserve linear momentum, angular momentum, and energy. &lt;/p&gt;   &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-YbL_hH_D8t8/YPBEBWXLoRI/AAAAAAAAH6U/eAo3Drp5iKAn4QNp5PjEKpvMXUuG5t-CQCLcBGAsYHQ/s1072/image11.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="310" data-original-width="1072" height="186" src="https://1.bp.blogspot.com/-YbL_hH_D8t8/YPBEBWXLoRI/AAAAAAAAH6U/eAo3Drp5iKAn4QNp5PjEKpvMXUuG5t-CQCLcBGAsYHQ/w640-h186/image11.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Linear momentum (&lt;b&gt;left&lt;/b&gt;), angular momentum (&lt;b&gt;middle&lt;/b&gt;), and energy (&lt;b&gt;right&lt;/b&gt;) non-conservation scaling for Brax as well as several other physics engines. The y-axis indicates drift from the expected calculation (higher is smaller drift, which is better), and the x axis indicates the amount of time being simulated.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;p&gt;This measure of physics simulation quality was first &lt;a href="https://homes.cs.washington.edu/~todorov/papers/ErezICRA15.pdf"&gt;proposed by the authors of MuJoCo&lt;/a&gt; as a way to understand how the simulation drifts off course as it is tasked with computing larger and larger time steps. Here, Brax performs similarly as its neighbors. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Conclusion&lt;/b&gt;&lt;br/&gt;We invite researchers to perform a more qualitative measure of Brax’s physics fidelity by training their own policies in the &lt;a href="https://colab.research.google.com/github/google/brax/blob/main/notebooks/training.ipynb"&gt;Brax Training Colab&lt;/a&gt;. The learned trajectories are recognizably similar to those seen in OpenAI Gym. &lt;/p&gt;&lt;p&gt;Our work makes fast, scalable RL and robotics research much more accessible — what was formerly only possible via large compute clusters can now be run on workstations, or for free &lt;a href="https://colab.research.google.com/github/google/brax/blob/main/notebooks/training.ipynb"&gt;via hosted Google Colaboratory&lt;/a&gt;. &lt;a href="https://github.com/google/brax"&gt;Our Github repository&lt;/a&gt; includes not only the Brax simulation engine, but also a host of reference RL algorithms for fast training. We can’t wait to see what kind of new research Brax enables. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Acknowledgements&lt;/b&gt;&lt;br/&gt;&lt;em&gt;We'd like to thank our paper co-authors: Anton Raichuk, Sertan Girgin, Igor Mordatch, and Olivier Bachem.  We also thank Erwin Coumans for advice on building physics engines, Blake Hechtman and James Bradbury for providing optimization help with JAX and XLA, and Luke Metz and Shane Gu for their advice.  We’d also like to thank Vijay Sundaram, Wright Bagwell, Matt Leffler, Gavin Dodd, Brad Mckee, and Logan Olson, for helping to incubate this project.&lt;/em&gt;&lt;/p&gt; &lt;!--Footnotes--&gt;&lt;hr width="80%" /&gt;&lt;p&gt;  &lt;span class="Apple-style-span" style="font-size: x-small;"&gt;&lt;sup&gt;&lt;a name="fn1"&gt;&lt;b&gt;1&lt;/b&gt;&lt;/a&gt;&lt;/sup&gt; Due to the complexity of the real world, there is also ongoing research exploring the physics of &lt;a href="https://ai.googleblog.com/2021/05/learning-to-manipulate-deformable.html"&gt;deformable bodies&lt;/a&gt;.&amp;nbsp;&lt;a href="#fnref1" rev="footnote"&gt;&lt;sup&gt;↩&lt;/sup&gt;&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/blogspot/gJZg?a=n_Mjs-FnDXU:Rqc7JTNknck:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/blogspot/gJZg/~4/n_Mjs-FnDXU" height="1" width="1" alt=""/&gt;</content>
    <link rel="replies" type="application/atom+xml" href="http://ai.googleblog.com/feeds/6520074264803989229/comments/default" title="Post Comments"/>
    <link rel="replies" type="text/html" href="http://ai.googleblog.com/2021/07/speeding-up-reinforcement-learning-with.html#comment-form" title="0 Comments"/>
    <link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/6520074264803989229"/>
    <link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/6520074264803989229"/>
    <link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/blogspot/gJZg/~3/n_Mjs-FnDXU/speeding-up-reinforcement-learning-with.html" title="Speeding Up Reinforcement Learning with a New Physics Simulation Engine"/>
    <author>
      <name>Google AI</name>
      <uri>http://www.blogger.com/profile/12098626514775266161</uri>
      <email>noreply@blogger.com</email>
      <gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="16" height="16" src="https://img1.blogblog.com/img/b16-rounded.gif"/>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://1.bp.blogspot.com/-Za-dyrqXP24/YPBB_LLLRHI/AAAAAAAAH48/2R922TQkSwsh38UEPztNA86DqAZqAMBfACLcBGAsYHQ/s72-w640-h326-c/image1.gif" height="72" width="72"/>
    <thr:total>0</thr:total>
    <feedburner:origLink>http://ai.googleblog.com/2021/07/speeding-up-reinforcement-learning-with.html</feedburner:origLink>
  </entry>
  <entry>
    <id>tag:blogger.com,1999:blog-8474926331452026626.post-7000929806584590164</id>
    <published>2021-07-14T11:28:00.001-07:00</published>
    <updated>2021-07-14T11:30:40.317-07:00</updated>
    <category scheme="http://www.blogger.com/atom/ns#" term="Natural Language Processing"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="Search"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="Self-Supervised Learning"/>
    <title type="text">From Vision to Language: Semi-supervised Learning in Action…at Scale</title>
    <content type="html">&lt;span class="byline-author"&gt;Posted by Thang Luong, Staff Research Scientist, Google Research and Jingcao Hu, Senior Staff Software Engineer, Google Search&lt;/span&gt; &lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Supervised_learning"&gt;Supervised learning&lt;/a&gt;, the machine learning task of training predictive models using data points with known outcomes (i.e., labeled data), is generally the preferred approach in industry because of its simplicity. However, supervised learning requires accurately labeled data, the collection of which is often labor intensive. In addition, as model efficiency &lt;a href="https://openai.com/blog/ai-and-efficiency/"&gt;improves&lt;/a&gt; with better architectures, algorithms, and hardware (&lt;a href="https://en.wikipedia.org/wiki/Graphics_processing_unit"&gt;GPUs&lt;/a&gt; / &lt;a href="https://en.wikipedia.org/wiki/Tensor_Processing_Unit"&gt;TPUs&lt;/a&gt;), training &lt;a href="https://arxiv.org/abs/2006.16668"&gt;large models&lt;/a&gt; to achieve better quality becomes more accessible, which, in turn, requires even more labeled data for continued progress.  &lt;/p&gt;&lt;p&gt;To mitigate such data acquisition challenges, &lt;a href="https://en.wikipedia.org/wiki/Semi-supervised_learning"&gt;semi-supervised learning&lt;/a&gt;, a machine learning paradigm that combines a small amount of labeled data with a large amount of unlabeled data, has recently seen success with methods such as &lt;a href="https://ai.googleblog.com/2019/07/advancing-semi-supervised-learning-with.html"&gt;UDA&lt;/a&gt;, &lt;a href="https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html"&gt;SimCLR&lt;/a&gt;, and many others. In our previous work, we demonstrated for the first time that a semi-supervised learning approach, &lt;a href="https://arxiv.org/abs/1911.04252"&gt;Noisy Student&lt;/a&gt;, can achieve state-of-the-art performance on &lt;a href="http://www.image-net.org/"&gt;ImageNet&lt;/a&gt;, a large-scale academic benchmark for image classification, by utilizing many more unlabeled examples.  &lt;/p&gt;&lt;p&gt;Inspired by these results, today we are excited to present semi-supervised distillation (SSD), a simplified version of Noisy Student, and demonstrate its successful application to the language domain. We apply SSD to language understanding within the context of Google Search, resulting in high performance gains. This is the first successful instance of semi-supervised learning applied at such a large scale and demonstrates the potential impact of such approaches for production-scale systems.  &lt;/p&gt;&lt;p&gt;&lt;b&gt;Noisy Student Training&lt;/b&gt;&lt;br /&gt;Prior to our development of Noisy Student, there was a large body of research into semi-supervised learning. In spite of this extensive research, however, such systems typically worked well only in the low-data regime, e.g., &lt;a href="https://en.wikipedia.org/wiki/CIFAR-10"&gt;CIFAR&lt;/a&gt;, &lt;a href="http://ufldl.stanford.edu/housenumbers/"&gt;SVHN&lt;/a&gt;, and 10% &lt;a href="http://www.image-net.org/"&gt;ImageNet&lt;/a&gt;. When labeled data were abundant, such models were unable to compete with fully supervised learning systems, which prevented semi-supervised approaches from being applied to important applications in production, such as search engines and self-driving cars. This shortcoming motivated our development of &lt;a href="https://arxiv.org/abs/1911.04252"&gt;Noisy Student Training&lt;/a&gt;, a semi-supervised learning approach that worked well in the high-data regime, and at the time achieved state-of-the-art accuracy on ImageNet using 130M additional unlabeled images.  &lt;/p&gt;&lt;p&gt;Noisy Student Training has 4 simple steps: &lt;/p&gt;&lt;ol&gt;&lt;li&gt;Train a classifier (the teacher) on labeled data. &lt;/li&gt;&lt;li&gt;The teacher then infers pseudo-labels on a much larger unlabeled dataset. &lt;/li&gt;&lt;li&gt;Then, it trains a larger classifier on the combined labeled and pseudo-labeled data, while also adding noise (noisy student). &lt;/li&gt;&lt;li&gt;(Optional) Going back to step 2, the student may be used as a new teacher. &lt;/li&gt;&lt;/ol&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-P84HreAGlaM/YO3u9rxOftI/AAAAAAAAH4Y/ertrhpUxGY0kkWpoAzB0WFcWaxbvGYE5gCLcBGAsYHQ/s1854/image4.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="1046" data-original-width="1854" height="226" src="https://1.bp.blogspot.com/-P84HreAGlaM/YO3u9rxOftI/AAAAAAAAH4Y/ertrhpUxGY0kkWpoAzB0WFcWaxbvGYE5gCLcBGAsYHQ/w400-h226/image4.png" width="400" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;&lt;span style="text-align: left;"&gt;An illustration of Noisy Student Training through four simple steps. We use two types of noise: model noise (&lt;/span&gt;&lt;a href="https://jmlr.org/papers/v15/srivastava14a.html" style="text-align: left;"&gt;Dropout&lt;/a&gt;&lt;span style="text-align: left;"&gt;,&amp;nbsp;&lt;/span&gt;&lt;a href="https://arxiv.org/abs/1603.09382" style="text-align: left;"&gt;Stochastic Depth&lt;/a&gt;&lt;span style="text-align: left;"&gt;) and input noise (data augmentation, such as&amp;nbsp;&lt;/span&gt;&lt;a href="https://arxiv.org/abs/1909.13719" style="text-align: left;"&gt;RandAugment&lt;/a&gt;&lt;span style="text-align: left;"&gt;).&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;  One can view Noisy Student as a form of self-training, because the model generates pseudo-labels with which it retrains itself to improve performance. A surprising property of Noisy Student Training is that the trained models work extremely well on robustness test sets for which it was not optimized, including &lt;a href="https://arxiv.org/abs/1907.07174"&gt;ImageNet-A&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/1903.12261"&gt;ImageNet-C, and ImageNet-P&lt;/a&gt;. We hypothesize that the noise added during training not only helps with the learning, but also makes the model more robust. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-p5-t-HXLa88/YO3vn03u2OI/AAAAAAAAH4g/2QA2YZ1ce3Ig7E8yg_T6spKWhHRGB5EMQCLcBGAsYHQ/s1999/image1.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="892" data-original-width="1999" height="286" src="https://1.bp.blogspot.com/-p5-t-HXLa88/YO3vn03u2OI/AAAAAAAAH4g/2QA2YZ1ce3Ig7E8yg_T6spKWhHRGB5EMQCLcBGAsYHQ/w640-h286/image1.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Examples of images that are classified incorrectly by the baseline model, but correctly by Noisy Student. &lt;b&gt;Left:&lt;/b&gt; An unmodified image from ImageNet-A. &lt;b&gt;Middle&lt;/b&gt; and &lt;b&gt;Right:&lt;/b&gt; Images with noise added, selected from ImageNet-C. For more examples including ImageNet-P, please see the &lt;a href="https://arxiv.org/abs/1911.04252"&gt;paper&lt;/a&gt;.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;  &lt;b&gt;Connections to Knowledge Distillation&lt;/b&gt;&lt;br /&gt;Noisy Student is similar to &lt;a href="https://arxiv.org/abs/1503.02531"&gt;knowledge distillation&lt;/a&gt;, which is a process of transferring knowledge from a large model (i.e., the teacher) to a smaller model (the student). The goal of distillation is to improve speed in order to build a model that is fast to run in production without sacrificing much in quality compared to the teacher. The simplest setup for distillation involves a single teacher and uses the same data, but in practice, one can use multiple teachers or a separate dataset for the student. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-6Fw3xwmDgZs/YO31Hc9WWJI/AAAAAAAAH4o/woD6ZKVll0wkn71Z_XbHh_x9Ak0YzzaVwCLcBGAsYHQ/s1484/image2.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="522" data-original-width="1484" height="226" src="https://1.bp.blogspot.com/-6Fw3xwmDgZs/YO31Hc9WWJI/AAAAAAAAH4o/woD6ZKVll0wkn71Z_XbHh_x9Ak0YzzaVwCLcBGAsYHQ/w640-h226/image2.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;&lt;span style="text-align: left;"&gt;Simple illustrations of Noisy Student and knowledge distillation.&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;  Unlike Noisy Student, knowledge distillation does not add noise during training (e.g., data augmentation or model regularization) and typically involves a smaller student model. In contrast, one can think of Noisy Student as the process of “knowledge expansion”.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Semi-Supervised Distillation&lt;/b&gt;&lt;br /&gt;Another strategy for training production models is to apply Noisy Student training twice: first to get a larger teacher model T’ and then to derive a &lt;em&gt;smaller&lt;/em&gt; student S. This approach produces a model that is better than either training with supervised learning or with Noisy Student training alone. Specifically, when applied to the vision domain for a family of &lt;a href="https://arxiv.org/abs/1905.11946"&gt;EfficientNet&lt;/a&gt; models, ranging from EfficientNet-B0 with 5.3M parameters to EfficientNet-B7 with 66M parameters, this strategy achieves much better performance for each given model size  (see Table 9 of the &lt;a href="https://arxiv.org/abs/1911.04252"&gt;Noisy Student paper&lt;/a&gt; for more details). &lt;/p&gt;&lt;p&gt;Noisy Student training needs data augmentation, e.g., &lt;a href="https://arxiv.org/abs/1909.13719"&gt;RandAugment&lt;/a&gt; (for vision) or &lt;a href="https://arxiv.org/abs/1904.08779"&gt;SpecAugment&lt;/a&gt; (for speech), to work well. But in certain applications, e.g., natural language processing, such types of input noise are not readily available. For those applications, Noisy Student Training can be simplified to have no noise. In that case, the above two-stage process becomes a simpler method, which we call Semi-Supervised Distillation (SSD). First, the teacher model infers pseudo-labels on the unlabeled dataset from which we then train a new teacher model (T’) that is of &lt;em&gt;equal-or-larger&lt;/em&gt; size than the original teacher model. This step, which is essentially self-training, is then followed by knowledge distillation to produce a smaller  student model for production.  &lt;/p&gt; &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-jsvnvPxipNI/YO31TKpwRhI/AAAAAAAAH4s/Tk8NjSteu8A9J-F_xHKHA37QtLVFpcYFACLcBGAsYHQ/s998/image5.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="412" data-original-width="998" height="264" src="https://1.bp.blogspot.com/-jsvnvPxipNI/YO31TKpwRhI/AAAAAAAAH4s/Tk8NjSteu8A9J-F_xHKHA37QtLVFpcYFACLcBGAsYHQ/w640-h264/image5.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;&lt;span style="text-align: left;"&gt;An illustration of Semi-Supervised Distillation (SSD), a 2-stage process that self-trains an equal-or-larger teacher (T’) before distilling to a student (S).&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;  &lt;b&gt;Improving Search&lt;/b&gt;&lt;br /&gt;Having succeeded in the vision domain, an application in the language understanding domain, like Google Search, is a logical next step with broader user impact. In this case, we focus on an important ranking component in Search, which builds on &lt;a href="https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html"&gt;BERT&lt;/a&gt; to &lt;a href="https://blog.google/products/search/search-language-understanding-bert/"&gt;better understand languages&lt;/a&gt;. This task turns out to be well-suited for SSD. Indeed, applying SSD to the ranking component to better understand the relevance of candidate search results to queries achieved one of the highest performance gains among top launches at Search in 2020. Below is an example of a query where the improved model demonstrates better language understanding.  &lt;/p&gt; &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-_H-Li7Myyxo/YO31deYvthI/AAAAAAAAH40/bLLq9E9vEEsnOsSO1t5M7Gcb7S0fERiwgCLcBGAsYHQ/s1435/image3.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="645" data-original-width="1435" height="288" src="https://1.bp.blogspot.com/-_H-Li7Myyxo/YO31deYvthI/AAAAAAAAH40/bLLq9E9vEEsnOsSO1t5M7Gcb7S0fERiwgCLcBGAsYHQ/w640-h288/image3.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;&lt;span style="text-align: left;"&gt;With the implementation of SSD, Search is able to find documents that are more relevant to user queries.&lt;/span&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;  &lt;b&gt;Future Research &amp;amp; Challenges&lt;/b&gt;&lt;br /&gt;We have presented a successful instance of semi-supervised distillation (SSD) in the production scale setting of Search. We believe SSD will continue changing the landscape of machine learning usage in the industry from predominantly supervised learning to semi-supervised learning. While our results are promising, there is still much research needed in how to efficiently utilize unlabeled examples in the real world, which is often noisy, and apply them to various domains. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Acknowledgements&lt;/b&gt;&lt;br /&gt;&lt;em&gt;Zhenshuai Ding, Yanping Huang, Elizabeth Tucker, Hai Qian, and Steve He contributed immensely to this successful launch. The project would not have succeeded without contributions from members of both the Brain and Search teams: Shuyuan Zhang, Rohan Anil, Zhifeng Chen, Rigel Swavely, Chris Waterson, Avinash Atreya. Thanks to Qizhe Xie and Zihang Dai for feedback on the work. Also, thanks to Quoc Le, Yonghui Wu, Sundeep Tirumalareddy, Alexander Grushetsky,  Pandu Nayak for their leadership support. &lt;/em&gt;&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/blogspot/gJZg?a=z3nwWEfus8M:D8RmJOVAXUg:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/blogspot/gJZg/~4/z3nwWEfus8M" height="1" width="1" alt=""/&gt;</content>
    <link rel="replies" type="application/atom+xml" href="http://ai.googleblog.com/feeds/7000929806584590164/comments/default" title="Post Comments"/>
    <link rel="replies" type="text/html" href="http://ai.googleblog.com/2021/07/from-vision-to-language-semi-supervised.html#comment-form" title="0 Comments"/>
    <link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/7000929806584590164"/>
    <link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/7000929806584590164"/>
    <link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/blogspot/gJZg/~3/z3nwWEfus8M/from-vision-to-language-semi-supervised.html" title="From Vision to Language: Semi-supervised Learning in Action…at Scale"/>
    <author>
      <name>Google AI</name>
      <uri>http://www.blogger.com/profile/12098626514775266161</uri>
      <email>noreply@blogger.com</email>
      <gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="16" height="16" src="https://img1.blogblog.com/img/b16-rounded.gif"/>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://1.bp.blogspot.com/-P84HreAGlaM/YO3u9rxOftI/AAAAAAAAH4Y/ertrhpUxGY0kkWpoAzB0WFcWaxbvGYE5gCLcBGAsYHQ/s72-w400-h226-c/image4.png" height="72" width="72"/>
    <thr:total>0</thr:total>
    <feedburner:origLink>http://ai.googleblog.com/2021/07/from-vision-to-language-semi-supervised.html</feedburner:origLink>
  </entry>
  <entry>
    <id>tag:blogger.com,1999:blog-8474926331452026626.post-3503000124032758615</id>
    <published>2021-07-13T11:12:00.001-07:00</published>
    <updated>2021-07-13T11:34:15.150-07:00</updated>
    <category scheme="http://www.blogger.com/atom/ns#" term="Reinforcement Learning"/>
    <title type="text">Reducing the Computational Cost of Deep Reinforcement Learning Research</title>
    <content type="html">&lt;span class="byline-author"&gt;Posted by Pablo Samuel Castro, Staff Software Engineer, Google Research&lt;/span&gt; &lt;p&gt;It is widely accepted that the enormous growth of deep reinforcement learning research, which combines traditional &lt;a href="https://en.wikipedia.org/wiki/Reinforcement_learning"&gt;reinforcement learning&lt;/a&gt; with &lt;a href="https://en.wikipedia.org/wiki/Deep_learning#Deep_neural_networks"&gt;deep neural networks&lt;/a&gt;, began with the publication of the seminal &lt;a href="https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning"&gt;DQN&lt;/a&gt; algorithm. This paper demonstrated the potential of this combination, showing that it could produce agents that could play a number of Atari 2600 games very effectively. Since then, there have been &lt;a href="https://arxiv.org/abs/1509.06461"&gt;several&lt;/a&gt; &lt;a href="https://arxiv.org/abs/1706.10295"&gt;approaches&lt;/a&gt; that have built on and improved the original DQN. The popular &lt;a href="https://arxiv.org/abs/1710.02298"&gt;Rainbow algorithm&lt;/a&gt; combined a number of these recent advances to achieve state-of-the-art performance on the &lt;a href="https://arxiv.org/abs/1207.4708"&gt;ALE benchmark&lt;/a&gt;. This advance, however, came at a very high computational cost, which has the unfortunate side effect of widening the gap between those with ample access to computational resources and those without. &lt;/p&gt;&lt;p&gt;In “&lt;a href="https://arxiv.org/abs/2011.14826"&gt;Revisiting Rainbow: Promoting more Insightful and Inclusive Deep Reinforcement Learning Research&lt;/a&gt;”, to be presented at &lt;a href="https://icml.cc/"&gt;ICML 2021&lt;/a&gt;, we revisit this algorithm on a set of small- and medium-sized tasks. We first discuss the computational cost associated with the Rainbow algorithm. We explore how the same conclusions regarding the benefits of combining the various algorithmic components can be reached with smaller-scale experiments, and further generalize that idea to how research done on a smaller computational budget can provide valuable scientific insights. &lt;/p&gt;&lt;p&gt;&lt;b&gt;The Cost of Rainbow&lt;/b&gt;&lt;br/&gt;A major reason for the computational cost of Rainbow is that the standards in academic publishing often require evaluating new algorithms on large benchmarks like &lt;a href="https://arxiv.org/abs/1207.4708"&gt;ALE&lt;/a&gt;, which consists of 57 Atari 2600 games that reinforcement learning agents may learn to play. For a typical game, it takes roughly five days to train a model using a &lt;a href="https://www.nvidia.com/en-us/data-center/tesla-p100/"&gt;Tesla P100 GPU&lt;/a&gt;. Furthermore, if one wants to establish meaningful confidence bounds, it is common to perform at least five independent runs. Thus, to train Rainbow on the full suite of 57 games required around 34,200 GPU hours (or 1425 days) in order to provide convincing empirical performance statistics. In other words, such experiments are only feasible if one is able to train on multiple GPUs in parallel, which can be prohibitive for smaller research groups.  &lt;/p&gt;&lt;p&gt;&lt;b&gt;Revisiting Rainbow&lt;/b&gt;&lt;br/&gt;As in the original Rainbow paper, we evaluate the effect of adding the following components to the original DQN algorithm: &lt;a href="https://arxiv.org/abs/1509.06461"&gt;double Q-learning&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/1511.05952"&gt;prioritized experience replay&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/1511.06581"&gt;dueling networks&lt;/a&gt;, &lt;a href="https://link.springer.com/article/10.1007/BF00115009"&gt;multi-step learning&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/1707.06887"&gt;distributional RL&lt;/a&gt;, and &lt;a href="https://arxiv.org/abs/1706.10295"&gt;noisy nets&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;We evaluate on a set of four classic control environments, which can be fully trained in 10-20 minutes (compared to five days for ALE games): &lt;/p&gt;   &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-V4326gyjRSs/YO2jV38rGvI/AAAAAAAAH3w/2mpLzssovDIrH94hMACOci0Zc8iMNMjMACLcBGAsYHQ/s1065/image2.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="771" data-original-width="1065" height="464" src="https://1.bp.blogspot.com/-V4326gyjRSs/YO2jV38rGvI/AAAAAAAAH3w/2mpLzssovDIrH94hMACOci0Zc8iMNMjMACLcBGAsYHQ/w640-h464/image2.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;&lt;b&gt;Upper left: &lt;/b&gt;In &lt;a href="https://gym.openai.com/envs/CartPole-v1/"&gt;CartPole&lt;/a&gt;, the task is to balance a pole on a cart that the agent can move left and right. &lt;b&gt;Upper right:&lt;/b&gt; In &lt;a href="https://gym.openai.com/envs/Acrobot-v1/"&gt;Acrobot&lt;/a&gt;, there are two arms and two joints, where the agent applies force to the joint between the two arms in order to raise the lower arm above a threshold. &lt;b&gt;Lower left:&lt;/b&gt; In &lt;a href="https://gym.openai.com/envs/LunarLander-v2/"&gt;LunarLander&lt;/a&gt;, the agent is meant to land the spaceship between the two flags. &lt;b&gt;Lower right:&lt;/b&gt; In &lt;a href="https://gym.openai.com/envs/MountainCar-v0/"&gt;MountainCar&lt;/a&gt;, the agent must build up momentum between two hills to drive to the top of the rightmost hill.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;     &lt;p&gt;We investigated the effect of both independently adding each of the components to DQN, as well as removing each from the full Rainbow algorithm. As in the original Rainbow paper, we find that, in aggregate, the addition of each of these algorithms does improve learning over the base DQN. However, we also found some important differences, such as the fact that &lt;a href="https://arxiv.org/abs/1707.06887"&gt;distributional RL&lt;/a&gt; — commonly thought to be a positive addition on its own — does not always yield improvements on its own. Indeed, in contrast to the ALE results in the Rainbow paper, in the classic control environments, distributional RL only yields an improvement &lt;em&gt;when combined with another component&lt;/em&gt;.  &lt;/p&gt;   &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-gw5s7FbNvaI/YO2j6DC8UDI/AAAAAAAAH34/ZxJ1D2fJR1EPrcj4cDvEZjRzoMRlXAcugCLcBGAsYHQ/s991/image1.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="952" data-original-width="991" height="614" src="https://1.bp.blogspot.com/-gw5s7FbNvaI/YO2j6DC8UDI/AAAAAAAAH34/ZxJ1D2fJR1EPrcj4cDvEZjRzoMRlXAcugCLcBGAsYHQ/w640-h614/image1.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Each plot shows the training progress when adding the various components to DQN. The x-axis is training steps,the y-axis is performance (higher is better).&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-86hRRLWGWAs/YO3HAyHEvqI/AAAAAAAAH4A/977Qa_n0KyUoLq0IABie30au1Bo3ZedIQCLcBGAsYHQ/s991/image3.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="952" data-original-width="991" height="614" src="https://1.bp.blogspot.com/-86hRRLWGWAs/YO3HAyHEvqI/AAAAAAAAH4A/977Qa_n0KyUoLq0IABie30au1Bo3ZedIQCLcBGAsYHQ/w640-h614/image3.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Each plot shows the training progress when removing the various components from Rainbow. The x-axis is training steps,the y-axis is performance (higher is better).&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;    &lt;p&gt;We also re-ran the Rainbow experiments on the &lt;a href="https://github.com/kenjyoung/MinAtar"&gt;MinAtar environment&lt;/a&gt;, which consists of a set of five miniaturized Atari games, and found qualitatively similar results. The MinAtar games are roughly 10 times faster to train than the regular Atari 2600 games on which the original Rainbow algorithm was evaluated, but still share some interesting aspects, such as game dynamics and having pixel-based inputs to the agent. As such, they provide a challenging mid-level environment, in between the classic control and the full Atari 2600 games. &lt;/p&gt;&lt;p&gt;When viewed in aggregate, we find our results to be consistent with those of &lt;a href="https://arxiv.org/abs/1710.02298"&gt;the original Rainbow paper&lt;/a&gt; — the impact resulting from each algorithmic component can vary from environment to environment. If we were to suggest a single agent that balances the tradeoffs of the different algorithmic components, our version of Rainbow would likely be consistent with the original, in that combining all components produces a better overall agent. However, there are important details in the variations of the different algorithmic components that merit a more thorough investigation.  &lt;/p&gt;&lt;p&gt;&lt;b&gt;Beyond the Rainbow&lt;/b&gt;&lt;br/&gt;When DQN was introduced, it made use of the &lt;a href="https://en.wikipedia.org/wiki/Huber_loss"&gt;Huber loss&lt;/a&gt; and the &lt;a href="https://keras.io/api/optimizers/rmsprop/"&gt;RMSProp Optimizer&lt;/a&gt;. It has been common practice for researchers to use these same choices when building on DQN, as most of their effort is spent on other algorithmic design decisions. In the spirit of reassessing these assumptions, we revisited the &lt;a href="https://en.wikipedia.org/wiki/Loss_function"&gt;loss function&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/Mathematical_optimization"&gt;optimizer&lt;/a&gt; used by &lt;a href="https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning"&gt;DQN&lt;/a&gt; on a lower-cost, small-scale classic control and MinAtar environments. We ran some initial experiments using the &lt;a href="https://arxiv.org/abs/1412.6980"&gt;Adam optimizer&lt;/a&gt;, which has lately been the most popular optimizer choice, combined with a simpler loss function, the &lt;a href="https://en.wikipedia.org/wiki/Mean_squared_error"&gt;mean-squared error loss&lt;/a&gt; (MSE). Since the selection of optimizer and loss function is often overlooked when developing a new algorithm, we were surprised to see that we observed a dramatic improvement on all the classic control and MinAtar environments. &lt;/p&gt;&lt;p&gt;We thus decided to evaluate the different ways of combining the two optimizers (RMSProp and Adam) with the two losses (Huber and MSE) on the full ALE suite (60 Atari 2600 games). We found that Adam+MSE is a superior combination than RMSProp+Huber. &lt;/p&gt;  &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-kTFSYKdJ0Eo/YO3HYhr4x3I/AAAAAAAAH4M/Av5cOBT8adcfej9GMfKBszIKjSRQNSrTwCLcBGAsYHQ/s1999/image4.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="648" data-original-width="1999" height="208" src="https://1.bp.blogspot.com/-kTFSYKdJ0Eo/YO3HYhr4x3I/AAAAAAAAH4M/Av5cOBT8adcfej9GMfKBszIKjSRQNSrTwCLcBGAsYHQ/w640-h208/image4.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Measuring the improvement Adam+MSE gives over the default DQN settings (RMSProp + Huber); higher is better.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;p&gt;Additionally, when comparing the various optimizer-loss combinations, we find that when using RMSProp, the Huber loss tends to perform better than MSE (illustrated by the gap between the solid and dotted orange lines). &lt;/p&gt;  &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-vQSdnchQtik/YO3HStmaeJI/AAAAAAAAH4I/DfEzo0J0hXwPGrG9lEG97LGi28xjXDMXwCLcBGAsYHQ/s1999/image5.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="551" data-original-width="1999" height="176" src="https://1.bp.blogspot.com/-vQSdnchQtik/YO3HStmaeJI/AAAAAAAAH4I/DfEzo0J0hXwPGrG9lEG97LGi28xjXDMXwCLcBGAsYHQ/w640-h176/image5.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Normalized scores aggregated over all 60 Atari 2600 games, comparing the different optimizer-loss combinations.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;    &lt;p&gt;&lt;b&gt;Conclusion&lt;/b&gt;&lt;br/&gt;On a limited computational budget we were able to reproduce, at a high-level, the findings of &lt;a href="https://arxiv.org/abs/1710.02298"&gt;the Rainbow paper&lt;/a&gt; and uncover new and interesting phenomena. Evidently it is much easier to revisit something than to discover it in the first place. Our intent with this work, however, was to argue for the relevance and significance of empirical research on small- and medium-scale environments. We believe that these less computationally intensive environments lend themselves well to a more critical and thorough analysis of the performance, behaviors, and intricacies of new algorithms. &lt;/p&gt;&lt;p&gt;We are by no means calling for less emphasis to be placed on large-scale benchmarks. We are simply urging researchers to consider smaller-scale environments as a valuable tool in their investigations, and reviewers to avoid dismissing empirical work that focuses on smaller-scale environments. By doing so, in addition to reducing the environmental impact of our experiments, we will get both a clearer picture of the research landscape and reduce the barriers for researchers from diverse and often underresourced communities, which can only help make our community and scientific advances stronger. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Acknowledgments&lt;/b&gt;&lt;br/&gt;&lt;em&gt;Thank you to Johan, the first author of this paper, for his hard work and persistence in seeing this through! We would also like to thank Marlos C. Machado, Sara Hooker, Matthieu Geist, Nino Vieillard, Hado van Hasselt, Eleni Triantafillou, and Brian Tanner for their insightful comments on this work.&lt;/em&gt;&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/blogspot/gJZg?a=CqWWMkz8b0M:WRFG0AlOJeQ:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/blogspot/gJZg/~4/CqWWMkz8b0M" height="1" width="1" alt=""/&gt;</content>
    <link rel="replies" type="application/atom+xml" href="http://ai.googleblog.com/feeds/3503000124032758615/comments/default" title="Post Comments"/>
    <link rel="replies" type="text/html" href="http://ai.googleblog.com/2021/07/reducing-computational-cost-of-deep.html#comment-form" title="0 Comments"/>
    <link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/3503000124032758615"/>
    <link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/3503000124032758615"/>
    <link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/blogspot/gJZg/~3/CqWWMkz8b0M/reducing-computational-cost-of-deep.html" title="Reducing the Computational Cost of Deep Reinforcement Learning Research"/>
    <author>
      <name>Google AI</name>
      <uri>http://www.blogger.com/profile/12098626514775266161</uri>
      <email>noreply@blogger.com</email>
      <gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="16" height="16" src="https://img1.blogblog.com/img/b16-rounded.gif"/>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://1.bp.blogspot.com/-V4326gyjRSs/YO2jV38rGvI/AAAAAAAAH3w/2mpLzssovDIrH94hMACOci0Zc8iMNMjMACLcBGAsYHQ/s72-w640-h464-c/image2.png" height="72" width="72"/>
    <thr:total>0</thr:total>
    <feedburner:origLink>http://ai.googleblog.com/2021/07/reducing-computational-cost-of-deep.html</feedburner:origLink>
  </entry>
  <entry>
    <id>tag:blogger.com,1999:blog-8474926331452026626.post-756990546465936835</id>
    <published>2021-06-29T10:20:00.003-07:00</published>
    <updated>2021-06-30T14:53:23.948-07:00</updated>
    <category scheme="http://www.blogger.com/atom/ns#" term="API"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="Machine Learning"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="Reinforcement Learning"/>
    <title type="text">Quickly Training Game-Playing Agents with Machine Learning</title>
    <content type="html">&lt;span class="byline-author"&gt;Posted by Leopold Haller and Hernan Moraldo, Software Engineers, Google Research&lt;/span&gt; &lt;p&gt;In the last two decades, dramatic advances in compute and connectivity have allowed game developers to create works of ever-increasing scope and complexity. Simple linear levels have evolved into photorealistic open worlds, procedural algorithms have enabled games with unprecedented variety, and expanding internet access has transformed games into dynamic online services. Unfortunately, scope and complexity have grown more rapidly than the size of quality assurance teams or the capabilities of traditional automated testing. This poses a challenge to both product quality (such as delayed releases and post-launch patches) and developer quality of life. &lt;/p&gt;&lt;p&gt;Machine learning (ML) techniques offer a possible solution, as they have demonstrated the potential to profoundly impact game development flows — they can help designers &lt;a href="https://ai.googleblog.com/2021/03/leveraging-machine-learning-for-game.html"&gt;balance their game&lt;/a&gt; and empower artists to &lt;a href="https://ai.googleblog.com/2020/11/using-gans-to-create-fantastical.html"&gt;produce high-quality assets&lt;/a&gt; in a fraction of the time traditionally required. Furthermore, they can be used to train challenging opponents that can &lt;a href="https://openai.com/projects/five/"&gt;compete&lt;/a&gt; at the &lt;a href="https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii"&gt;highest levels of play&lt;/a&gt;. Yet some ML techniques can pose requirements that currently make them impractical for production game teams, including the design of game-specific network architectures, the development of expertise in implementing ML algorithms, or the generation of billions of frames of training data. Conversely, game developers operate in a setting that offers unique advantages to leverage ML techniques, such as direct access to the game source, an abundance of expert demonstrations, and the uniquely interactive nature of video games. &lt;/p&gt;&lt;p&gt;Today, we present an ML-based system that game developers can use to quickly and efficiently train game-testing agents, helping developers find serious bugs quickly, while allowing human testers to focus on more complex and intricate problems. The resulting solution requires no ML expertise, works on many of the most popular game genres, and can train an ML policy, which generates game actions from the game state, in less than an hour on a single game instance. We have also released an &lt;a href="https://github.com/google-research/falken"&gt;open source library&lt;/a&gt; that demonstrates a functional application of these techniques. &lt;/p&gt;   &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-dMg144dMPXA/YNsaMJgPaoI/AAAAAAAAH08/NxPKw1dbD1IcVsM4PdE8Sw6HK_ksYuG4QCLcBGAsYHQ/s1974/Screen%2BShot%2B2021-06-29%2Bat%2B9.00.11%2BAM.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="474" data-original-width="1974" height="640" src="https://1.bp.blogspot.com/-dMg144dMPXA/YNsaMJgPaoI/AAAAAAAAH08/NxPKw1dbD1IcVsM4PdE8Sw6HK_ksYuG4QCLcBGAsYHQ/w640-h640/Screen%2BShot%2B2021-06-29%2Bat%2B9.00.11%2BAM.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Supported genres include arcade, action/adventure, and racing games.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;    &lt;p&gt;&lt;b&gt;The Right Tool for the Right Job&lt;/b&gt;&lt;br /&gt;The most elemental form of &lt;a href="https://en.wikipedia.org/wiki/Game_testing"&gt;video game testing&lt;/a&gt; is to simply play the game. A lot. Many of the most serious bugs (such as crashes or falling out of the world) are easy to detect and fix; the challenge is finding them within the vast state space of a modern game. As such, we decided to focus on training a system that could “just play the game” at scale. &lt;/p&gt;&lt;p&gt;We found that the most effective way to do this was not to try to train a single, super-effective agent that could play the entire game from end-to-end, but to provide developers with the ability to train an ensemble of game-testing agents, each of which could effectively accomplish tasks of a few minutes each, which game developers refer to as “gameplay loops”. &lt;/p&gt;      &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-qgidPnQTJBg/YNsajA8IGoI/AAAAAAAAH1E/-P26GB8K1ewoM3JK627vY4aI7Y3yj6OvwCLcBGAsYHQ/s1500/image5.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="1024" data-original-width="1500" height="436" src="https://1.bp.blogspot.com/-qgidPnQTJBg/YNsajA8IGoI/AAAAAAAAH1E/-P26GB8K1ewoM3JK627vY4aI7Y3yj6OvwCLcBGAsYHQ/w640-h436/image5.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;   These core gameplay behaviors are often expensive to program through traditional means, but are much more efficient to train than a single end-to-end ML model. In practice, commercial games create longer loops by repeating and remixing core gameplay loops, which means that developers can test large stretches of gameplay by combining ML policies with a small amount of simple scripting. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Simulation-centric, Semantic API&lt;/b&gt;&lt;br /&gt;One of the most fundamental challenges in applying ML to game development is bridging the chasm between the simulation-centric world of video games and the data-centric world of ML. Rather than ask developers to directly convert the game state into custom, low-level ML features (which would be too labor intensive) or attempting to learn from raw pixels (which would require too much data to train), our system provides developers with an idiomatic, game-developer friendly API that allows them to describe their game in terms of the essential state that a player observes and the semantic actions they can perform. All of this information is expressed via concepts that are familiar to game developers, such as &lt;a href="https://en.wikipedia.org/wiki/Entity_component_system"&gt;entities&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Ray_casting#Concept"&gt;raycasts&lt;/a&gt;, 3D positions and rotations, buttons and joysticks.  &lt;/p&gt;&lt;p&gt;As you can see in the example below, the API allows the specification of observations and actions in just a few lines of code. &lt;/p&gt;  &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-XUGqrMT_HT8/YNsa3lQ3WQI/AAAAAAAAH1M/tUIkskRHXaAEMD4j0tyIKjpFV-6VljrjQCLcBGAsYHQ/s1792/Screen%2BShot%2B2021-06-29%2Bat%2B9.00.47%2BAM.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="1006" data-original-width="1792" height="360" src="https://1.bp.blogspot.com/-XUGqrMT_HT8/YNsa3lQ3WQI/AAAAAAAAH1M/tUIkskRHXaAEMD4j0tyIKjpFV-6VljrjQCLcBGAsYHQ/w640-h360/Screen%2BShot%2B2021-06-29%2Bat%2B9.00.47%2BAM.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Example actions and observations for a racing game.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;    &lt;p&gt;&lt;b&gt;From API to Neural Network&lt;/b&gt;&lt;br /&gt;This high level, semantic API is not just easy to use, but also allows the system to flexibly adapt to the specific game being developed — the specific combination of API building blocks employed by the game developer informs our choice of network architecture, since it provides information about the type of gaming scenario in which the system is deployed. Some examples of this include: handling action outputs differently depending on whether they represent a digital button or analog joystick, or using &lt;a href="https://en.wikipedia.org/wiki/Convolutional_neural_network"&gt;techniques from image processing&lt;/a&gt; to handle observations that result from an agent probing its environment with raycasts (similar to how autonomous vehicles probe their environment with &lt;a href="https://en.wikipedia.org/wiki/Lidar"&gt;LIDAR&lt;/a&gt;).&lt;br /&gt;&lt;/p&gt;&lt;p&gt;Our API is sufficiently general to allow modeling of many common control-schemes (the configuration of action outputs that control movement) in games, such as first-person games, third-person games with camera-relative controls, racing games, twin stick shooters, etc. Since 3D movement and aiming are often an integral aspect of gameplay in general, we create networks that automatically tend towards simple behaviors such as aiming, approach or avoidance in these games. The system accomplishes this by analyzing the game’s control scheme to create neural network layers that perform custom processing of observations and actions in that game. For example, positions and rotations of objects in the world are automatically translated into directions and distances from the point of view of the AI-controlled game entity. This transformation typically increases the speed of learning and helps the learned network generalize better. &lt;/p&gt; &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-GrV5ey1Djtk/YNzngDvmC9I/AAAAAAAAH1g/M7a2ky1OzXAFsiqU9aUadMBhKlMv6mg1wCLcBGAsYHQ/s525/neural%2Bnetwork%2Bnot%2Btransparent.jpg" imageanchor="1" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="247" data-original-width="525" height="302" src="https://1.bp.blogspot.com/-GrV5ey1Djtk/YNzngDvmC9I/AAAAAAAAH1g/M7a2ky1OzXAFsiqU9aUadMBhKlMv6mg1wCLcBGAsYHQ/w640-h302/neural%2Bnetwork%2Bnot%2Btransparent.jpg" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;An example neural network generated for a game with joystick controls and raycast inputs. Depending on the inputs (&lt;b&gt;red&lt;/b&gt;) and the control scheme, the system generates custom pre- and post-processing layers (&lt;b&gt;orange&lt;/b&gt;).&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;&lt;b&gt;Learning From The Experts in Real Time&lt;/b&gt;&lt;br /&gt;After generating a neural network architecture, the network needs to be trained to play the game using an appropriate choice of learning algorithm. &lt;/p&gt;&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Reinforcement_learning#:~:text=Reinforcement%20learning%20(RL)%20is%20an,supervised%20learning%20and%20unsupervised%20learning."&gt;Reinforcement learning&lt;/a&gt; (RL), in which an ML policy is trained directly to maximize a reward, may seem like the obvious choice since they have been successfully used to train &lt;a href="https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii"&gt;highly competent ML policies&lt;/a&gt; for games. However, RL algorithms tend to require more data than a single game instance can produce in a reasonable amount of time, and achieving good results in a new domain often requires &lt;a href="https://en.wikipedia.org/wiki/Hyperparameter_optimization"&gt;hyperparameter tuning&lt;/a&gt; and strong ML domain knowledge. &lt;/p&gt;&lt;p&gt;Instead, we found that &lt;a href="https://arxiv.org/abs/1811.06711"&gt;imitation learning&lt;/a&gt; (IL), which trains ML policies based  by observing experts play the game, works well for our use case. Unlike RL, where the agent needs to discover a good policy on its own, IL only needs to recreate the behavior of a human expert. Since game developers and testers are experts in their own games, they can easily provide demonstrations of how to play the game. &lt;/p&gt;&lt;p&gt;We use an IL approach inspired by the &lt;a href="https://arxiv.org/abs/1011.0686"&gt;DAgger algorithm&lt;/a&gt;, which allows us to take advantage of video games’ most compelling quality — interactivity. Thanks to the reductions in training time and data requirements enabled by our semantic API, training is effectively realtime, giving a developer the ability to fluidly switch between providing gameplay demonstrations and watching the system play. This results in a natural feedback loop, in which a developer iteratively provides corrections to a continuous stream of ML policies. &lt;/p&gt;&lt;p&gt;From the developer’s perspective, providing a demonstration or a correction to faulty behavior is as simple as picking up the controller and starting to play the game. Once they are done, they can put the controller down and watch the ML policy play. The result is a training experience that is real-time, interactive, highly experiential, and, very often, more than a little fun.  &lt;/p&gt;   &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-p3BAq08zio0/YNsbP2z9YwI/AAAAAAAAH1Y/zFdCODpDUewyr2VjAAAwxqB2r8jspqKUgCLcBGAsYHQ/s674/image4.gif" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="674" data-original-width="674" height="400" src="https://1.bp.blogspot.com/-p3BAq08zio0/YNsbP2z9YwI/AAAAAAAAH1Y/zFdCODpDUewyr2VjAAAwxqB2r8jspqKUgCLcBGAsYHQ/w400-h400/image4.gif" width="400" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;ML policy for an FPS game, trained with our system.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;p&gt;&lt;b&gt;Conclusion&lt;/b&gt;&lt;br /&gt;We present a system that combines a high-level semantic API with a &lt;a href="https://arxiv.org/abs/1011.0686"&gt;DAgger&lt;/a&gt;-inspired interactive training flow that enables training of useful ML policies for video game testing in a wide variety of genres.  We have released an &lt;a href="https://github.com/google-research/falken"&gt;open source library&lt;/a&gt; as a functional illustration of our system. No ML expertise is required and training of agents for test applications often takes less than an hour on a single developer machine. We hope that this work will help inspire the development of ML techniques that can be deployed in real-world game-development flows in ways that are accessible, effective, and fun to use.  &lt;/p&gt;&lt;p&gt;&lt;b&gt;Acknowledgements&lt;/b&gt;&lt;br /&gt;&lt;em&gt;We’d like to thank the core members of the project: Dexter Allen, Leopold Haller, Nathan Martz, Hernan Moraldo, Stewart Miles and Hina Sakazaki. Training algorithms are provided by &lt;a href="https://www.tensorflow.org/agents"&gt;TF Agents&lt;/a&gt;, and on-device inference by &lt;a href="https://www.tensorflow.org/lite"&gt;TF Lite&lt;/a&gt;. Special thanks to our research advisors, Olivier Bachem, Erik Frey, and Toby Pohlen, and to Eugene Brevdo, Jared Duke, Oscar Ramirez and Neal Wu who provided helpful guidance and support.&lt;/em&gt;&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/blogspot/gJZg?a=NH30BgHCkCw:v5g299ej8TQ:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/blogspot/gJZg/~4/NH30BgHCkCw" height="1" width="1" alt=""/&gt;</content>
    <link rel="replies" type="application/atom+xml" href="http://ai.googleblog.com/feeds/756990546465936835/comments/default" title="Post Comments"/>
    <link rel="replies" type="text/html" href="http://ai.googleblog.com/2021/06/quickly-training-game-playing-agents.html#comment-form" title="0 Comments"/>
    <link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/756990546465936835"/>
    <link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/756990546465936835"/>
    <link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/blogspot/gJZg/~3/NH30BgHCkCw/quickly-training-game-playing-agents.html" title="Quickly Training Game-Playing Agents with Machine Learning"/>
    <author>
      <name>Google AI</name>
      <uri>http://www.blogger.com/profile/12098626514775266161</uri>
      <email>noreply@blogger.com</email>
      <gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="16" height="16" src="https://img1.blogblog.com/img/b16-rounded.gif"/>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://1.bp.blogspot.com/-dMg144dMPXA/YNsaMJgPaoI/AAAAAAAAH08/NxPKw1dbD1IcVsM4PdE8Sw6HK_ksYuG4QCLcBGAsYHQ/s72-w640-h640-c/Screen%2BShot%2B2021-06-29%2Bat%2B9.00.11%2BAM.png" height="72" width="72"/>
    <thr:total>0</thr:total>
    <feedburner:origLink>http://ai.googleblog.com/2021/06/quickly-training-game-playing-agents.html</feedburner:origLink>
  </entry>
  <entry>
    <id>tag:blogger.com,1999:blog-8474926331452026626.post-5159347988023824086</id>
    <published>2021-06-28T10:15:00.000-07:00</published>
    <updated>2021-06-28T10:15:34.794-07:00</updated>
    <category scheme="http://www.blogger.com/atom/ns#" term="Computational Imaging"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="Computational Photography"/>
    <title type="text">Take All Your Pictures to the Cleaners, with Google Photos Noise and Blur Reduction</title>
    <content type="html">&lt;span class="byline-author"&gt;Posted by Mauricio Delbracio, Research Scientist and Sungjoon Choi, Software Engineer, Google Research&lt;/span&gt; &lt;p&gt;Despite recent leaps in imaging technology, &lt;a href="https://arxiv.org/abs/2102.09000"&gt;especially on mobile devices&lt;/a&gt;, image noise and limited sharpness remain two of the most important levers for improving the visual quality of a photograph. These are particularly relevant when taking pictures in poor light conditions, where cameras may compensate by increasing the &lt;a href="https://en.wikipedia.org/wiki/Film_speed#Digital_camera_ISO_speed_and_exposure_index"&gt;ISO&lt;/a&gt; or slowing the shutter speed, thereby exacerbating the presence of noise and, at times, increasing image blur. Noise can be associated with the particle nature of light (&lt;a href="https://en.wikipedia.org/wiki/Shot_noise"&gt;shot noise&lt;/a&gt;) or be introduced by electronic components during the readout process (&lt;a href="https://en.wikipedia.org/wiki/Image_noise#Read_noise"&gt;read noise&lt;/a&gt;). The captured noisy signal is then processed by the camera &lt;a href="https://en.wikipedia.org/wiki/Image_processor"&gt;image processor&lt;/a&gt; (ISP) and later may be further enhanced, amplified, or distorted by a photographic editing process. Image blur can be caused by a wide variety of phenomena, from inadvertent camera shake during capture, an incorrect setting of the camera’s focus (automatic or not), or due to the &lt;a href="https://en.wikipedia.org/wiki/Airy_disk"&gt;finite lens aperture&lt;/a&gt;, sensor resolution or the camera’s image processing.  &lt;/p&gt;&lt;p&gt;It is far easier to minimize the effects of noise and blur within a camera pipeline, where details of  the sensor, optical hardware and software blocks are understood. However, when presented with an image produced from an arbitrary (possibly unknown) camera, improving noise and sharpness becomes much more challenging due to the lack of detailed knowledge and access to the internal parameters of the camera. In most situations, these two problems are intrinsically related: noise reduction tends to eliminate fine structures along with unwanted details, while blur reduction seeks to boost structures and fine details. This interconnectedness increases the difficulty of developing image enhancement techniques that are computationally efficient to run on mobile devices. &lt;/p&gt;&lt;p&gt;Today, we present a new approach for &lt;em&gt;camera-agnostic&lt;/em&gt; estimation and elimination of noise and blur that can improve the quality of most images.  We developed a &lt;a href="https://ieeexplore.ieee.org/document/7532702"&gt;pull-push denoising&lt;/a&gt; algorithm that is paired with a deblurring method, called &lt;a href="https://arxiv.org/abs/2012.09322"&gt;polyblur&lt;/a&gt;. Both of these components are designed to maximize computational efficiency, so users can successfully enhance the quality of a multi-megapixel image in milliseconds on a mobile device. These noise and blur reduction strategies are critical components of the recent &lt;a href="https://blog.google/products/photos/new-helpful-editor/"&gt;Google Photos editor&lt;/a&gt; updates, which includes “&lt;em&gt;Denoise&lt;/em&gt;” and “&lt;em&gt;Sharpen&lt;/em&gt;” tools that enable users to enhance images that may have been captured under less than ideal conditions, or with older devices that may have had more noisy sensors or less sharp optics.  &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-bblz9X-bJro/YNOlX2r8KtI/AAAAAAAAHxI/6HhG4EJDPIokKzB6DUtQh5pvDZLVIUgqwCLcBGAsYHQ/s1520/image3.gif" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="1520" data-original-width="720" height="640" src="https://1.bp.blogspot.com/-bblz9X-bJro/YNOlX2r8KtI/AAAAAAAAHxI/6HhG4EJDPIokKzB6DUtQh5pvDZLVIUgqwCLcBGAsYHQ/w304-h640/image3.gif" width="304" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;A demonstration of the “Denoise” and “Sharpen” tools now available in the Google Photos editor.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;b&gt;How Noisy is An Image? &lt;/b&gt;&lt;br&gt;In order to accurately process a photographic image and successfully reduce the unwanted effects of noise and blur, it is vitally important to first characterize the types and levels of noise and blur found in the image. So, a camera-agnostic approach for noise reduction begins by formulating  a method to gauge the strength of noise at the pixel level from any given image, regardless of the device that created it. The noise level is modeled as a function of the brightness of the underlying pixel. That is, for each possible brightness level, the model estimates a corresponding noise level in a manner agnostic to either the actual source of the noise or the processing pipeline.  &lt;/p&gt;&lt;p&gt;To estimate this brightness-based noise level, we sample a number of small patches across the image and measure the noise level within each patch, after roughly removing any underlying structure in the image. This process is repeated at multiple scales, making it robust to artifacts that may arise from compression, image resizing, or other non-linear camera processing operations. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-6TDvzbLVI5o/YNOlghgNykI/AAAAAAAAHxM/qBnd7byMgRgPHY_xxM4PzwRzpmExTYRUACLcBGAsYHQ/s1999/image1.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="785" data-original-width="1999" height="252" src="https://1.bp.blogspot.com/-6TDvzbLVI5o/YNOlghgNykI/AAAAAAAAHxM/qBnd7byMgRgPHY_xxM4PzwRzpmExTYRUACLcBGAsYHQ/w640-h252/image1.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;The two segments on the left illustrate signal-dependent noise present in the input image (&lt;b&gt;center&lt;/b&gt;). The noise is more prominent in the bottom, darker crop and is unrelated to the underlying structure, but rather to the light level. Such image segments are sampled and processed to generate the spatially-varying noise map (&lt;b&gt;right&lt;/b&gt;) where red indicates more noise is present.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;b&gt;Reducing Noise Selectively with a Pull-Push Method&lt;/b&gt;&lt;br&gt;We take advantage of self-similarity of patches across the image to denoise with high fidelity. The general principle behind such so-called &lt;a href="https://en.wikipedia.org/wiki/Non-local_means"&gt;“non-local” denoising&lt;/a&gt; is that noisy pixels can be denoised by averaging pixels with similar local structure. However, these approaches typically incur high computational costs because they require a brute force search for pixels with similar local structure, making them impractical for on-device use. In our “pull-push” approach&lt;sup id="fnref1"&gt;&lt;a href="#fn1" rel="footnote"&gt;&lt;span style="font-size: x-small;"&gt;1&lt;/span&gt;&lt;/a&gt;&lt;/sup&gt;, the algorithmic complexity is decoupled from the size of filter footprints thanks to effective information propagation across spatial scales. &lt;/p&gt;&lt;p&gt;The first step in pull-push is to build an image pyramid (i.e., multiscale representation) in which each successive level is generated recursively by a “pull” filter (analogous to &lt;a href="https://en.wikipedia.org/wiki/Image_scaling"&gt;downsampling&lt;/a&gt;). This filter uses a per-pixel weighting scheme to &lt;em&gt;selectively&lt;/em&gt; combine existing noisy pixels together based on their patch similarities and estimated noise, thus reducing the noise at each successive, “coarser” level. Pixels at coarser levels (i.e., with lower resolution) pull and aggregate only compatible pixels from higher resolution, “finer” levels. In addition to this, each merged pixel in the coarser layers also includes an estimated reliability measure computed from the similarity weights used to generate it. Thus, merged pixels provide a simple per-pixel, per-level characterization of the image and its local statistics. By efficiently propagating this information through each level (i.e., each spatial scale), we are able to track a model of the neighborhood statistics for increasingly larger regions in a multiscale manner. &lt;/p&gt;&lt;p&gt;After the pull stage is evaluated to the coarsest level, the “push” stage fuses the results, starting from the coarsest level and generating finer levels iteratively. At a given scale, the push stage generates “filtered” pixels following a process similar to that of the pull stage, but going from coarse to finer levels. The pixels at each level are fused with those of coarser levels by doing a weighted average of same-level pixels along with coarser-level filtered pixels using the respective reliability weights. This enables us to reduce pixel noise while preserving local structure, because only average reliable information is included. This selective filtering and reliability (i.e. information) multiscale propagation is what makes push-pull different from existing frameworks. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-TD5fmjQFQyA/YNOlmsR7WPI/AAAAAAAAHxQ/ocx9gqSi4DM5Bhd5F0kYUyg_KoKtw8zfACLcBGAsYHQ/s1999/image6.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="1227" data-original-width="1999" height="392" src="https://1.bp.blogspot.com/-TD5fmjQFQyA/YNOlmsR7WPI/AAAAAAAAHxQ/ocx9gqSi4DM5Bhd5F0kYUyg_KoKtw8zfACLcBGAsYHQ/w640-h392/image6.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;This series of images shows how filtering progresses through the pull-push process. Coarser level pixels pull and aggregate only compatible pixels from finer levels, as opposed to the traditional multiscale approaches using a fixed (non-data dependent) kernel. Notice how the noise is reduced throughout the stages.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;The pull-push approach has a low computational cost, because the algorithm to selectively filter similar pixels over a very large neighborhood has a complexity that is only linear with the number of image pixels. In practice, the quality of this denoising approach is comparable to traditional non-local methods with much larger kernel footprints, but operates at a fraction of the computational cost. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-BnzI4F32xGk/YNOlstli8gI/AAAAAAAAHxU/o1nIYzP05RgWID9v4LwfMXpzXBq-gBW1wCLcBGAsYHQ/s1999/image5.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="1250" data-original-width="1999" height="400" src="https://1.bp.blogspot.com/-BnzI4F32xGk/YNOlstli8gI/AAAAAAAAHxU/o1nIYzP05RgWID9v4LwfMXpzXBq-gBW1wCLcBGAsYHQ/w640-h400/image5.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-XBJuHvtX1Gg/YNOlwM4KmuI/AAAAAAAAHxY/k6WT7xZZ-_Qm54uGRgNwETW4VpBM71GhQCLcBGAsYHQ/s1999/image2.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="1250" data-original-width="1999" height="400" src="https://1.bp.blogspot.com/-XBJuHvtX1Gg/YNOlwM4KmuI/AAAAAAAAHxY/k6WT7xZZ-_Qm54uGRgNwETW4VpBM71GhQCLcBGAsYHQ/w640-h400/image2.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Image enhanced using the pull-push denoising method.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;b&gt;How Blurry Is an Image?&lt;/b&gt;&lt;br&gt;An image with poor sharpness can be thought of as being a more pristine latent image that was operated on by a blur kernel. So, if one can identify the blur kernel, it can be used to reduce the effect. This is referred to as “deblurring”, i.e., the removal or reduction of an undesired blur effect induced by a particular kernel on a particular image. In contrast, “sharpening”  refers to applying a sharpening filter, built from scratch and without reference to any particular image or blur kernel. Typical sharpening filters are also, in general, local operations that do not take account of any other information from other parts of the image, whereas deblurring algorithms estimate the blur from the whole image. Unlike arbitrary sharpening, which can result in worse image quality when applied to an image that is already sharp, deblurring a sharp image with a blur kernel accurately estimated from the image itself will have very little effect. &lt;/p&gt;&lt;p&gt;We specifically target relatively mild blur, as this scenario is more technically tractable, more computationally efficient, and produces consistent results. We model the blur kernel as an anisotropic (elliptical) Gaussian kernel, specified by three parameters that control the strength, direction and aspect ratio of the blur.   &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-NdigDKPMQqg/YNOl3zapNNI/AAAAAAAAHxk/nJrFTISkd3cqPsPh1BSQjWEy3XlIpHxBgCLcBGAsYHQ/s973/image9.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="538" data-original-width="973" height="354" src="https://1.bp.blogspot.com/-NdigDKPMQqg/YNOl3zapNNI/AAAAAAAAHxk/nJrFTISkd3cqPsPh1BSQjWEy3XlIpHxBgCLcBGAsYHQ/w640-h354/image9.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Gaussian blur model and example blur kernels. Each row of the plot on the right represents possible combinations of &lt;em&gt;σ&lt;sub&gt;0&lt;/sub&gt;&lt;/em&gt;, &lt;em&gt;ρ&lt;/em&gt; and &lt;em&gt;θ&lt;/em&gt;. We show three different &lt;em&gt;σ&lt;sub&gt;0&lt;/sub&gt;&lt;/em&gt; values with three different &lt;em&gt;ρ&lt;/em&gt; values for each.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Computing and removing blur without noticeable delay for the user requires an algorithm that is much more computationally efficient than existing approaches, which typically cannot be executed on a mobile device. We rely on an intriguing empirical observation: the maximal value of the image gradient across all directions at any point in a sharp image follows a particular distribution. Finding the maximum gradient value is efficient, and can yield a reliable estimate of the strength of the blur in the given direction. With this information in hand, we can directly recover the parameters that characterize the blur.  &lt;/p&gt;&lt;p&gt;&lt;b&gt;Polyblur: Removing Blur by Re-blurring&lt;/b&gt;&lt;br&gt;To recover the sharp image given the estimated blur, we would (in theory) need to solve a numerically unstable inverse problem (i.e., deblurring). The inversion problem grows exponentially more unstable with the strength of the blur. As such, we target the case of mild blur removal. That is, we &lt;em&gt;assume&lt;/em&gt; that the image at hand is not so blurry as to be beyond practical repair. This enables a more practical approach — by carefully combining different re-applications of an operator we can approximate its inverse. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-uq2YWo1rPnw/YNOl_OctgLI/AAAAAAAAHxs/_6Q3GKI47EcpFHonzjCOLBKzcIGEeSEeQCLcBGAsYHQ/s1999/image8.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="1125" data-original-width="1999" height="360" src="https://1.bp.blogspot.com/-uq2YWo1rPnw/YNOl_OctgLI/AAAAAAAAHxs/_6Q3GKI47EcpFHonzjCOLBKzcIGEeSEeQCLcBGAsYHQ/w640-h360/image8.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-UMJT4p3BfZo/YNOmD9BxfCI/AAAAAAAAHx0/jUQ3YWbmniUYV-m9lMWvLbZ9KwO_MBsqgCLcBGAsYHQ/s1999/image7.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="888" data-original-width="1999" height="284" src="https://1.bp.blogspot.com/-UMJT4p3BfZo/YNOmD9BxfCI/AAAAAAAAHx0/jUQ3YWbmniUYV-m9lMWvLbZ9KwO_MBsqgCLcBGAsYHQ/w640-h284/image7.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Mild blur, as shown in these examples, can be effectively removed by combining multiple applications of the estimated blur.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;This means, rather counterintuitively, that we can &lt;em&gt;deblur&lt;/em&gt; an image by &lt;em&gt;re-blurring&lt;/em&gt; it several times with the estimated blur kernel. Each application of the (estimated) blur corresponds to a first order polynomial, and the repeated applications (adding or subtracting) correspond to higher order terms in a polynomial. A key aspect of this approach, which we call &lt;a href="https://arxiv.org/abs/2012.09322"&gt;polyblur&lt;/a&gt;, is that it is very fast, because it only requires a few applications of the blur itself. This allows it to operate on megapixel images in a fraction of a second on a typical mobile device. The degree of the polynomial and its coefficients are set to invert the blur without boosting noise and other unwanted artifacts.  &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-YCtQk62teYY/YNOmJzVVlnI/AAAAAAAAHx8/69Vo-SFE2GEZvzLdz8fb9EcMUxvt7hpeQCLcBGAsYHQ/s788/image4.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="266" data-original-width="788" height="216" src="https://1.bp.blogspot.com/-YCtQk62teYY/YNOmJzVVlnI/AAAAAAAAHx8/69Vo-SFE2GEZvzLdz8fb9EcMUxvt7hpeQCLcBGAsYHQ/w640-h216/image4.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;The deblurred image is generated by adding and subtracting multiple re-applications of the estimated blur (polyblur).&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;b&gt;Integration with Google Photos&lt;/b&gt;&lt;br&gt;The innovations described here have been integrated and made available to users in the Google Photos image editor in two new adjustment sliders called “Denoise” and “Sharpen”.  These features allow users to improve the quality of everyday images, from any capture device. The features often complement each other, allowing both denoising to reduce unwanted artifacts, and sharpening to bring clarity to the image subjects. Try using this pair of  tools in tandem in your images for best results. To learn more about the details of the work described here, check out our papers on &lt;a href="https://arxiv.org/abs/2012.09322"&gt;polyblur&lt;/a&gt; and &lt;a href="https://ieeexplore.ieee.org/document/7532702"&gt;pull-push denoising&lt;/a&gt;. To see some examples of the effect of our denoising and sharpening up close, have a look at the images in this &lt;a href="https://photos.app.goo.gl/eV2HhMAXNGpS62ZYA"&gt;album&lt;/a&gt;.  &lt;/p&gt;&lt;p&gt;&lt;b&gt;Acknowledgements&lt;/b&gt;&lt;br&gt;&lt;em&gt;The authors gratefully acknowledge the contributions of Ignacio Garcia-Dorado, Ryan Campbell, Damien Kelly, Peyman Milanfar, and John Isidoro. We are also thankful for support and feedback from Navin Sarma, Zachary Senzer, Brandon Ruffin, and Michael Milne.&lt;/em&gt;&lt;/p&gt; &lt;!--Footnotes--&gt;&lt;hr width="80%" /&gt;&lt;p&gt;  &lt;span class="Apple-style-span" style="font-size: x-small;"&gt;&lt;sup&gt;&lt;a name="fn1"&gt;&lt;b&gt;1&lt;/b&gt;&lt;/a&gt;&lt;/sup&gt; The original &lt;a href="https://cseweb.ucsd.edu/~ravir/6160/papers/p43-gortler.pdf"&gt;pull-push algorithm&lt;/a&gt; was developed as an efficient scattered data interpolation method to estimate and fill in the missing pixels in an image where only a subset of the pixels are specified. Here, we extend its methodology and present a data-dependent multiscale algorithm for denoising images efficiently.&amp;nbsp;&lt;a href="#fnref1" rev="footnote"&gt;&lt;sup&gt;↩&lt;/sup&gt;&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/blogspot/gJZg?a=YzrGZK_H1dE:kz2iSuHd3gs:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/blogspot/gJZg/~4/YzrGZK_H1dE" height="1" width="1" alt=""/&gt;</content>
    <link rel="replies" type="application/atom+xml" href="http://ai.googleblog.com/feeds/5159347988023824086/comments/default" title="Post Comments"/>
    <link rel="replies" type="text/html" href="http://ai.googleblog.com/2021/06/take-all-your-pictures-to-cleaners-with.html#comment-form" title="0 Comments"/>
    <link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/5159347988023824086"/>
    <link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/5159347988023824086"/>
    <link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/blogspot/gJZg/~3/YzrGZK_H1dE/take-all-your-pictures-to-cleaners-with.html" title="Take All Your Pictures to the Cleaners, with Google Photos Noise and Blur Reduction"/>
    <author>
      <name>Google AI</name>
      <uri>http://www.blogger.com/profile/12098626514775266161</uri>
      <email>noreply@blogger.com</email>
      <gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="16" height="16" src="https://img1.blogblog.com/img/b16-rounded.gif"/>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://1.bp.blogspot.com/-bblz9X-bJro/YNOlX2r8KtI/AAAAAAAAHxI/6HhG4EJDPIokKzB6DUtQh5pvDZLVIUgqwCLcBGAsYHQ/s72-w304-h640-c/image3.gif" height="72" width="72"/>
    <thr:total>0</thr:total>
    <feedburner:origLink>http://ai.googleblog.com/2021/06/take-all-your-pictures-to-cleaners-with.html</feedburner:origLink>
  </entry>
  <entry>
    <id>tag:blogger.com,1999:blog-8474926331452026626.post-6206828479795872652</id>
    <published>2021-06-25T09:56:00.000-07:00</published>
    <updated>2021-06-25T09:56:59.720-07:00</updated>
    <category scheme="http://www.blogger.com/atom/ns#" term="Quantum AI"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="Quantum Computing"/>
    <title type="text">Achieving Precision in Quantum Material Simulations</title>
    <content type="html">&lt;span class="byline-author"&gt;Posted by Charles Neill and Zhang Jiang, Senior Research Scientists, Google Quantum AI&lt;/span&gt; &lt;p&gt;In fall of 2019, &lt;a href="https://ai.googleblog.com/2019/10/quantum-supremacy-using-programmable.html"&gt;we demonstrated&lt;/a&gt; that the Sycamore quantum processor could &lt;a href="https://www.nature.com/articles/s41586-019-1666-5"&gt;outperform the most powerful classical computers&lt;/a&gt; when applied to a tailor-made problem. The next challenge is to extend this result to solve practical problems&lt;strong&gt; &lt;/strong&gt;in materials science, chemistry and physics. But going beyond the capabilities of classical computers for these problems is challenging and will require new insights to achieve state-of-the-art accuracy. Generally, the difficulty in performing quantum simulations of such physical problems is rooted in the wave nature of quantum particles, where &lt;a href="https://ai.googleblog.com/2019/10/improving-quantum-computation-with.html"&gt;deviations in the initial setup, interference from the environment, or small errors in the calculations&lt;/a&gt; can lead to large deviations in the computational result. &lt;/p&gt;&lt;p&gt;In two upcoming publications, we outline a blueprint for achieving record levels of precision for the task of simulating quantum materials. In the first work, we consider one-dimensional systems, like thin wires, and demonstrate how to accurately compute electronic properties, such as current and conductance. In the second work, we show how to map the &lt;a href="https://arxiv.org/abs/1505.02290"&gt;Fermi-Hubbard model&lt;/a&gt;, which describes interacting electrons, to a quantum processor in order to simulate important physical properties. These works take a significant step towards realizing our long-term goal of simulating more complex systems with practical applications, like batteries and pharmaceuticals. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-nc5W1EQwsFg/YNOtzBn1ccI/AAAAAAAAHyE/GaAwaetZpeMExLp3wPkEFDNfgvS-HgVowCLcBGAsYHQ/s1999/image3.jpg" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="1331" data-original-width="1999" height="426" src="https://1.bp.blogspot.com/-nc5W1EQwsFg/YNOtzBn1ccI/AAAAAAAAHyE/GaAwaetZpeMExLp3wPkEFDNfgvS-HgVowCLcBGAsYHQ/w640-h426/image3.jpg" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;A bottom view of one of the quantum &lt;a href="https://en.wikipedia.org/wiki/Dilution_refrigerator"&gt;dilution refrigerators&lt;/a&gt; during maintenance. During the operation, the microwave wires that are floating in this image are connected to the quantum processor, e.g., the Sycamore chip, bringing the temperature of the lowest stage to a few tens of milli-degrees above absolute zero temperature.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;b&gt;Computing Electronic Properties of Quantum Materials&lt;/b&gt;&lt;br /&gt;In “&lt;a href="https://arxiv.org/abs/2012.00921"&gt;Accurately computing electronic properties of a quantum ring&lt;/a&gt;”, to be published in &lt;em&gt;&lt;a href="https://www.nature.com/"&gt;Nature&lt;/a&gt;&lt;/em&gt;, we show how to reconstruct key electronic properties of quantum materials. The focus of this work is on one-dimensional conductors, which we simulate by forming a loop out of 18 qubits on the Sycamore processor in order to mimic a very narrow wire. We illustrate the underlying physics through a series of simple text-book experiments, starting with a computation of the “&lt;a href="https://en.wikipedia.org/wiki/Electronic_band_structure"&gt;band-structure&lt;/a&gt;” of this wire, which describes the relationship between the energy and momentum of electrons in the metal. Understanding such structure is a key step in computing electronic properties such as current and conductance. Despite being an 18-qubit algorithm consisting of over 1,400 logical operations, a significant computational task for near-term devices, we are able to achieve a total error as low as 1%.  &lt;/p&gt;&lt;p&gt;The key insight enabling this level of accuracy stems from robust properties of the &lt;a href="https://en.wikipedia.org/wiki/Fourier_transform"&gt;Fourier transform&lt;/a&gt;. The quantum signal that we measure oscillates in time with a small number of frequencies. Taking a Fourier transform of this signal reveals peaks at the oscillation frequencies (in this case, the energy of electrons in the wire). While experimental imperfections affect the height of the observed peaks (corresponding to the strength of the oscillation), the center frequencies are robust to these errors. On the other hand, the center frequencies are especially sensitive to the physical properties of the wire that we hope to study (e.g., revealing small disorders in the local electric field felt by the electrons). The essence of our work is that studying quantum signals in the Fourier domain enables robust protection against experimental errors while providing a sensitive probe of the underlying quantum system. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-OX4fa3f84xE/YNOuP1YmIaI/AAAAAAAAHyY/fwA6DH192zQ1RphxKayjHwSNKSjEQr9NgCLcBGAsYHQ/s1999/image1%2B%25288%2529.jpg" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="663" data-original-width="1999" height="212" src="https://1.bp.blogspot.com/-OX4fa3f84xE/YNOuP1YmIaI/AAAAAAAAHyY/fwA6DH192zQ1RphxKayjHwSNKSjEQr9NgCLcBGAsYHQ/w640-h212/image1%2B%25288%2529.jpg" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;&lt;b&gt;(Left)&lt;/b&gt; Schematic of the 54-qubit quantum processor, Sycamore. Qubits are shown as gray crosses and tunable couplers as blue squares. Eighteen of the qubits are isolated to form a ring. &lt;b&gt;(Middle)&lt;/b&gt; Fourier transform of the measured quantum signal. Peaks in the Fourier spectrum correspond to the energy of electrons in the ring. Each peak can be associated with a traveling wave that has fixed momentum. &lt;b&gt;(Right)&lt;/b&gt; The center frequency of each peak (corresponding to the energy of electrons in the wire) is plotted versus the peak index (corresponding to the momentum). The measured relationship between energy and momentum is referred to as the ‘band structure’ of the quantum wire and provides valuable information about electronic properties of the material, such as current and conductance.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;b&gt;Quantum Simulation of the Fermi-Hubbard Model&lt;/b&gt;&lt;br /&gt;In “&lt;a href="https://arxiv.org/abs/2010.07965"&gt;Observation of separated dynamics of charge and spin in the Fermi-Hubbard model&lt;/a&gt;”, we focus on the dynamics of interacting electrons. Interactions between particles give rise to novel phenomena such as &lt;a href="https://en.wikipedia.org/wiki/High-temperature_superconductivity"&gt;high temperature superconductivity&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/Spin%E2%80%93charge_separation"&gt;spin-charge separation&lt;/a&gt;. The simplest model that captures this behavior is known as the &lt;a href="https://arxiv.org/abs/1505.02290"&gt;Fermi-Hubbard model&lt;/a&gt;. In materials such as metals, the atomic nuclei form a crystalline lattice and electrons hop from lattice site to lattice site carrying electrical current. In order to accurately model these systems, it is necessary to include the repulsion that electrons feel when getting close to one another.  The Fermi-Hubbard model captures this physics with two simple parameters that describe the hopping rate (&lt;em&gt;J&lt;/em&gt;) and the repulsion strength (&lt;em&gt;U&lt;/em&gt;).  &lt;/p&gt;&lt;p&gt;We realize the dynamics of this model by mapping the two physical parameters to logical operations on the qubits of the processor. Using these operations, we simulate a state of the electrons where both the electron charge and spin densities are peaked near the center of the qubit array. As the system evolves, the charge and spin densities spread at different rates due to the &lt;a href="https://en.wikipedia.org/wiki/Strongly_correlated_material"&gt;strong correlations&lt;/a&gt; between electrons. Our results provide an intuitive picture of interacting electrons and serve as a benchmark for simulating quantum materials with superconducting qubits.  &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-D_PQaMZF3ys/YNOuVZf5ONI/AAAAAAAAHyc/-OBDaKvQNzQqI0LeAHZs4nO_ovrRJDZyQCLcBGAsYHQ/s1999/image2%2B%25285%2529.jpg" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="873" data-original-width="1999" height="280" src="https://1.bp.blogspot.com/-D_PQaMZF3ys/YNOuVZf5ONI/AAAAAAAAHyc/-OBDaKvQNzQqI0LeAHZs4nO_ovrRJDZyQCLcBGAsYHQ/w640-h280/image2%2B%25285%2529.jpg" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;&lt;b&gt;(Left top)&lt;/b&gt; Illustration of the one-dimensional Fermi-Hubbard model in a periodic potential. Electrons are shown in blue, with their spin indicated by the connected arrow. &lt;em&gt;J&lt;/em&gt;, the distance between troughs in the electric potential field, reflects the “hopping” rate, i.e., the rate at which electrons transition from one trough in the potential to another, and &lt;em&gt;U&lt;/em&gt;, the amplitude, represents the strength of repulsion between electrons. &lt;b&gt;(Left bottom)&lt;/b&gt; The simulation of the model on a qubit ladder, where each qubit (square) represents a fermionic state with spin-up or spin-down (arrows). &lt;b&gt;(Right)&lt;/b&gt; Time evolution of the model reveals separated spreading rates of charge and spin. Points and solid lines represent experimental and numerical exact results, respectively. At &lt;em&gt;t&lt;/em&gt; = 0, the charge and spin densities are peaked at the middle sites. At later times, the charge density spreads and reaches the boundaries faster than the spin density.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;b&gt;Conclusion&lt;/b&gt;&lt;br /&gt;Quantum processors hold the promise to solve computationally hard tasks beyond the capability of classical approaches. However, in order for these engineered platforms to be considered as serious contenders, they must offer computational accuracy beyond the current state-of-the-art classical methods. In our first experiment, we demonstrate an unprecedented level of accuracy in simulating simple materials, and in our second experiment, we show how to embed realistic models of interacting electrons into a quantum processor. It is our hope that these experimental results help progress the goal of moving beyond the classical computing horizon.  &lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/blogspot/gJZg?a=lFnNMAnC5TY:eI9r5xO35h0:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/blogspot/gJZg/~4/lFnNMAnC5TY" height="1" width="1" alt=""/&gt;</content>
    <link rel="replies" type="application/atom+xml" href="http://ai.googleblog.com/feeds/6206828479795872652/comments/default" title="Post Comments"/>
    <link rel="replies" type="text/html" href="http://ai.googleblog.com/2021/06/achieving-precision-in-quantum-material.html#comment-form" title="0 Comments"/>
    <link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/6206828479795872652"/>
    <link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/6206828479795872652"/>
    <link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/blogspot/gJZg/~3/lFnNMAnC5TY/achieving-precision-in-quantum-material.html" title="Achieving Precision in Quantum Material Simulations"/>
    <author>
      <name>Google AI</name>
      <uri>http://www.blogger.com/profile/12098626514775266161</uri>
      <email>noreply@blogger.com</email>
      <gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="16" height="16" src="https://img1.blogblog.com/img/b16-rounded.gif"/>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://1.bp.blogspot.com/-nc5W1EQwsFg/YNOtzBn1ccI/AAAAAAAAHyE/GaAwaetZpeMExLp3wPkEFDNfgvS-HgVowCLcBGAsYHQ/s72-w640-h426-c/image3.jpg" height="72" width="72"/>
    <thr:total>0</thr:total>
    <feedburner:origLink>http://ai.googleblog.com/2021/06/achieving-precision-in-quantum-material.html</feedburner:origLink>
  </entry>
  <entry>
    <id>tag:blogger.com,1999:blog-8474926331452026626.post-819110334029306444</id>
    <published>2021-06-24T12:55:00.015-07:00</published>
    <updated>2021-06-24T15:51:42.509-07:00</updated>
    <category scheme="http://www.blogger.com/atom/ns#" term="datasets"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="Google Translate"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="Machine Translation"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="ML Fairness"/>
    <title type="text">A Dataset for Studying Gender Bias in Translation</title>
    <content type="html">&lt;span class="byline-author"&gt;Posted by Romina Stella, Product Manager, Google Translate&lt;/span&gt; &lt;p&gt;Advances on &lt;a href="https://ai.googleblog.com/2020/06/recent-advances-in-google-translate.html"&gt;neural machine translation&lt;/a&gt; (NMT) have enabled more natural and fluid translations, but they still can reflect the societal biases and stereotypes of the data on which they're trained. As such, it is an ongoing goal at Google to develop &lt;a href="https://ai.googleblog.com/2018/12/providing-gender-specific-translations.html"&gt;innovative techniques&lt;/a&gt; to &lt;a href="https://ai.googleblog.com/2020/04/a-scalable-approach-to-reducing-gender.html"&gt;reduce gender bias&lt;/a&gt; in machine translation, in alignment with our &lt;a href="https://ai.google/principles/"&gt;AI Principles&lt;/a&gt;.  &lt;/p&gt;&lt;p&gt;One research area has been using context from surrounding sentences or passages to improve gender accuracy. This is a challenge because traditional NMT methods translate sentences individually, but gendered information is not always explicitly stated in each individual sentence. For example, in the following passage in Spanish (a language where subjects aren’t always &lt;a href="https://en.wikipedia.org/wiki/Null-subject_language"&gt;explicitly mentioned&lt;/a&gt;), the first sentence refers explicitly to Marie Curie as the subject, but the second one doesn't explicitly mention the subject. In isolation, this second sentence could refer to a person of any gender. When translating to English, however, a pronoun needs to be picked, and the information needed for an accurate translation is in the first sentence. &lt;/p&gt; &lt;table&gt;  &lt;tbody&gt;    &lt;tr&gt;      &lt;td&gt;&lt;em&gt;&lt;b&gt;Spanish Text&lt;/b&gt;&lt;/em&gt;&lt;/td&gt;      &lt;td&gt;&lt;em&gt;&lt;b&gt;Translation to English&lt;/b&gt;&lt;/em&gt;&lt;/td&gt;    &lt;/tr&gt;&lt;tr&gt;   &lt;td&gt;Marie Curie nació en Varsovia. Fue la primera persona en recibir dos premios Nobel en distintas especialidades.    &lt;/td&gt;      &lt;td&gt;Marie Curie was born in Warsaw. &lt;b&gt;She&lt;/b&gt; was the first person to receive two Nobel Prizes in different specialties.    &lt;/td&gt;  &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;p&gt;Advancing translation techniques beyond single sentences requires new metrics for measuring progress and new datasets with the most common context-related errors. Adding to this challenge is the fact that translation errors related to gender (such as picking the correct pronoun or having gender agreement) are particularly sensitive, because they may directly refer to people and how they self identify. &lt;/p&gt;&lt;p&gt;To help facilitate progress against the common challenges on contextual translation (e.g., pronoun drop, gender agreement and accurate possessives), we are releasing the &lt;a href="https://storage.googleapis.com/gresearch/translate-gender-challenge-sets/Readme.html"&gt;Translated Wikipedia Biographies&lt;/a&gt;&lt;strong&gt; &lt;/strong&gt;dataset, which can be used to evaluate the gender bias of translation models.&lt;strong&gt; &lt;/strong&gt;Our intent with this release is to support long-term improvements on ML systems focused on pronouns and gender in translation by providing a benchmark in which translations’ accuracy can be measured pre- and post-model changes.&lt;strong&gt;  &lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;b&gt;A Source of Common Translation Errors &lt;/b&gt;&lt;br /&gt;Because they are well-written, geographically diverse, contain multiple sentences, and refer to subjects in the third person (and so contain plenty of pronouns), &lt;a href="https://en.wikipedia.org/wiki/Wikipedia:Biographies_of_living_persons"&gt;Wikipedia biographies&lt;/a&gt; offer a high potential for common translation errors associated with gender. These often occur when articles refer to a person explicitly in early sentences of a paragraph, but there is no explicit mention of the person in later sentences. Some examples:  &lt;/p&gt;  &lt;table&gt;  &lt;tbody&gt;    &lt;tr&gt;&lt;td&gt;&lt;b&gt;&lt;em&gt;Translation Error&lt;/em&gt;&lt;/b&gt;&lt;/td&gt;      &lt;td&gt;&lt;b&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b&gt;&lt;/td&gt;      &lt;td&gt;&lt;b&gt;&lt;em&gt;Text&lt;/em&gt;&lt;/b&gt;&lt;/td&gt;      &lt;td&gt;&lt;b&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b&gt;&lt;/td&gt;      &lt;td&gt;&lt;b&gt;&lt;em&gt;Translation&lt;/em&gt;&lt;/b&gt;&lt;/td&gt;    &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;b&gt;&lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Pro-drop_language"&gt;Pro-drop&lt;/a&gt; in Spanish →  English&lt;/em&gt;&lt;/b&gt;&lt;/td&gt;&lt;td&gt;&lt;b&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b&gt;&lt;/td&gt;   &lt;td&gt;Marie Curie nació en Varsovia. Recibió el Premio Nobel en 1903 y en 1911.    &lt;/td&gt;&lt;td&gt;&lt;b&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b&gt;&lt;/td&gt;   &lt;td&gt;&lt;b&gt;Marie Curie&lt;/b&gt; was born in Warsaw. &lt;b&gt;He&lt;/b&gt; received the Nobel Prize in 1903 and in 1911.&lt;br /&gt;   &lt;/td&gt;  &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;br /&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;        &lt;tr&gt;&lt;td&gt;&lt;b&gt;&lt;em&gt;Neutral &lt;a href="https://en.wikipedia.org/wiki/Possessive_determiner#Other_languages"&gt;possessives&lt;/a&gt; in Spanish →  English&lt;/em&gt;&lt;/b&gt;&lt;/td&gt;&lt;td&gt;&lt;b&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b&gt;&lt;/td&gt;   &lt;td&gt;Marie Curie nació en Varsovia. Su carrera profesional fue desarrollada en Francia.    &lt;/td&gt;&lt;td&gt;&lt;b&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b&gt;&lt;/td&gt;   &lt;td&gt;&lt;b&gt;Marie Curie&lt;/b&gt;  was born in Warsaw. &lt;b&gt;His&lt;/b&gt; professional career was developed in France.&lt;br /&gt;   &lt;/td&gt;  &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;br /&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;     &lt;tr&gt;&lt;td&gt;&lt;b&gt;&lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Grammatical_gender"&gt;Gender agreement&lt;/a&gt; in English  → German&lt;/em&gt;&lt;/b&gt;&lt;/td&gt;&lt;td&gt;&lt;b&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b&gt;&lt;/td&gt;         &lt;td&gt;Marie Curie was born in Warsaw. The distinguished scientist received the Nobel Prize in 1903 and in 1911.    &lt;/td&gt;&lt;td&gt;&lt;b&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b&gt;&lt;/td&gt;   &lt;td&gt;&lt;b&gt;Marie Curie&lt;/b&gt; wurde in Varsovia geboren. &lt;b&gt;Der&lt;/b&gt; angesehene &lt;b&gt;Wissenschaftler&lt;/b&gt; erhielt 1903 und 1911 den Nobelpreis.&lt;br /&gt;   &lt;/td&gt;  &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;br /&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;       &lt;tr&gt;&lt;td&gt;&lt;b&gt;&lt;em&gt;&lt;a href="https://en.wikipedia.org/wiki/Grammatical_gender"&gt;Gender agreement&lt;/a&gt; in English  →  Spanish  &lt;/em&gt;&lt;/b&gt;&lt;/td&gt;&lt;td&gt;&lt;b&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b&gt;&lt;/td&gt;       &lt;td&gt;Marie Curie was born in Warsaw. The distinguished scientist received the Nobel Prize in 1903 and in 1911.    &lt;/td&gt;&lt;td&gt;&lt;b&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/b&gt;&lt;/td&gt;         &lt;td&gt;&lt;b&gt;Marie Curie&lt;/b&gt; nació en Varsovia. &lt;b&gt;El distinguido científico&lt;/b&gt; recibió el Premio Nobel en 1903 y en 1911.    &lt;/td&gt;  &lt;/tr&gt;&lt;tr&gt;&lt;td&gt;&lt;br /&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;p&gt;&lt;b&gt;Building the Dataset&lt;/b&gt;&lt;br /&gt;The &lt;a href="https://storage.googleapis.com/gresearch/translate-gender-challenge-sets/Readme.html"&gt;Translated Wikipedia Biographies&lt;/a&gt; dataset has been designed to analyze common gender errors in machine translation, such as those illustrated above. Each instance of the dataset represents a person (identified in the biographies as feminine or masculine), a rock band or a sports team (considered genderless).  Each instance is represented by a long text translation of 8 to 15 connected sentences referring to that central subject (the person, rock band, or sports team).  Articles are written in native English and have been professionally translated to Spanish and German. For Spanish, translations were optimized for pronoun-drop, so the same set could be used to analyze pro-drop (Spanish → English) and gender agreement (English → Spanish).  &lt;/p&gt;&lt;p&gt;The dataset was built by selecting a group of instances that has equal representation across geographies and genders. To do this, we extracted biographies from Wikipedia according to occupation, profession, job and/or activity. To ensure an unbiased selection of occupations, we chose nine occupations that represented a range of stereotypical gender associations (either feminine, masculine, or neither) based on Wikipedia statistics. Then, to mitigate any geography-based bias, we divided all these instances based on geographical diversity. For each occupation category, we looked to have one candidate per region (using regions from &lt;a href="https://www.census.gov/prod/2004pubs/wp-02.pdf"&gt;census.gov&lt;/a&gt; as a proxy of geographical diversity). When an instance was associated with a region, we checked that the selected person had a relevant relationship with a country that belongs to a designated region (nationality, place of birth, lived for a big portion of their life, etc.). By using this criteria, the dataset contains entries about individuals from more than 90 countries and all regions of the world.  &lt;/p&gt;&lt;p&gt;Although gender is non-binary, we focused on having equal representation of “feminine” and “masculine” entities. It's worth mentioning that because the entities are represented as such on Wikipedia, the set doesn't include individuals that identify as non-binary,  as, unfortunately, there are not enough instances currently represented in Wikipedia to accurately reflect the non-binary community. To label each instance as "feminine" or "masculine" we relied on the biographical information from Wikipedia, which contained gender-specific references to the person (she, he, woman, son, father, etc.).&lt;/p&gt;&lt;p&gt;After applying all these filters, we randomly selected an instance for each occupation-region-gender triplet. For each occupation, there are two biographies (one masculine and one feminine), for each of the seven geographic regions. &lt;/p&gt;&lt;p&gt;Finally, we added 12 instances with no gender. We picked rock bands and sports teams because they are usually referred to by non-gendered third person pronouns (such as “it” or singular “they”). The purpose of including these instances is to study over triggering, i.e., when models learn that they are rewarded for producing gender-specific pronouns, leading them to produce these pronouns in cases where they shouldn't. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Results and Applications&lt;/b&gt;&lt;br /&gt;This dataset enables a new method of evaluation for gender bias reduction in machine translations (introduced in a &lt;a href="https://ai.googleblog.com/2020/04/a-scalable-approach-to-reducing-gender.html"&gt;previous post&lt;/a&gt;). Because each instance refers to a subject with a known gender, we can compute the accuracy of the gender-specific translations that refer to this subject. This computation is easier when translating into English (cases of languages with pro-drop or neutral pronouns) since computation is mainly based on  gender-specific pronouns in English. In these cases, the gender datasets have resulted in a 67% reduction in errors on context-aware models vs. previous models. As mentioned before, the neutral entities have allowed us to discover cases of over triggering like the usage of feminine or masculine pronouns to refer to genderless entities. This new dataset also enables new research directions into the performance of different models across types of occupations or geographic regions. &lt;/p&gt;&lt;p&gt;As an example, the dataset allowed us to discover the following improvements in an excerpt of the translated biography of Marie Curie from Spanish. &lt;/p&gt;   &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-6gpgZOiYors/YNS7uoth6oI/AAAAAAAAHyo/AgABelsRxJwEJnu1GYeYc3sAcKq0ewD6wCLcBGAsYHQ/s1999/image1.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="1273" data-original-width="1999" height="408" src="https://1.bp.blogspot.com/-6gpgZOiYors/YNS7uoth6oI/AAAAAAAAHyo/AgABelsRxJwEJnu1GYeYc3sAcKq0ewD6wCLcBGAsYHQ/w640-h408/image1.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Translation result with the previous NMT model.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-gp3P_kKYfrg/YNS73FUz2fI/AAAAAAAAHys/NA90YKWiyig5rwVSlJzAttquDte1QTxzwCLcBGAsYHQ/s1999/image2.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="1273" data-original-width="1999" height="408" src="https://1.bp.blogspot.com/-gp3P_kKYfrg/YNS73FUz2fI/AAAAAAAAHys/NA90YKWiyig5rwVSlJzAttquDte1QTxzwCLcBGAsYHQ/w640-h408/image2.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Translation result with the new contextual model.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;p&gt;&lt;b&gt;Conclusion&lt;/b&gt;&lt;br /&gt;This &lt;a href="https://storage.googleapis.com/gresearch/translate-gender-challenge-sets/Readme.html"&gt;Translated Wikipedia Biographies&lt;/a&gt; dataset is the result of our own studies and work on identifying biases associated with gender and machine translation. This set focuses on a specific problem related to gender bias and doesn't aim to cover the whole problem. It's worth mentioning that by releasing this dataset, we don't aim to be prescriptive in determining what's the optimal approach to address gender bias. This contribution aims to foster progress on this challenge across the global research community.   &lt;/p&gt;&lt;p&gt;&lt;b&gt;Acknowledgements&lt;/b&gt;&lt;br /&gt;&lt;em&gt;The datasets were built with help from Anja Austermann, Melvin Johnson, Michelle Linch, Mengmeng Niu, Mahima Pushkarna, Apu Shah, Romina Stella, and Kellie Webster.&lt;/em&gt;&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/blogspot/gJZg?a=SqLDbUeZeYM:UOurCmUGih4:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/blogspot/gJZg/~4/SqLDbUeZeYM" height="1" width="1" alt=""/&gt;</content>
    <link rel="replies" type="application/atom+xml" href="http://ai.googleblog.com/feeds/819110334029306444/comments/default" title="Post Comments"/>
    <link rel="replies" type="text/html" href="http://ai.googleblog.com/2021/06/a-dataset-for-studying-gender-bias-in.html#comment-form" title="0 Comments"/>
    <link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/819110334029306444"/>
    <link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/819110334029306444"/>
    <link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/blogspot/gJZg/~3/SqLDbUeZeYM/a-dataset-for-studying-gender-bias-in.html" title="A Dataset for Studying Gender Bias in Translation"/>
    <author>
      <name>Google AI</name>
      <uri>http://www.blogger.com/profile/12098626514775266161</uri>
      <email>noreply@blogger.com</email>
      <gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="16" height="16" src="https://img1.blogblog.com/img/b16-rounded.gif"/>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://1.bp.blogspot.com/-6gpgZOiYors/YNS7uoth6oI/AAAAAAAAHyo/AgABelsRxJwEJnu1GYeYc3sAcKq0ewD6wCLcBGAsYHQ/s72-w640-h408-c/image1.png" height="72" width="72"/>
    <thr:total>0</thr:total>
    <feedburner:origLink>http://ai.googleblog.com/2021/06/a-dataset-for-studying-gender-bias-in.html</feedburner:origLink>
  </entry>
  <entry>
    <id>tag:blogger.com,1999:blog-8474926331452026626.post-613959647323225034</id>
    <published>2021-06-23T12:58:00.002-07:00</published>
    <updated>2021-06-24T07:19:21.458-07:00</updated>
    <category scheme="http://www.blogger.com/atom/ns#" term="AI"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="Deep Learning"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="Health"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="Machine Learning"/>
    <title type="text">Improving Genomic Discovery with Machine Learning</title>
    <content type="html">&lt;span class="byline-author"&gt;Posted by Andrew Carroll, Product Manager and Cory McLean, Software Engineer, Google Health&lt;/span&gt; &lt;p&gt;Each person’s genome, which collectively encodes the biochemical machinery they are born with, is composed of over 3 billion letters of DNA. However, only a small subset of the genome (~4-5 million &lt;a href="https://en.wikipedia.org/wiki/Locus_(genetics)"&gt;positions&lt;/a&gt;) varies between two people. Nonetheless, each person’s unique genome interacts with the environment they experience to determine the &lt;a href="https://www.healthaffairs.org/doi/10.1377/hlthaff.21.2.78"&gt;majority of their health outcomes&lt;/a&gt;. A key method of understanding the relationship between genetic variants and traits is a &lt;a href="https://www.genome.gov/genetics-glossary/Genome-Wide-Association-Studies"&gt;genome-wide association study&lt;/a&gt; (GWAS), in which each genetic variant present in a cohort is individually examined for correlation with the trait of interest. GWAS results can be used to identify and prioritize potential therapeutic targets by identifying genes that are strongly associated with a disease of interest, and can also be used to build a &lt;a href="https://www.genome.gov/Health/Genomics-and-Medicine/Polygenic-risk-scores"&gt;polygenic risk score&lt;/a&gt; (PRS) to predict disease predisposition based on the combined influence of variants present in an individual. However, while accurate measurement of traits in an individual (called phenotyping) is essential to GWAS,  it often requires painstaking expert curation and/or subjective judgment calls. &lt;/p&gt;&lt;p&gt;In “&lt;a href="https://www.cell.com/ajhg/fulltext/S0002-9297(21)00188-9"&gt;Large-scale machine learning-based phenotyping significantly improves genomic discovery for optic nerve head morphology&lt;/a&gt;”, we demonstrate how using machine learning (ML) models to classify medical imaging data can be used to improve GWAS. We describe how models can be trained for &lt;a href="https://en.wikipedia.org/wiki/Phenotype"&gt;phenotypes&lt;/a&gt; to generate trait predictions and how these predictions are used to identify novel genetic associations. We then show that the novel associations discovered improve PRS accuracy and, using glaucoma as an example, that the improvements for anatomical eye traits relate to human disease. We have released the model training code and detailed documentation for its use on our &lt;a href="https://github.com/Google-Health/genomics-research/tree/main/ml-based-vcdr"&gt;Genomics Research GitHub repository&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Identifying genetic variants associated with eye anatomical traits&lt;/b&gt;&lt;br /&gt;Previous work has demonstrated that ML models can identify &lt;a href="https://ai.googleblog.com/2016/11/deep-learning-for-detection-of-diabetic.html"&gt;eye diseases&lt;/a&gt;, &lt;a href="https://blog.google/technology/health/ai-dermatology-preview-io-2021"&gt;skin diseases&lt;/a&gt;, and &lt;a href="https://blog.google/technology/health/improving-breast-cancer-screening/"&gt;abnormal mammogram results&lt;/a&gt; with accuracy approaching or exceeding state-of-the-art methods by domain experts. Because identifying disease is a subset of phenotyping, we reasoned that ML models could be broadly used to improve the speed and quality of phenotyping for GWAS. &lt;/p&gt;&lt;p&gt;To test this, we chose a model that uses a &lt;a href="https://en.wikipedia.org/wiki/Fundus_photography"&gt;fundus image&lt;/a&gt; of the eye to accurately &lt;a href="https://www.aaojournal.org/article/S0161-6420(19)31875-5/fulltext"&gt;predict whether a patient should be referred for assessment for glaucoma&lt;/a&gt;. This model uses the fundus images to predict the diameters of the &lt;a href="https://en.wikipedia.org/wiki/Optic_disc"&gt;optic disc&lt;/a&gt; (the region where the optic nerve connects to the retina) and the &lt;a href="https://en.wikipedia.org/wiki/Optic_cup_(anatomical)"&gt;optic cup&lt;/a&gt; (a whitish region in the center of the optic disc). The ratio of the diameters of these two anatomical features (called the vertical cup-to-disc ratio, or VCDR) correlates strongly with glaucoma risk.&lt;/p&gt;  &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-5lyl-oGlCtU/YNNk8-JpCEI/AAAAAAAAHww/Fgyb8zfkRncpkaRvpEGzh2W7Wwc1SGh4gCLcBGAsYHQ/s899/genomics%2Bfundus%2Bimage%2BVCDR.jpg" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="293" data-original-width="899" height="208" src="https://1.bp.blogspot.com/-5lyl-oGlCtU/YNNk8-JpCEI/AAAAAAAAHww/Fgyb8zfkRncpkaRvpEGzh2W7Wwc1SGh4gCLcBGAsYHQ/w640-h208/genomics%2Bfundus%2Bimage%2BVCDR.jpg" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;A representative retinal fundus image showing the vertical cup-to-disc ratio, which is an important diagnostic measurement for glaucoma.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;We applied this model to predict VCDR in all fundus images from individuals in the &lt;a href="https://www.ukbiobank.ac.uk/"&gt;UK Biobank&lt;/a&gt;, which is the world’s largest dataset available to researchers worldwide for health-related research in the public interest, containing extensive phenotyping and genetic data for ~500,000 pseudonymized (the UK Biobank's standard for de-identification) individuals. We then performed GWAS in this dataset to identify genetic variants that are associated with the model-based predictions of VCDR. &lt;/p&gt;   &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-czEja2pkbL8/YNNjJZUCQsI/AAAAAAAAHwU/WqO4yDtk83ow_0IqycLUYps5Sf6-O0IRQCLcBGAsYHQ/s1880/image3.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="964" data-original-width="1880" height="328" src="https://1.bp.blogspot.com/-czEja2pkbL8/YNNjJZUCQsI/AAAAAAAAHwU/WqO4yDtk83ow_0IqycLUYps5Sf6-O0IRQCLcBGAsYHQ/w640-h328/image3.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Applying a VCDR prediction model trained on clinical data to generate predicted values for VCDR to enable discovery of genetic associations for the VCDR trait.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;The ML-based GWAS identified 156 distinct genomic regions associated with VCDR. We compared these results to a VCDR GWAS conducted by another group on the same UK Biobank data, &lt;a href="https://www.nature.com/articles/s41588-019-0556-y"&gt;Craig et al. 2020&lt;/a&gt;, where experts had painstakingly labeled all images for VCDR. The ML-based GWAS replicates 62 of the 65 associations found in Craig &lt;em&gt;et al.&lt;/em&gt;, which indicates that the model accurately predicts VCDR in the UK Biobank images. Additionally, the ML-based GWAS discovered 93 novel associations. &lt;/p&gt;   &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-sZzLM8IZYcs/YNNlD-28SNI/AAAAAAAAHw0/VpXxgbDgTDkIMYX6307GvbwFOeGmVp0kgCLcBGAsYHQ/s403/genomics%2Bvenn.jpg" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="234" data-original-width="403" height="233" src="https://1.bp.blogspot.com/-sZzLM8IZYcs/YNNlD-28SNI/AAAAAAAAHw0/VpXxgbDgTDkIMYX6307GvbwFOeGmVp0kgCLcBGAsYHQ/w400-h233/genomics%2Bvenn.jpg" width="400" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Number of statistically significant GWAS associations discovered by exhaustive expert labeling approach (Craig &lt;em&gt;et al&lt;/em&gt;., left), and by our ML-based approach (right), with shared associations in the middle.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;p&gt;&lt;b&gt;The ML-based GWAS improves polygenic model predictions&lt;/b&gt;&lt;br /&gt;To validate that the novel associations discovered in the ML-based GWAS are biologically relevant, we developed independent PRSes using the Craig &lt;em&gt;et al.&lt;/em&gt; and ML-based GWAS results, and tested their ability to predict human-expert-labeled VCDR in a subset of UK Biobank as well as a fully independent cohort (&lt;a href="https://www.epic-norfolk.org.uk/"&gt;EPIC-Norfolk&lt;/a&gt;). The PRS developed from the ML-based GWAS showed greater predictive ability than the PRS built from the expert labeling approach in both datasets, providing strong evidence that the novel associations discovered by the ML-based method influence VCDR biology, and suggesting that the improved phenotyping accuracy (i.e., more accurate VCDR measurement) of the model translates into a more powerful GWAS. &lt;/p&gt; &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-UMiJ7ass7y8/YNNo7SboEgI/AAAAAAAAHxA/_JGvVES_fq0akw5KaHj7Ob0pOLDJBirVACLcBGAsYHQ/s938/image4.jpg" imageanchor="1" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="475" data-original-width="938" height="324" src="https://1.bp.blogspot.com/-UMiJ7ass7y8/YNNo7SboEgI/AAAAAAAAHxA/_JGvVES_fq0akw5KaHj7Ob0pOLDJBirVACLcBGAsYHQ/w640-h324/image4.jpg" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;The correlation between a polygenic risk score (PRS) for VCDR generated from the ML-based approach and the exhaustive expert labeling approach (Craig &lt;em&gt;et al.&lt;/em&gt;). In these plots, higher values on the y-axis indicate a greater correlation and therefore greater prediction from only the genetic data. [* — p ≤ 0.05; *** — p ≤ 0.001]&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;As a second validation, because we know that VCDR is strongly correlated with glaucoma, we also investigated whether the ML-based PRS was correlated with individuals who had either self-reported that they had glaucoma or had medical procedure codes suggestive of glaucoma or glaucoma treatment. We found that the PRS for VCDR determined using our model predictions were also predictive of the probability that an individual had indications of glaucoma. Individuals with a PRS 2.5 or more standard deviations higher than the mean were more than 3 times as likely to have glaucoma in this cohort. We also observed that the VCDR PRS from ML-based phenotypes was more predictive of glaucoma than the VCDR PRS produced from the extensive manual phenotyping.  &lt;/p&gt;   &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-Qw9SCuYaOjY/YNNjJWGOICI/AAAAAAAAHwQ/e5RE3jUNBjU2lA4tD96eE7QNsQZuWPRXgCLcBGAsYHQ/s1704/image1.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="1200" data-original-width="1704" height="450" src="https://1.bp.blogspot.com/-Qw9SCuYaOjY/YNNjJWGOICI/AAAAAAAAHwQ/e5RE3jUNBjU2lA4tD96eE7QNsQZuWPRXgCLcBGAsYHQ/w640-h450/image1.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;The odds ratio of glaucoma (self-report or ICD code) stratified by the PRS for VCDR determined using the ML-based phenotypes (in standard deviations from the mean). In this plot, the y-axis shows the probability that the individual has glaucoma relative to the baseline rate (represented by the dashed line). The x-axis shows standard deviations from the mean for the PRS. Data are visualized as a standard &lt;a href="https://en.wikipedia.org/wiki/Box_plot"&gt;box plot&lt;/a&gt;, which illustrates values for the mean (the orange line), first and third quartiles, and minimum and maximum.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;p&gt;&lt;b&gt;Conclusion&lt;/b&gt;&lt;br /&gt;We have shown that ML models can be used to quickly phenotype large cohorts for GWAS, and that these models can increase statistical power in such studies. Although these examples were shown for eye traits predicted from retinal imaging, we look forward to exploring how this concept could generally apply to other diseases and data types.  &lt;/p&gt;&lt;p&gt;&lt;b&gt;Acknowledgments&lt;/b&gt;&lt;br /&gt;&lt;em&gt;We would like to especially thank co-author &lt;a href="https://www.moorfields.nhs.uk/consultant/anthony-khawaja"&gt;Dr. Anthony Khawaja&lt;/a&gt; of &lt;a href="https://www.moorfields.nhs.uk/"&gt;Moorfields Eye Hospital&lt;/a&gt; for contributing his extensive medical expertise. We also recognize the  efforts of Professor Jamie Craig and colleagues for their exhaustive labeling of UK Biobank images, which allowed us to make comparisons with our method. Several authors of that work, as well as Professor Stuart MacGregor and collaborators in Australia and at Max Kelsen have &lt;a href="https://www.cell.com/ajhg/fulltext/S0002-9297(21)00189-0"&gt;independently replicated these findings&lt;/a&gt;, and we value these scientific contributions as well. Last, this work summarizes the work of the following Google contributors, who we would like to thank: Babak Alipanahi, Farhad Hormozdiari, Babak Behsaz, Justin Cosentino, Zachary R. McCaw, Emanuel Schorsch, D. Sculley, Elizabeth H. Dorfman, Sonia Phene, Naama Hammel, Andrew Carroll, and Cory Y. McLean&lt;/em&gt;&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/blogspot/gJZg?a=dUKd8nUp3vY:icK6m6lFdqs:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/blogspot/gJZg/~4/dUKd8nUp3vY" height="1" width="1" alt=""/&gt;</content>
    <link rel="replies" type="application/atom+xml" href="http://ai.googleblog.com/feeds/613959647323225034/comments/default" title="Post Comments"/>
    <link rel="replies" type="text/html" href="http://ai.googleblog.com/2021/06/improving-genomic-discovery-with.html#comment-form" title="0 Comments"/>
    <link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/613959647323225034"/>
    <link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/613959647323225034"/>
    <link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/blogspot/gJZg/~3/dUKd8nUp3vY/improving-genomic-discovery-with.html" title="Improving Genomic Discovery with Machine Learning"/>
    <author>
      <name>Google AI</name>
      <uri>http://www.blogger.com/profile/12098626514775266161</uri>
      <email>noreply@blogger.com</email>
      <gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="16" height="16" src="https://img1.blogblog.com/img/b16-rounded.gif"/>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://1.bp.blogspot.com/-5lyl-oGlCtU/YNNk8-JpCEI/AAAAAAAAHww/Fgyb8zfkRncpkaRvpEGzh2W7Wwc1SGh4gCLcBGAsYHQ/s72-w640-h208-c/genomics%2Bfundus%2Bimage%2BVCDR.jpg" height="72" width="72"/>
    <thr:total>0</thr:total>
    <feedburner:origLink>http://ai.googleblog.com/2021/06/improving-genomic-discovery-with.html</feedburner:origLink>
  </entry>
  <entry>
    <id>tag:blogger.com,1999:blog-8474926331452026626.post-2976383705310409753</id>
    <published>2021-06-22T13:33:00.000-07:00</published>
    <updated>2021-06-22T13:33:53.451-07:00</updated>
    <category scheme="http://www.blogger.com/atom/ns#" term="Deep Learning"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="Quantum AI"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="Quantum Computing"/>
    <title type="text">Quantum Machine Learning and the Power of Data</title>
    <content type="html">&lt;span class="byline-author"&gt;Posted by Jarrod McClean, Staff Research Scientist and Hsin-Yuan (Robert) Huang&lt;sup id="fnref1"&gt;&lt;a href="#fn1" rel="footnote"&gt;&lt;span style="font-size: x-small;"&gt;1&lt;/span&gt;&lt;/a&gt;&lt;/sup&gt;, Intern, Google Quantum AI&lt;/span&gt; &lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Quantum_computing"&gt;Quantum computing&lt;/a&gt; has rapidly advanced in both theory and practice in recent years, and with it the hope for the potential impact in real applications. One key area of interest is how quantum computers might affect machine learning. We recently &lt;a href="https://ai.googleblog.com/2019/10/quantum-supremacy-using-programmable.html"&gt;demonstrated experimentally&lt;/a&gt; that quantum computers are able to naturally solve certain problems with complex correlations between inputs that can be incredibly hard for traditional, or “classical”, computers. This suggests that learning models made on quantum computers may be dramatically more powerful for select applications, potentially boasting faster computation, better generalization on less data, or both. Hence it is of great interest to understand in what situations such a “quantum advantage” might be achieved. &lt;/p&gt;&lt;p&gt;The idea of quantum advantage is typically phrased in terms of computational advantages. That is, given some task with well defined inputs and outputs, can a quantum computer achieve a more accurate result  than a  classical machine in a comparable runtime? There are a number of algorithms for which quantum computers are suspected to have overwhelming advantages, such as &lt;a href="https://en.wikipedia.org/wiki/Shor%27s_algorithm"&gt;Shor’s factoring algorithm&lt;/a&gt; for factoring products of large primes (relevant to &lt;a href="https://en.wikipedia.org/wiki/RSA_(cryptosystem)"&gt;RSA encryption&lt;/a&gt;) or the &lt;a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.654.7909&amp;amp;rep=rep1&amp;amp;type=pdf"&gt;quantum simulation of quantum systems&lt;/a&gt;. However, the difficulty of solving a problem, and hence the potential advantage for a quantum computer, can be greatly impacted by the &lt;em&gt;availability of data&lt;/em&gt;. As such, understanding when a quantum computer can help in a machine learning task depends not only on the task, but also the data available, and a complete understanding of this must include both. &lt;/p&gt;&lt;p&gt;In “&lt;a href="https://www.nature.com/articles/s41467-021-22539-9"&gt;Power of data in quantum machine learning&lt;/a&gt;”, published in &lt;em&gt;&lt;a href="https://www.nature.com/ncomms/"&gt;Nature Communications&lt;/a&gt;&lt;/em&gt;, we dissect the problem of quantum advantage in machine learning to better understand when it will apply. We show how the complexity of a problem formally changes with the availability of data, and how this sometimes has the power to elevate classical learning models to be competitive with quantum algorithms. We then develop a practical method for screening when there may be a quantum advantage for a chosen set of data embeddings in the context of &lt;a href="https://en.wikipedia.org/wiki/Kernel_method"&gt;kernel methods&lt;/a&gt;. We use the insights from the screening method and learning bounds to introduce a novel method that projects select aspects of feature maps from a quantum computer back into classical space. This enables us to imbue the quantum approach with additional insights from classical machine learning that shows the best empirical separation in quantum learning advantages to date. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Computational Power of Data&lt;/b&gt;&lt;br /&gt;The idea of quantum advantage over a classical computer is often framed in terms of computational complexity classes. Examples such as factoring large numbers and simulating quantum systems are classified as bounded quantum polynomial time (BQP) problems, which are those thought to be handled more easily by quantum computers than by classical systems. Problems easily solved on classical computers are called bounded probabilistic polynomial (BPP) problems.  &lt;/p&gt;&lt;p&gt;We show that learning algorithms equipped with data from a quantum process, such as a natural process like fusion or chemical reactions, form a new class of problems (which we call BPP/Samp) that can efficiently perform some tasks that traditional algorithms without data cannot, and is a subclass of the problems efficiently solvable with polynomial sized advice (P/poly). This demonstrates that for some machine learning tasks, understanding the quantum advantage requires examination of available data as well. &lt;/p&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-zgqODKx4UZM/YNI5FjTWbYI/AAAAAAAAHv0/j9pRkvsY_WccThdwo_qbhGiepC4S3y2lQCLcBGAsYHQ/s1999/image1.png" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="1710" data-original-width="1999" height="343" src="https://1.bp.blogspot.com/-zgqODKx4UZM/YNI5FjTWbYI/AAAAAAAAHv0/j9pRkvsY_WccThdwo_qbhGiepC4S3y2lQCLcBGAsYHQ/w400-h343/image1.png" width="400" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;b&gt;&lt;br /&gt;Geometric Test for Quantum Learning Advantage&lt;/b&gt;&lt;br /&gt;Informed by the results that the potential for advantage changes depending on the availability of data, one may ask how a practitioner can quickly evaluate if their problem may be well suited for a quantum computer. To help with this, we developed a workflow for assessing the potential for advantage within a kernel learning framework. We examined a number of tests, the most powerful and informative of which was a novel geometric test we developed. &lt;p&gt;&lt;/p&gt;&lt;p&gt;In &lt;a href="https://en.wikipedia.org/wiki/Quantum_machine_learning"&gt;quantum machine learning&lt;/a&gt; methods, such as &lt;a href="https://ai.googleblog.com/2018/12/exploring-quantum-neural-networks.html"&gt;quantum neural networks&lt;/a&gt; or quantum kernel methods, a quantum program is often divided into two parts, a quantum embedding of the data (an embedding map for the feature space using a quantum computer), and the evaluation of a function applied to the data embedding. In the context of quantum computing, quantum kernel methods make use of traditional kernel methods, but use the quantum computer to evaluate part or all of the kernel on the quantum embedding, which has a different geometry than a classical embedding. It was conjectured that a quantum advantage might arise from the quantum embedding, which might be much better suited to a particular problem than any accessible classical geometry.  &lt;/p&gt;&lt;p&gt;We developed a quick and rigorous test that can be used to quickly compare a particular quantum embedding, kernel, and data set to a range of classical kernels and assess if there is any opportunity for quantum advantage across, e.g., possible label functions such as those used for image recognition tasks. We define a geometric constant &lt;em&gt;g&lt;/em&gt;, which quantifies the amount of data that could theoretically close that gap, based on the geometric test. This is an extremely useful technique for deciding, based on data constraints, if a quantum solution is right for the given problem. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Projected Quantum Kernel Approach&lt;/b&gt;&lt;br /&gt;One insight revealed by the geometric test, was that existing quantum kernels often suffered from a geometry that was easy to best classically because they encouraged memorization, instead of understanding. This inspired us to develop a &lt;em&gt;projected&lt;/em&gt; quantum kernel, in which the quantum embedding is projected back to a classical representation.  While this representation is still hard to compute with a classical computer directly, it comes with a number of practical advantages in comparison to staying in the quantum space entirely.   &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-YTYoa70wmQI/YNI5LbKYWvI/AAAAAAAAHv4/chQCZqtHBBIboAApvTYe5fk-59t4GWjUQCLcBGAsYHQ/s1999/image2.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="1589" data-original-width="1999" height="509" src="https://1.bp.blogspot.com/-YTYoa70wmQI/YNI5LbKYWvI/AAAAAAAAHv4/chQCZqtHBBIboAApvTYe5fk-59t4GWjUQCLcBGAsYHQ/w640-h509/image2.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Geometric quantity &lt;em&gt;g&lt;/em&gt;, which quantifies the potential for quantum advantage, depicted for several embeddings, including the projected quantum kernel introduced here.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;By selectly projecting back to classical space, we can retain aspects of the quantum geometry that are still hard to simulate classically, but now it is much easier to develop distance functions, and hence kernels, that are better behaved with respect to modest changes in the input than was the original quantum kernel. In addition the projected quantum kernel facilitates better integration with powerful non-linear kernels (like a squared exponential) that have been developed classically, which is much more challenging to do in the native quantum space. &lt;/p&gt;&lt;p&gt;This projected quantum kernel has a number of benefits over previous approaches, including an improved ability to describe non-linear functions of the existing embedding, a reduction in the resources needed to process the kernel from quadratic to linear with the number of data points, and the ability to generalize better at larger sizes. The kernel also helps to expand the geometric &lt;em&gt;g&lt;/em&gt;, which helps to ensure the greatest potential for quantum advantage. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Data Sets Exhibit Learning Advantages&lt;/b&gt;&lt;br /&gt;The geometric test quantifies potential advantage for all possible label functions, however in practice we are most often interested in specific label functions.  Using learning theoretic approaches, we also bound the generalization error for specific tasks, including those which are definitively quantum in origin.  As the advantage of a quantum computer relies on its ability to use many qubits simultaneously but previous approaches scale poorly in number of qubits, it is important to verify the tasks at reasonably large qubit sizes ( &amp;gt; 20 ) to ensure a method has the potential to scale to real problems.  For our studies we verified up to 30 qubits, which was enabled by the open source tool, &lt;a href="https://ai.googleblog.com/2020/03/announcing-tensorflow-quantum-open.html"&gt;TensorFlow-Quantum&lt;/a&gt;, enabling scaling to petaflops of compute. &lt;/p&gt;&lt;p&gt;Interestingly, we showed that many naturally quantum problems, even up to 30 qubits, were readily handled by classical learning methods when sufficient data were provided. Hence one conclusion is that even for some problems that look quantum, classical machine learning methods empowered by data can match the power of quantum computers.  However, using the geometric construction in combination with the projected quantum kernel, we were able to construct a data set that exhibited an empirical learning advantage for a quantum model over a classical one. Thus, while it remains an open question to find such data sets in natural problems, we were able to show the existence of label functions where this can be the case. Although this problem was engineered and a quantum computational advantage would require the embeddings to be larger and more challenging, this work represents an important step in understanding the role data plays in quantum machine learning.  &lt;/p&gt;&lt;p&gt;&lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-LcPnij2Zt_g/YNI5VrtfhyI/AAAAAAAAHwA/B0mtaA3z02ErcKqaY53cqjlQiJ-iRQiFACLcBGAsYHQ/s1999/image3.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="996" data-original-width="1999" height="318" src="https://1.bp.blogspot.com/-LcPnij2Zt_g/YNI5VrtfhyI/AAAAAAAAHwA/B0mtaA3z02ErcKqaY53cqjlQiJ-iRQiFACLcBGAsYHQ/w640-h318/image3.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Prediction accuracy as a function of the number of qubits (&lt;i&gt;n&lt;/i&gt;) for a problem engineered to maximize the potential for learning advantage in a quantum model.  The data is shown for two different sizes of training data (&lt;i&gt;N&lt;/i&gt;).&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;For this problem, we scaled up the number of qubits (&lt;em&gt;n&lt;/em&gt;) and compared the prediction accuracy of the projected quantum kernel to existing kernel approaches and the best classical machine learning model in our dataset. Moreover, a key takeaway from these results is that although we showed the existence of datasets where a quantum computer has an advantage, for many quantum problems, classical learning methods were still the best approach. Understanding how data can affect a given problem is a key factor to consider when discussing quantum advantage in learning problems, unlike traditional computation problems for which that is not a consideration. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Conclusions&lt;/b&gt;&lt;br /&gt;When considering the ability of quantum computers to aid in machine learning, we have shown that the availability of data fundamentally changes the question. In our work, we develop a practical set of tools for examining these questions, and use them to develop a new projected quantum kernel method that has a number of advantages over existing approaches. We build towards the largest numerical demonstration to date, 30 qubits, of potential learning advantages for quantum embeddings. While a complete computational advantage on a real world application remains to be seen, this work helps set the foundation for the path forward. We encourage any interested readers to check out both &lt;a href="https://www.nature.com/articles/s41467-021-22539-9"&gt;the paper&lt;/a&gt; and &lt;a href="https://www.tensorflow.org/quantum/tutorials/quantum_data"&gt;related TensorFlow-Quantum tutorials&lt;/a&gt; that make it easy to build on this work. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Acknowledgements&lt;/b&gt;&lt;br /&gt;&lt;em&gt;We would like to acknowledge our co-authors on this paper — Michael Broughton, Masoud Mohseni, Ryan Babbush, Sergio Boixo, and Hartmut Neven, as well as the entirety of the &lt;a href="https://quantumai.google/"&gt;Google Quantum AI&lt;/a&gt; team. In addition, we acknowledge valuable help and feedback from Richard Kueng, John Platt, John Preskill, Thomas Vidick, Nathan Wiebe, Chun-Ju Wu, and Balint Pato.&lt;/em&gt;&lt;/p&gt;&lt;!--Footnotes--&gt;&lt;hr width="80%" /&gt;&lt;p&gt;  &lt;span class="Apple-style-span" style="font-size: x-small;"&gt;&lt;sup&gt;&lt;a name="fn1"&gt;&lt;b&gt;1&lt;/b&gt;&lt;/a&gt;&lt;/sup&gt;Current affiliation — Institute for Quantum Information and Matter and Department of Computing and Mathematical Sciences, Caltech, Pasadena, CA, USA&lt;a href="#fnref1" rev="footnote"&gt;&lt;sup&gt;↩&lt;/sup&gt;&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/blogspot/gJZg?a=5mwX1gN-7UE:VMkMsuoXIc4:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/blogspot/gJZg/~4/5mwX1gN-7UE" height="1" width="1" alt=""/&gt;</content>
    <link rel="replies" type="application/atom+xml" href="http://ai.googleblog.com/feeds/2976383705310409753/comments/default" title="Post Comments"/>
    <link rel="replies" type="text/html" href="http://ai.googleblog.com/2021/06/quantum-machine-learning-and-power-of.html#comment-form" title="0 Comments"/>
    <link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/2976383705310409753"/>
    <link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/2976383705310409753"/>
    <link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/blogspot/gJZg/~3/5mwX1gN-7UE/quantum-machine-learning-and-power-of.html" title="Quantum Machine Learning and the Power of Data"/>
    <author>
      <name>Google AI</name>
      <uri>http://www.blogger.com/profile/12098626514775266161</uri>
      <email>noreply@blogger.com</email>
      <gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="16" height="16" src="https://img1.blogblog.com/img/b16-rounded.gif"/>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://1.bp.blogspot.com/-zgqODKx4UZM/YNI5FjTWbYI/AAAAAAAAHv0/j9pRkvsY_WccThdwo_qbhGiepC4S3y2lQCLcBGAsYHQ/s72-w400-h343-c/image1.png" height="72" width="72"/>
    <thr:total>0</thr:total>
    <feedburner:origLink>http://ai.googleblog.com/2021/06/quantum-machine-learning-and-power-of.html</feedburner:origLink>
  </entry>
  <entry>
    <id>tag:blogger.com,1999:blog-8474926331452026626.post-7776510198374933258</id>
    <published>2021-06-21T14:04:00.001-07:00</published>
    <updated>2021-07-16T06:54:36.294-07:00</updated>
    <category scheme="http://www.blogger.com/atom/ns#" term="Computer Vision"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="conferences"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="CVPR"/>
    <title type="text">Google at CVPR 2021</title>
    <content type="html">&lt;span class="byline-author"&gt;Posted by Emily Knapp and Tim Herrmann, Program Managers&lt;/span&gt; &lt;p&gt;This week marks the start of the &lt;a href="http://cvpr2021.thecvf.com/"&gt;2021 Conference on Computer Vision and Pattern Recognition&lt;/a&gt; (CVPR 2021), the premier annual computer vision event consisting of the &lt;a href="http://cvpr2021.thecvf.com/node/141"&gt;main conference&lt;/a&gt;, &lt;a href="http://cvpr2021.thecvf.com/workshops-schedule"&gt;workshops&lt;/a&gt; and &lt;a href="http://cvpr2021.thecvf.com/program"&gt;tutorials&lt;/a&gt;. As a leader in computer vision research and a &lt;a href="http://cvpr2021.thecvf.com/sponsors"&gt;Champion Level Sponsor&lt;/a&gt;, Google will have a strong presence at CVPR 2021, with over 70 publications accepted, along with the organization of and participation in multiple workshops and tutorials. &lt;/p&gt;&lt;p&gt;If you are participating in CVPR this year, please visit our virtual booth to learn about Google research into the next generation of intelligent systems that utilize the latest machine learning techniques applied to various areas of &lt;a href="https://research.google/research-areas/machine-perception/"&gt;machine perception&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;You can also learn more about our research being presented at CVPR 2021 in the list below (&lt;strong&gt;Google affiliations in bold&lt;/strong&gt;). &lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;span style="text-decoration:underline;"&gt;Organizing Committee Members&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;General Chair: &lt;b&gt;&lt;em&gt;Rahul Sukthankar&lt;/em&gt;&lt;/b&gt;&lt;br/&gt;Finance Chair: &lt;b&gt;&lt;em&gt;Ramin Zabih&lt;/em&gt;&lt;/b&gt;&lt;br/&gt;Workshop Chair: &lt;b&gt;&lt;em&gt;Caroline Pantofaru&lt;/em&gt;&lt;/b&gt;&lt;br/&gt;Area Chairs: &lt;b&gt;&lt;em&gt;Chen Sun, Golnaz Ghiasi, Jonathan Barron, Kostas Rematas, Negar Rostamzadeh, Noah Snavely, Sanmi Koyejo, Tsung-Yi Lin&lt;/em&gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;span style="text-decoration:underline;"&gt;Publications&lt;/span&gt;&lt;/b&gt;&lt;/p&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2101.04702.pdf"&gt;Cross-Modal Contrastive Learning for Text-to-Image Generation&lt;/a&gt; (see the &lt;a href="https://ai.googleblog.com/2021/05/cross-modal-contrastive-learning-for.html"&gt;blog post&lt;/a&gt;)&lt;br/&gt;    &lt;em&gt;&lt;b&gt;Han Zhang, Jing Yu Koh, Jason Baldridge&lt;/b&gt;, Honglak Lee*, &lt;b&gt;Yinfei Yang&lt;/b&gt;&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2102.01987.pdf"&gt;Learning Graph Embeddings for Compositional Zero-Shot Learning&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Muhammad Ferjad Naeem, Yongqin Xian, &lt;b&gt;Federico Tombari&lt;/b&gt;, Zeynep Akata&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2006.14660.pdf"&gt;SPSG: Self-Supervised Photometric Scene Generation From RGB-D Scans&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Angela Dai, Yawar Siddiqui, Justus Thies, &lt;b&gt;Julien Valentin&lt;/b&gt;, Matthias Nießner&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2103.16054.pdf"&gt;3D-MAN: 3D Multi-Frame Attention Network for Object Detection&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Zetong Yang*, Yin Zhou, &lt;b&gt;Zhifeng Chen, Jiquan Ngiam&lt;/b&gt;&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/1811.10725.pdf"&gt;MIST: Multiple Instance Spatial Transformer&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Baptiste Angles, Yuhe Jin, &lt;b&gt;Simon Kornblith, Andrea Tagliasacchi&lt;/b&gt;, Kwang Moo Yi&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="http://people.csail.mit.edu/celiu/pdfs/CVPR21_SemanticUncrop.pdf"&gt;OCONet: Image Extrapolation by Object Completion&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Richard Strong Bowen*, &lt;b&gt;Huiwen Chang&lt;/b&gt;, Charles Herrmann*, Piotr Teterwak*, &lt;b&gt;Ce Liu, Ramin Zabih&lt;/b&gt;&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2011.11200.pdf"&gt;Ranking Neural Checkpoints&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Yandong Li, Xuhui Jia, Ruoxin Sang, Yukun Zhu, Bradley Green, Liqiang Wang, Boqing Gong&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2106.04185.pdf"&gt;LipSync3D: Data-Efficient Learning of Personalized 3D Talking Faces From Video Using Pose and Lighting Normalization&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Avisek Lahiri, Vivek Kwatra, Christian Frueh, John Lewis, Chris Bregler&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2104.03059.pdf"&gt;Differentiable Patch Selection for Image Recognition&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Jean-Baptiste Cordonnier*, &lt;b&gt;Aravindh Mahendran, Alexey Dosovitskiy, Dirk Weissenborn, Jakob Uszkoreit, Thomas Unterthiner&lt;/b&gt;&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2103.15573.pdf"&gt;HumanGPS: Geodesic PreServing Feature for Dense Human Correspondences&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Feitong Tan, Danhang Tang, Mingsong Dou, Kaiwen Guo, Rohit Pandey, Cem Keskin, Ruofei Du, Deqing Sun, Sofien Bouaziz, Sean Fanello, Ping Tan, Yinda Zhang&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2012.05258.pdf"&gt;VIP-DeepLab: Learning Visual Perception With Depth-Aware Video Panoptic Segmentation&lt;/a&gt; (see the &lt;a href="https://ai.googleblog.com/2021/04/holistic-video-scene-understanding-with.html"&gt;blog post&lt;/a&gt;)&lt;br/&gt;  &lt;em&gt;Siyuan Qiao*, &lt;b&gt;Yukun Zhu, Hartwig Adam&lt;/b&gt;, Alan Yuille, &lt;b&gt;Liang-Chieh Chen&lt;/b&gt;&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2012.00595.pdf"&gt;DeFMO: Deblurring and Shape Recovery of Fast Moving Objects&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Denys Rozumnyi, Martin R. Oswald, &lt;b&gt;Vittorio Ferrari&lt;/b&gt;, Jiri Matas, Marc Pollefeys&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Mi_HDMapGen_A_Hierarchical_Graph_Generative_Model_of_High_Definition_Maps_CVPR_2021_paper.pdf"&gt;HDMapGen: A Hierarchical Graph Generative Model of High Definition Maps&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Lu Mi, Hang Zhao, Charlie Nash&lt;b&gt;, &lt;/b&gt;Xiaohan Jin, Jiyang Gao,&lt;b&gt; Chen Sun, Cordelia Schmid, &lt;/b&gt;Nir Shavit,&lt;b&gt; &lt;/b&gt;Yuning Chai, Dragomir Anguelov&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2106.03336.pdf"&gt;Wide-Baseline Relative Camera Pose Estimation With Directional Learning&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Kefan Chen, Noah Snavely, Ameesh Makadia&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2004.14525.pdf"&gt;MobileDets: Searching for Object Detection Architectures for Mobile Accelerators&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Yunyang Xiong, &lt;b&gt;Hanxiao Liu, Suyog Gupta, Berkin Akin, Gabriel Bender, Yongzhe Wang, Pieter-Jan Kindermans, Mingxing Tan&lt;/b&gt;, Vikas Singh, &lt;b&gt;Bo Chen&lt;/b&gt;&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2105.07014.pdf"&gt;SMURF: Self-Teaching Multi-Frame Unsupervised RAFT With Full-Image Warping&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Austin Stone, Daniel Maurer, Alper Ayvaci, Anelia Angelova, Rico Jonschkowski&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2102.08981.pdf"&gt;Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Soravit Changpinyo, Piyush Sharma, Nan Ding, Radu Soricut&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2012.06777.pdf"&gt;Uncalibrated Neural Inverse Rendering for Photometric Stereo of General Surfaces&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Berk Kaya, Suryansh Kumar, Carlos Oliveira,&lt;b&gt; Vittorio Ferrari&lt;/b&gt;, Luc Van Gool&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2104.00303.pdf"&gt;MeanShift++: Extremely Fast Mode-Seeking With Applications to Segmentation and Object Tracking&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Jennifer Jang,&lt;b&gt; Heinrich Jiang&lt;/b&gt;&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2103.16183.pdf"&gt;Repopulating Street Scenes&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Yifan Wang*, &lt;b&gt;Andrew Liu, Richard Tucker&lt;/b&gt;, Jiajun Wu, &lt;b&gt;Brian L. Curless, Steven M. Seitz, Noah Snavely&lt;/b&gt;&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2012.00759.pdf"&gt;MaX-DeepLab: End-to-End Panoptic Segmentation With Mask Transformers&lt;/a&gt; (see the &lt;a href="https://ai.googleblog.com/2021/04/max-deeplab-dual-path-transformers-for.html"&gt;blog post&lt;/a&gt;)&lt;br/&gt;  &lt;em&gt;Huiyu Wang*, &lt;b&gt;Yukun Zhu, Hartwig Adam&lt;/b&gt;, Alan Yuille, &lt;b&gt;Liang-Chieh Chen&lt;/b&gt;&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2102.13090.pdf"&gt;IBRNet: Learning Multi-View Image-Based Rendering&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul Srinivasan, Howard Zhou, Jonathan T. Barron, Ricardo Martin-Brualla, Noah Snavely, Thomas Funkhouser&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2012.11575.pdf"&gt;From Points to Multi-Object 3D Reconstruction&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Francis Engelmann*, &lt;b&gt;Konstantinos Rematas&lt;/b&gt;, Bastian Leibe, &lt;b&gt;Vittorio Ferrari&lt;/b&gt;&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2103.08271.pdf"&gt;Learning Compositional Representation for 4D Captures With Neural ODE&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Boyan Jiang, &lt;b&gt;Yinda Zhang&lt;/b&gt;, Xingkui Wei, Xiangyang Xue, Yanwei Fu&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Kapishnikov_Guided_Integrated_Gradients_An_Adaptive_Path_Method_for_Removing_Noise_CVPR_2021_paper.pdf"&gt;Guided Integrated Gradients: An Adaptive Path Method for Removing Noise&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Andrei Kapishnikov, Subhashini Venugopalan, Besim Avci, Ben Wedin, Michael Terry, Tolga Bolukbasi&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2104.03954.pdf"&gt;De-Rendering the World’s Revolutionary Artefacts&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Shangzhe Wu*, &lt;b&gt;Ameesh Makadia&lt;/b&gt;, Jiajun Wu, &lt;b&gt;Noah Snavely, Richard Tucker, Angjoo Kanazawa&lt;/b&gt;&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2008.03800.pdf"&gt;Spatiotemporal Contrastive Video Representation Learning&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Rui Qian, Tianjian Meng, Boqing Gong, Ming-Hsuan Yang, Huisheng Wang, Serge Belongie, Yin Cui&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2104.14107.pdf"&gt;Decoupled Dynamic Filter Networks&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Jingkai Zhou, &lt;b&gt;Varun Jampani&lt;/b&gt;, Zhixiong Pi, Qiong Liu, &lt;b&gt;Ming-Hsuan Yang&lt;/b&gt;&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2103.07700.pdf"&gt;NeuralHumanFVV: Real-Time Neural Volumetric Human Performance Rendering Using RGB Cameras&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Xin Suo, Yuheng Jiang, Pei Lin, Yingliang Zhang, &lt;b&gt;Kaiwen Guo&lt;/b&gt;, Minye Wu, Lan Xu&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2104.03310.pdf"&gt;Regularizing Generative Adversarial Networks Under Limited Data&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Hung-Yu Tseng*,&lt;b&gt; Lu Jiang, Ce Liu, Ming-Hsuan Yang, Weilong Yang&lt;/b&gt;&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2103.14898.pdf"&gt;SceneGraphFusion: Incremental 3D Scene Graph Prediction From RGB-D Sequences&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Shun-Cheng Wu, Johanna Wald, &lt;b&gt;Keisuke Tateno&lt;/b&gt;, Nassir Navab, &lt;b&gt;Federico Tombari&lt;/b&gt;&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2012.03927.pdf"&gt;NeRV: Neural Reflectance and Visibility Fields for Relighting and View Synthesis&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Pratul P. Srinivasan, Boyang Deng, Xiuming Zhang, Matthew Tancik, Ben Mildenhall, Jonathan T. Barron&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2106.01899.pdf"&gt;Adversarially Adaptive Normalization for Single Domain Generalization&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Xinjie Fan*, &lt;b&gt;Qifei Wang, Junjie Ke, Feng Yang, Boqing Gong&lt;/b&gt;, Mingyuan Zhou&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2104.01893.pdf"&gt;Adaptive Prototype Learning and Allocation for Few-Shot Segmentation&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Gen Li, &lt;b&gt;Varun Jampani&lt;/b&gt;, Laura Sevilla-Lara, &lt;b&gt;Deqing Sun&lt;/b&gt;, Jonghyun Kim, Joongkyu Kim&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2012.00802.pdf"&gt;Adversarial Robustness Across Representation Spaces&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Pranjal Awasthi, George Yu, Chun-Sung Ferng, Andrew Tomkins, Da-Cheng Juan&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2008.12873.pdf"&gt;Background Splitting: Finding Rare Classes in a Sea of Background&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Ravi Teja Mullapudi, Fait Poms, &lt;b&gt;William R. Mark&lt;/b&gt;, Deva Ramanan, Kayvon Fatahalian&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2102.05610.pdf"&gt;Searching for Fast Model Families on Datacenter Accelerators&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Sheng Li, Mingxing Tan, Ruoming Pang, Andrew Li, Liqun Cheng, Quoc Le, Norman P. Jouppi&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2012.09988.pdf"&gt;Objectron: A Large Scale Dataset of Object-Centric Videos in the Wild With Pose Annotations&lt;/a&gt; (see the &lt;a href="https://ai.googleblog.com/2020/11/announcing-objectron-dataset.html"&gt;blog post&lt;/a&gt;)&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Adel Ahmadyan, Liangkai Zhang, Jianing Wei, Artsiom Ablavatski, Matthias Grundmann&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2104.04015.pdf"&gt;CutPaste: Self-Supervised Learning for Anomaly Detection and Localization&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Chun-Liang Li, Kihyuk Sohn, Jinsung Yoon, Tomas Pfister&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2103.03375.pdf"&gt;Nutrition5k: Towards Automatic Nutritional Understanding of Generic Food&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Quin Thames, Arjun Karpur, Wade Norris, Fangting Xia, Liviu Panait, Tobias Weyand, Jack Sim&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2102.09559.pdf"&gt;CReST: A Class-Rebalancing Self-Training Framework for Imbalanced Semi-Supervised Learning&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Chen Wei*, &lt;b&gt;Kihyuk Sohn, Clayton Mellina&lt;/b&gt;, Alan Yuille, &lt;b&gt;Fan Yang&lt;/b&gt;&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2006.02334.pdf"&gt;DetectoRS: Detecting Objects With Recursive Feature Pyramid and Switchable Atrous Convolution&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Siyuan Qiao, &lt;b&gt;Liang-Chieh Chen&lt;/b&gt;, Alan Yuille&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2011.12490.pdf"&gt;DeRF: Decomposed Radiance Fields&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Daniel Rebain, Wei Jiang, &lt;b&gt;Soroosh Yazdani, Ke Li&lt;/b&gt;, Kwang Moo Yi,&lt;b&gt; Andrea Tagliasacchi&lt;/b&gt;&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2104.02416.pdf"&gt;Variational Transformer Networks for Layout Generation&lt;/a&gt; (see the &lt;a href="https://ai.googleblog.com/2021/06/using-variational-transformer-networks.html"&gt;blog post&lt;/a&gt;)&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Diego Martin Arroyo, Janis Postels, Federico Tombari&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Rich_Features_for_Perceptual_Quality_Assessment_of_UGC_Videos_CVPR_2021_paper.pdf"&gt;Rich Features for Perceptual Quality Assessment of UGC Videos&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Yilin Wang, Junjie Ke, Hossein Talebi, Joong Gon Yim, Neil Birkbeck, Balu Adsumilli, Peyman Milanfar, Feng Yang&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2007.08488.pdf"&gt;Complete &amp; Label: A Domain Adaptation Approach to Semantic Segmentation of LiDAR Point Clouds&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Li Yi, Boqing Gong, Thomas Funkhouser&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2008.06910.pdf"&gt;Neural Descent for Visual 3D Human Pose and Shape&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Andrei Zanfir, Eduard Gabriel Bazavan, Mihai Zanfir, William T. Freeman, Rahul Sukthankar, Cristian Sminchisescu&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2102.12145.pdf"&gt;GDR-Net: Geometry-Guided Direct Regression Network for Monocular 6D Object Pose Estimation&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Gu Wang, Fabian Manhardt, &lt;b&gt;Federico Tombari&lt;/b&gt;, Xiangyang Ji&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2012.05710.pdf"&gt;Look Before You Speak: Visually Contextualized Utterances&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Paul Hongsuck Seo, Arsha Nagrani, Cordelia Schmid&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2105.02976.pdf"&gt;LASR: Learning Articulated Shape Reconstruction From a Monocular Video&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Gengshan Yang*,&lt;b&gt; Deqing Sun, Varun Jampani, Daniel Vlasic, Forrester Cole, Huiwen Chang, &lt;/b&gt;Deva Ramanan&lt;b&gt;, William T. Freeman, Ce Liu&lt;/b&gt;&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2103.11511.pdf"&gt;MoViNets: Mobile Video Networks for Efficient Video Recognition&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Dan Kondratyuk, Liangzhe Yuan, Yandong Li, Li Zhang, Mingxing Tan, Matthew Brown, Boqing Gong&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2012.10565.pdf"&gt;No Shadow Left Behind: Removing Objects and Their Shadows Using Approximate Lighting and Geometry&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Edward Zhang,&lt;b&gt; Ricardo Martin-Brualla, Janne Kontkanen, Brian Curless&lt;/b&gt;&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2007.08558.pdf"&gt;On Robustness and Transferability of Convolutional Neural Networks&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Josip Djolonga, Jessica Yung, Michael Tschannen, Rob Romijnders, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Matthias Minderer, Alexander D'Amour, Dan Moldovan, Sylvain Gelly, Neil Houlsby, Xiaohua Zhai, Mario Lucic&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2103.13886.pdf"&gt;Robust and Accurate Object Detection via Adversarial Learning&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Xiangning Chen, Cihang Xie, Mingxing Tan, Li Zhang, Cho-Jui Hsieh, Boqing Gong&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Chai_To_the_Point_Efficient_3D_Object_Detection_in_the_Range_CVPR_2021_paper.pdf"&gt;To the Point: Efficient 3D Object Detection in the Range Image With Graph Convolution Kernels&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Yuning Chai, Pei Sun, &lt;b&gt;Jiquan Ngiam&lt;/b&gt;, Weiyue Wang, &lt;b&gt;Benjamin Caine, Vijay Vasudevan&lt;/b&gt;, Xiao Zhang, Dragomir Anguelov&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2101.11605.pdf"&gt;Bottleneck Transformers for Visual Recognition&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Aravind Srinivas,&lt;b&gt; Tsung-Yi Lin, Niki Parmar, Jonathon Shlens&lt;/b&gt;, Pieter Abbeel, &lt;b&gt;Ashish Vaswani&lt;/b&gt;&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2104.15092.pdf"&gt;Faster Meta Update Strategy for Noise-Robust Deep Learning&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Youjiang Xu, Linchao Zhu, &lt;b&gt;Lu Jiang&lt;/b&gt;, Yi Yang&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2105.10305.pdf"&gt;Correlated Input-Dependent Label Noise in Large-Scale Image Classification&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Mark Collier, Basil Mustafa, Efi Kokiopoulou, Rodolphe Jenatton, Jesse Berent&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2012.02189.pdf"&gt;Learned Initializations for Optimizing Coordinate-Based Neural Representations&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Matthew Tancik, Ben Mildenhall, Terrance Wang, Divi Schmidt, &lt;b&gt;Pratul P. Srinivasan, Jonathan T. Barron&lt;/b&gt;, Ren Ng&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2012.07177.pdf"&gt;Simple Copy-Paste Is a Strong Data Augmentation Method for Instance Segmentation&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Golnaz Ghiasi, Yin Cui, Aravind Srinivas*, Rui Qian, Tsung-Yi Lin, Ekin D. Cubuk, Quoc V. Le, Barret Zoph&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2105.01859.pdf"&gt;Function4D: Real-Time Human Volumetric Capture From Very Sparse Consumer RGBD Sensors&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Tao Yu, Zerong Zheng, &lt;b&gt;Kaiwen Guo&lt;/b&gt;, Pengpeng Liu, Qionghai Dai, Yebin Liu&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Sun_RSN_Range_Sparse_Net_for_Efficient_Accurate_LiDAR_3D_Object_CVPR_2021_paper.pdf"&gt;RSN: Range Sparse Net for Efficient, Accurate LiDAR 3D Object Detection&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Pei Sun, Weiyue Wang, Yuning Chai, &lt;b&gt;Gamaleldin Elsayed&lt;/b&gt;, &lt;b&gt;Alex Bewley&lt;/b&gt;, Xiao Zhang, &lt;b&gt;Cristian Sminchisescu&lt;/b&gt;, Dragomir Anguelov&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2008.02268.pdf"&gt;NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi, Jonathan T. Barron, Alexey Dosovitskiy, Daniel Duckworth&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2012.04746.pdf"&gt;Robust Neural Routing Through Space Partitions for Camera Relocalization in Dynamic Indoor Environments&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Siyan Dong, Qingnan Fan, He Wang, Ji Shi, &lt;b&gt;Li Yi, Thomas Funkhouser&lt;/b&gt;, Baoquan Chen, Leonidas Guibas&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2005.07289.pdf"&gt;Taskology: Utilizing Task Relations at Scale&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Yao Lu, Sören Pirk, Jan Dlabal, Anthony Brohan, Ankita Pasad*, Zhao Chen, Vincent Casser, Anelia Angelova, Ariel Gordon&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2105.06993.pdf"&gt;Omnimatte: Associating Objects and Their Effects in Video&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Erika Lu, Forrester Cole, Tali Dekel, Andrew Zisserman, William T. Freeman, Michael Rubinstein&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2104.14544.pdf"&gt;AutoFlow: Learning a Better Training Set for Optical Flow&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Deqing Sun, Daniel Vlasic, Charles Herrmann, Varun Jampani, Michael Krainin, Huiwen Chang, Ramin Zabih, William T. Freeman, and Ce Liu&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2104.01845.pdf"&gt;Unsupervised Multi-Source Domain Adaptation Without Access to Source Data&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Sk Miraj Ahmed, Dripta S. Raychaudhuri, &lt;b&gt;Sujoy Paul&lt;/b&gt;, Samet Oymak, Amit K. Roy-Chowdhury&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2003.10580.pdf"&gt;Meta Pseudo Labels&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Hieu Pham, Zihang Dai, Qizhe Xie, Minh-Thang Luong, Quoc V. Le&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2104.04160.pdf"&gt;Spatially-Varying Outdoor Lighting Estimation From Intrinsics&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Yongjie Zhu, &lt;b&gt;Yinda Zhang&lt;/b&gt;, Si Li, Boxin Shi&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2012.01405.pdf"&gt;Learning View-Disentangled Human Pose Representation by Contrastive Cross-View Mutual Information Maximization&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Long Zhao*, &lt;b&gt;Yuxiao Wang, Jiaping Zhao, Liangzhe Yuan, Jennifer J. Sun, Florian Schroff, Hartwig Adam&lt;/b&gt;, Xi Peng, Dimitris Metaxas, &lt;b&gt;Ting Liu&lt;/b&gt;&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2103.16483.pdf"&gt;Benchmarking Representation Learning for Natural World Image Collections&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Grant Van Horn, Elijah Cole, Sara Beery, &lt;b&gt;Kimberly Wilber, Serge Belongie&lt;/b&gt;, Oisin Mac Aodha&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2103.12731.pdf"&gt;Scaling Local Self-Attention for Parameter Efficient Visual Backbones&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Ashish Vaswani, Prajit Ramachandran, Aravind Srinivas, Niki Parmar, Blake Hechtman, Jonathon Shlens&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2104.11224.pdf"&gt;KeypointDeformer: Unsupervised 3D Keypoint Discovery for Shape Control&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Tomas Jakab*&lt;b&gt;, Richard Tucker, Ameesh Makadia&lt;/b&gt;, Jiajun Wu, &lt;b&gt;Noah Snavely, Angjoo Kanazawa&lt;/b&gt;&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2007.12140.pdf"&gt;HITNet: Hierarchical Iterative Tile Refinement Network for Real-time Stereo Matching&lt;/a&gt;&lt;br/&gt;  &lt;b&gt;&lt;em&gt;Vladimir Tankovich, Christian Häne, Yinda Zhang, Adarsh Kowdle, Sean Fanello, Sofien Bouaziz&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2103.15331.pdf"&gt;POSEFusion: Pose-Guided Selective Fusion for Single-View Human Volumetric Capture&lt;/a&gt;&lt;br/&gt;  &lt;em&gt;Zhe Li, Tao Yu, Zerong Zheng, &lt;b&gt;Kaiwen Guo&lt;/b&gt;, Yebin Liu&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;span style="text-decoration:underline;"&gt;Workshops&lt;/span&gt;&lt;/b&gt; (&lt;em&gt;only Google affiliations are noted&lt;/em&gt;)   &lt;/p&gt;&lt;p&gt;&lt;a href="https://sites.google.com/view/mediaforensics2021"&gt;Media Forensics&lt;/a&gt;&lt;br/&gt;  Organizers: &lt;b&gt;&lt;em&gt;Christoph Bregler&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://sites.google.com/corp/view/saiad2021/home?authuser=0"&gt;Safe Artificial Intelligence for Automated Driving&lt;/a&gt;&lt;br/&gt;  Invited Speakers: &lt;b&gt;&lt;em&gt;Been Kim&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://vizwiz.org/workshops/2021-workshop/"&gt;VizWiz Grand Challenge&lt;/a&gt;&lt;br/&gt;  Organizers: &lt;b&gt;&lt;em&gt;Meredith Morris&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://sites.google.com/corp/view/cvpr2021-3d-vision-robotics"&gt;3D Vision and Robotics&lt;/a&gt;&lt;br/&gt;  Invited Speaker:&lt;b&gt; &lt;em&gt;Andy Zeng&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://data.vision.ee.ethz.ch/cvl/ntire21/"&gt;New Trends in Image Restoration and Enhancement Workshop and Challenges on Image and Video Processing&lt;/a&gt;&lt;br/&gt;  Organizers:&lt;em&gt; &lt;b&gt;Ming-Hsuan Yang&lt;/b&gt;&lt;/em&gt;  Program Committee:&lt;b&gt; &lt;em&gt;George Toderici, Ming-Hsuan Yang&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://sites.google.com/corp/view/extremevision-v2/home?authuser=0"&gt;2nd Workshop on Extreme Vision Modeling&lt;/a&gt;&lt;br/&gt;  Invited Speakers:&lt;b&gt; &lt;em&gt;Quoc Le, Chen Sun&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://sites.google.com/corp/view/auvi-cvpr2021/home?authuser=0"&gt;First International Workshop on Affective Understanding in Video&lt;/a&gt;&lt;br/&gt;  Organizers: &lt;b&gt;&lt;em&gt;Gautam Prasad, Ting Liu&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://aisecure-workshop.github.io/amlcvpr2021/"&gt;Adversarial Machine Learning in Real-World Computer Vision Systems and Online Challenges&lt;/a&gt;&lt;br/&gt;  Program Committee:&lt;b&gt; &lt;em&gt;Nicholas Carlini, Nicolas Papernot&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://sites.google.com/corp/view/ec3v-cvpr2021/home?authuser=0"&gt;Ethical Considerations in Creative Applications of Computer Vision&lt;/a&gt;&lt;br/&gt;  Invited Speaker:&lt;b&gt; &lt;em&gt;Alex Hanna&lt;/em&gt;&lt;/b&gt;  Organizers:&lt;b&gt; &lt;em&gt;Negar Rostamzadeh, Emily Denton, Linda Petrini&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://visualqa.org/workshop"&gt;Visual Question Answering Workshop&lt;/a&gt;&lt;br/&gt;  Invited Speaker: &lt;b&gt;&lt;em&gt;Vittorio Ferrari&lt;/em&gt;&lt;/b&gt;      &lt;/p&gt;&lt;p&gt;&lt;a href="https://workshop2021.isic-archive.com/"&gt;Sixth International Skin Imaging Collaboration (ISIC) Workshop on Skin Image Analysis&lt;/a&gt;&lt;br/&gt;  Invited Speakers: &lt;b&gt;&lt;em&gt;Sandra Avila&lt;/em&gt;&lt;/b&gt;  Organizers:&lt;b&gt; &lt;em&gt;Yuan Liu&lt;/em&gt;&lt;/b&gt;  Steering Committee:&lt;b&gt; &lt;em&gt;Yuan Liu, Dale Webster&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="http://cvpr2021.ug2challenge.org/"&gt;The 4th Workshop and Prize Challenge: Bridging the Gap between Computational Photography and Visual Recognition (UG2+) in Conjunction with IEEE CVPR 2021&lt;/a&gt;&lt;br/&gt;  Invited Speakers:&lt;b&gt; &lt;em&gt;Peyman Milanfar, Chelsea Finn&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://scene-understanding.com/"&gt;The 3rd CVPR Workshop on 3D Scene Understanding for Vision, Graphics, and Robotics&lt;/a&gt;&lt;br/&gt;  Invited Speaker:&lt;b&gt; &lt;em&gt;Andrea Tagliasacchi&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="http://eval.vision.rwth-aachen.de/rvsu-workshop21/"&gt;Robust Video Scene Understanding: Tracking and Video Segmentation&lt;/a&gt;&lt;br/&gt;  Organizers:&lt;b&gt; &lt;em&gt;Jordi Pont-Tuset, Sergi Caelles, Jack Valmadre, Alex Bewley&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="http://cvpr2021.ug2challenge.org/"&gt;4th Workshop and Challenge on Learned Image Compression&lt;/a&gt;&lt;br/&gt;  Invited Speaker:&lt;b&gt; &lt;em&gt;Rianne van den Berg&lt;/em&gt;&lt;/b&gt;  Organizers: &lt;b&gt;&lt;em&gt;George Toderici, Lucas Theis, Johannes Ballé, Eirikur Agustsson, Nick Johnston, Fabian Mentzer&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://sites.google.com/corp/view/ieeecvf-cvpr2021-precognition/"&gt;The Third Workshop on Precognition: Seeing Through the Future&lt;/a&gt;&lt;br/&gt;  Invited Speaker:&lt;b&gt; &lt;em&gt;Anelia Angelova&lt;/em&gt;&lt;br&gt;&lt;/b&gt;Organizers: &lt;b&gt;&lt;em&gt;Utsav Prabhu&lt;/em&gt;&lt;/b&gt;  Program Committee:&lt;b&gt; &lt;em&gt;Chen Sun, David Ross&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://visual.ee.ucla.edu/ccd2021.htm/"&gt;Computational Cameras and Displays&lt;/a&gt;&lt;br/&gt;  Organizers: &lt;b&gt;&lt;em&gt;Tali Dekel&lt;/em&gt;&lt;/b&gt;  Keynote Talks: &lt;b&gt;&lt;em&gt;Paul Debevec&lt;/em&gt;&lt;/b&gt;  Program Committee:&lt;b&gt; &lt;em&gt;Ayan Chakrabarti, Tali Dekel&lt;/em&gt; &lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://embodied-ai.org/"&gt;2nd Embodied AI Workshop&lt;/a&gt;&lt;br/&gt;  Organizing Committee:&lt;b&gt; &lt;em&gt;Anthony Francis&lt;/em&gt;&lt;/b&gt;  Challenge Organizers:&lt;b&gt; &lt;em&gt;Peter Anderson, Anthony Francis, Alex Ku, Alexander Toshev&lt;/em&gt;&lt;/b&gt;  Scientific Advisory Board:&lt;b&gt; &lt;em&gt;Alexander Toshev&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://sites.google.com/corp/view/rcv-cvpr2021/home?authuser=0"&gt;Responsible Computer Vision&lt;/a&gt;&lt;br/&gt;  Program Committee: &lt;b&gt;&lt;em&gt;Caroline Pantofaru, Utsav Prabhu, Susanna Ricco, Negar Rostamzadeh, Candice Schumann&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://sites.google.com/corp/view/cvpr2021-dnetcv/home?authuser=0"&gt;Dynamic Neural Networks Meets Computer Vision&lt;/a&gt;&lt;br/&gt;  Invited Speaker: &lt;b&gt;&lt;em&gt;Azalia Mirhoseini&lt;/em&gt;&lt;/b&gt;      &lt;/p&gt;&lt;p&gt;&lt;a href="http://cmmc-cvpr21.com/"&gt;Interactive Workshop on Bridging the Gap between Subjective and Computational Measurements of Machine Creativity&lt;/a&gt;&lt;br/&gt;  Invited Speaker:&lt;b&gt; &lt;em&gt;David Bau&lt;/em&gt;&lt;/b&gt;      &lt;/p&gt;&lt;p&gt;&lt;a href="https://gazeworkshop.github.io/2021/"&gt;GAZE 2021: The 3rd International Workshop on Gaze Estimation and Prediction in the Wild&lt;/a&gt;&lt;br/&gt;  Organizer: &lt;b&gt;&lt;em&gt;Thabo Beeler&lt;/em&gt;&lt;/b&gt;  Program Committee:&lt;b&gt; &lt;em&gt;Thabo Beeler&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="http://sightsound.org/"&gt;Sight and Sound&lt;/a&gt;&lt;br/&gt;  Organizers:&lt;b&gt; &lt;em&gt;William Freeman&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://visualai.princeton.edu/fcvd/"&gt;Future of Computer Vision Datasets&lt;/a&gt;&lt;br/&gt;  Invited Speaker:&lt;b&gt; &lt;em&gt;Emily Denton, Caroline Pantofaru&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="http://www.cs.cmu.edu/~shuk/open-world-vision.html"&gt;Open World Vision&lt;/a&gt;&lt;br/&gt;  Invited Speakers:&lt;b&gt; &lt;em&gt;Rahul Sukthankar&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://sites.google.com/corp/view/luv2021/home?authuser=0"&gt;The 3rd Workshop on Learning from Unlabeled Videos&lt;/a&gt;&lt;br/&gt;  Organizers:&lt;b&gt; &lt;em&gt;Anelia Angelova, Honglak Lee&lt;/em&gt;&lt;/b&gt;  Program Committee:&lt;b&gt; &lt;em&gt;AJ Piergiovanni&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="http://www.cis.rit.edu/~glpci/vocvalc2021/"&gt;4th International Workshop on Visual Odometry and Computer Vision Applications Based on Location Clues — With a Focus on Mobile Platform Applications&lt;/a&gt;&lt;br/&gt;  Organizers: &lt;b&gt;&lt;em&gt;Anelia Angelova&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://sites.google.com/corp/view/ecv2021/home?authuser=0"&gt;4th Workshop on Efficient Deep Learning for Computer Vision&lt;/a&gt;&lt;br/&gt;  Invited Speaker:&lt;b&gt; &lt;em&gt;Andrew Howard&lt;/em&gt;&lt;br&gt;&lt;/b&gt;Organizers: &lt;b&gt;&lt;em&gt;Pete Warden, Andrew Howard&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://holistic-video-understanding.github.io/workshops/cvpr2021.html"&gt;Second International Workshop on Large Scale Holistic  Video Understanding&lt;/a&gt;&lt;br/&gt;  Invited Speaker: &lt;b&gt;&lt;em&gt;Cordelia Schmid&lt;/em&gt;&lt;/b&gt;  Program Committee: &lt;b&gt;&lt;em&gt;AJ Piergiovanni&lt;/em&gt;&lt;/b&gt;  Organizers:&lt;b&gt; &lt;em&gt;David Ross&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://cvpr21-nas.com/program"&gt;Neural Architecture Search 1st Lightweight NAS Challenge and Moving Beyond&lt;/a&gt;&lt;br/&gt;  Invited Speakers: &lt;b&gt;&lt;em&gt;Sara Sabour&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://fadetrcv.github.io/2021/"&gt;The Second Workshop on Fair, Data-Efficient, and Trusted Computer Vision&lt;/a&gt;&lt;br/&gt;  Invited Speakers: &lt;b&gt;&lt;em&gt;Gaurav Aggarwal&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://embeddedvisionworkshop.wordpress.com/"&gt;The 17th Embedded Vision Workshop&lt;/a&gt;&lt;br/&gt;  General Chair:&lt;b&gt; &lt;em&gt;Anelia Angelova&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://sites.google.com/corp/view/fgvc8/home?authuser=0"&gt;8th Workshop on Fine-Grained Visual Categorization&lt;/a&gt;&lt;br/&gt;  Organizers:&lt;b&gt; &lt;em&gt;Christine Kaeser-Chen, Kimberly Wilber&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://visual.cs.brown.edu/aicc2021"&gt;AI for Content Creation&lt;/a&gt;&lt;br/&gt;  Invited Speaker: &lt;b&gt;&lt;em&gt;Tali Dekel, Jon Barron, Emily Denton&lt;/em&gt;&lt;/b&gt;  Organizers: &lt;b&gt;&lt;em&gt;Deqing Sun&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://sites.google.com/corp/view/mono3d-workshop"&gt;Frontiers of Monocular 3D Perception&lt;/a&gt;&lt;br/&gt;  Invited Speakers: &lt;b&gt;&lt;em&gt;Anelia Angelova, Cordelia Schmid, Noah Snavely&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://sites.google.com/corp/view/beyond-fairness-cv/home?authuser=0"&gt;Beyond Fairness: Towards a Just, Equitable, and Accountable Computer Vision&lt;/a&gt;&lt;br/&gt;  Organizers:&lt;b&gt; &lt;em&gt;Emily Denton&lt;/em&gt;&lt;/b&gt;    &lt;/p&gt;&lt;p&gt;&lt;a href="https://fvc-workshop.github.io/index.html"&gt;The 1st Workshop on Future Video Conferencing&lt;/a&gt;&lt;br/&gt;  Invited Speakers:&lt;b&gt; &lt;em&gt;Chuo-Ling Chang, Sergi Caelles&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;span style="text-decoration:underline;"&gt;Tutorials&lt;/span&gt;&lt;/b&gt; &lt;em&gt;(only Google affiliations are noted)&lt;/em&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://sites.google.com/corp/view/fatecv-tutorial-2021/home?authuser=0"&gt;Tutorial on Fairness Accountability Transparency and Ethics in Computer Vision&lt;/a&gt;&lt;br/&gt;  Organizer:&lt;b&gt;&lt;em&gt; Emily Denton&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://vita-group.github.io/cvpr_2021_data_efficient_tutorial.html"&gt;Data-Efficient Learning in An Imperfect World&lt;/a&gt;&lt;br/&gt;  Organizers: &lt;b&gt;&lt;em&gt;Boqing Gong, Ting Chen&lt;/em&gt;&lt;/b&gt;      &lt;/p&gt;&lt;p&gt;&lt;a href="https://d2ch.dii.univpm.it/#A_Home"&gt;Semantic Segmentation of Point Clouds: a Deep Learning Framework for Cultural Heritage&lt;/a&gt;&lt;br/&gt;  Invited Speaker:&lt;b&gt;&lt;em&gt; Manzil Zaheer&lt;/em&gt;&lt;/b&gt;  &lt;/p&gt;&lt;p&gt;&lt;a href="https://vqa2vln-tutorial.github.io/"&gt;From VQA to VLN: Recent Advances in Vision-and-Language Research&lt;/a&gt;&lt;br/&gt;	   Organizer: &lt;b&gt;&lt;em&gt;Peter Anderson&lt;/em&gt;&lt;/b&gt; &lt;/p&gt;&lt;p&gt;  &lt;em&gt;* Indicates work done while at Google&lt;/em&gt;&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/blogspot/gJZg?a=TrNMZxLRY78:qaWq24C_WpI:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/blogspot/gJZg/~4/TrNMZxLRY78" height="1" width="1" alt=""/&gt;</content>
    <link rel="replies" type="application/atom+xml" href="http://ai.googleblog.com/feeds/7776510198374933258/comments/default" title="Post Comments"/>
    <link rel="replies" type="text/html" href="http://ai.googleblog.com/2021/06/google-at-cvpr-2021.html#comment-form" title="0 Comments"/>
    <link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/7776510198374933258"/>
    <link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/7776510198374933258"/>
    <link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/blogspot/gJZg/~3/TrNMZxLRY78/google-at-cvpr-2021.html" title="Google at CVPR 2021"/>
    <author>
      <name>Google AI</name>
      <uri>http://www.blogger.com/profile/12098626514775266161</uri>
      <email>noreply@blogger.com</email>
      <gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="16" height="16" src="https://img1.blogblog.com/img/b16-rounded.gif"/>
    </author>
    <thr:total>0</thr:total>
    <feedburner:origLink>http://ai.googleblog.com/2021/06/google-at-cvpr-2021.html</feedburner:origLink>
  </entry>
  <entry>
    <id>tag:blogger.com,1999:blog-8474926331452026626.post-9110431088752752802</id>
    <published>2021-06-16T08:22:00.006-07:00</published>
    <updated>2021-06-18T10:04:37.998-07:00</updated>
    <category scheme="http://www.blogger.com/atom/ns#" term="Google Brain"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="Machine Learning"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="Robotics"/>
    <title type="text">Learning an Accurate Physics Simulator via Adversarial Reinforcement Learning</title>
    <content type="html">&lt;span class="byline-author"&gt;Yifeng Jiang, Research Intern and Jie Tan, Research Scientist, Robotics at Google&lt;/span&gt; &lt;p&gt;Simulation empowers various engineering disciplines to quickly prototype with minimal human effort. In robotics, physics &lt;a href="https://ai.googleblog.com/2021/04/model-based-rl-for-decentralized-multi.html"&gt;simulations provide&lt;/a&gt; a safe and inexpensive virtual playground for robots to acquire &lt;a href="https://ai.googleblog.com/2020/04/exploring-nature-inspired-robot-agility.html"&gt;physical skills&lt;/a&gt; with techniques such as &lt;a href="https://en.wikipedia.org/wiki/Deep_reinforcement_learning"&gt;deep reinforcement learning&lt;/a&gt; (DRL). However, as the hand-derived physics in simulations does not match the real world exactly, control policies trained entirely within simulation can fail when tested on real hardware — a challenge known as the &lt;a href="https://ai.googleblog.com/2017/10/closing-simulation-to-reality-gap-for.html"&gt;sim-to-real&lt;/a&gt; gap or the domain adaptation problem. The sim-to-real gap for perception-based tasks (such as grasping) has been tackled using &lt;a href="https://ai.googleblog.com/2021/06/toward-generalized-sim-to-real-transfer.html"&gt;RL-CycleGAN and RetinaGAN&lt;/a&gt;, but there is still a gap caused by the dynamics of robotic systems. This prompts us to ask, can we learn a more accurate physics simulator from a handful of real robot trajectories? If so, such an improved simulator could be used to refine the robot controller using standard DRL training, so that it succeeds in the real world. &lt;/p&gt;&lt;p&gt;In our &lt;a href="https://www.ieee-icra.org/"&gt;ICRA 2021&lt;/a&gt; publication “&lt;a href="https://arxiv.org/abs/2101.06005"&gt;SimGAN: Hybrid Simulator Identification for Domain Adaptation via Adversarial Reinforcement Learning&lt;/a&gt;”, we propose to treat the physics simulator as a learnable component that is trained by DRL with a special reward function that penalizes discrepancies between the trajectories (i.e., the movement of the robots over time) generated in simulation and a small number of trajectories that are collected on real robots. We use &lt;a href="https://en.wikipedia.org/wiki/Generative_adversarial_network"&gt;generative adversarial networks&lt;/a&gt; (GANs) to provide such a reward, and formulate a hybrid simulator that combines learnable neural networks and analytical physics equations, to balance model expressiveness and physical correctness. On robotic locomotion tasks, our method outperforms multiple strong baselines, including &lt;a href="https://arxiv.org/abs/1703.06907"&gt;domain randomization&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;&lt;b&gt;A Learnable Hybrid Simulator &lt;/b&gt;&lt;br/&gt;A traditional physics simulator is a program that solves &lt;a href="https://en.wikipedia.org/wiki/Rigid_body_dynamics"&gt;differential equations&lt;/a&gt; to simulate the movement or interactions of objects in a virtual world. For this work, it is necessary to build different physical models to represent different environments – if a robot walks on a mattress, the deformation of the mattress needs to be taken into account (e.g., with the &lt;a href="https://en.wikipedia.org/wiki/Finite_element_method"&gt;finite element method&lt;/a&gt;). However, due to the diversity of the scenarios that robots could encounter in the real world, it would be tedious (or even impossible) for such environment-specific modeling techniques, which is why it is useful to instead take an approach based on machine learning. Although simulators &lt;em&gt;can&lt;/em&gt; be learned entirely &lt;a href="https://arxiv.org/abs/1612.00222"&gt;from&lt;/a&gt; &lt;a href="https://arxiv.org/abs/2002.09405"&gt;data&lt;/a&gt;, if the training data does not include a wide enough variety of situations, the learned simulator might violate the laws of physics (i.e., deviate from the real-world dynamics) if it needs to simulate situations for which it was not trained. As a result, the robot that is trained in such a limited simulator is more likely to fail in the real world. &lt;/p&gt;&lt;p&gt;To overcome this complication, we construct a hybrid simulator that combines both learnable neural networks and physics equations. Specifically, we replace what are often manually-defined simulator parameters — contact parameters (e.g., friction and &lt;a href="https://en.wikipedia.org/wiki/Coefficient_of_restitution"&gt;restitution coefficients&lt;/a&gt;) and motor parameters (e.g., motor gains) — with a learnable &lt;em&gt;simulation parameter function&lt;/em&gt; because the unmodeled details of contact and motor dynamics are major causes of the sim-to-real gap. Unlike conventional simulators in which these parameters are treated as constants, in the hybrid simulator they are state-dependent — they can change according to the state of the robot. For example, motors can become weaker at higher speed. These typically unmodeled physical phenomena can be captured using the state-dependent simulation parameter functions. Moreover, while contact and motor parameters are usually difficult to identify and subject to change due to wear-and-tear, our hybrid simulator can learn them automatically from data. For example, rather than having to manually specify the parameters of a robot’s foot against every possible surface it might contact, the simulation learns these parameters from training data.  &lt;/p&gt;   &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-4LgSX2uc-AM/YMoSG_Hh5-I/AAAAAAAAHvE/kym9kas4M7sWVRcIi1J6bSEAA6tLSlf_QCLcBGAsYHQ/s1999/image3.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="514" data-original-width="1999" height="164" src="https://1.bp.blogspot.com/-4LgSX2uc-AM/YMoSG_Hh5-I/AAAAAAAAHvE/kym9kas4M7sWVRcIi1J6bSEAA6tLSlf_QCLcBGAsYHQ/w640-h164/image3.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Comparison between a conventional simulator and our hybrid simulator.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;p&gt;The other part of the hybrid simulator is made up of physics equations that ensure the simulation obeys fundamental laws of physics, such as &lt;a href="https://en.wikipedia.org/wiki/Conservation_of_energy"&gt;conservation of energy&lt;/a&gt;, making it a closer approximation to the real world and thus reducing the sim-to-real gap. &lt;/p&gt;&lt;p&gt;In our earlier mattress example, the learnable hybrid simulator is able to mimic the contact forces from the mattress. Because the learned contact parameters are state-dependent, the simulator can modulate contact forces based on the distance and velocity of the robot’s feet relative to the mattress, mimicking the effect of the stiffness and damping of a deformable surface. As a result, we do not need to analytically devise a model specifically for deformable surfaces. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Using GANs for Simulator Learning&lt;/b&gt;&lt;br/&gt;Successfully learning the simulation parameter functions discussed above would result in a hybrid simulator that can generate similar trajectories to the ones collected on the real robot. The key that enables this learning is defining a metric for the similarity between trajectories. &lt;a href="https://en.wikipedia.org/wiki/Generative_adversarial_network"&gt;GANs&lt;/a&gt;, initially designed to generate synthetic &lt;em&gt;images&lt;/em&gt; that share the same distribution, or “style,” with a small number of real images, can be used to generate synthetic &lt;em&gt;trajectories&lt;/em&gt; that are indistinguishable from real ones. GANs have two main parts, a &lt;em&gt;generator&lt;/em&gt; that learns to generate new instances, and a &lt;em&gt;discriminator &lt;/em&gt;that evaluates how similar the new instances are to the training data. In this case, the learnable hybrid simulator serves as the GAN generator, while the GAN discriminator provides the similarity scores. &lt;/p&gt;   &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-0n1zEbAJA0I/YMoSSfbErII/AAAAAAAAHvI/ZO1h0eDuryEEzOaiOgRKHWtm1JrNnYOEACLcBGAsYHQ/s984/image2.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="650" data-original-width="984" height="422" src="https://1.bp.blogspot.com/-0n1zEbAJA0I/YMoSSfbErII/AAAAAAAAHvI/ZO1h0eDuryEEzOaiOgRKHWtm1JrNnYOEACLcBGAsYHQ/w640-h422/image2.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;The GAN discriminator provides the similarity metric that compares the movements of the simulated and the real robot.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;Fitting parameters of simulation models to data collected in the real world, a process called &lt;a href="https://en.wikipedia.org/wiki/System_identification"&gt;system identification&lt;/a&gt; (SysID), has been a common practice in many engineering fields. For example, the stiffness parameter of a deformable surface can be identified by measuring the displacements of the surface under different pressures. This process is typically manual and tedious, but using GANs can be much more efficient. For example, SysID often requires a hand-crafted metric for the discrepancy between simulated and real trajectories. With GANs, such a metric is automatically learned by the discriminator. Furthermore, to calculate the discrepancy metric, conventional SysID requires pairing each simulated trajectory to a corresponding real-world one that is generated using the same control policy. Since the GAN discriminator takes only one trajectory as the input and calculates the likelihood that it is collected in the real world, this one-to-one pairing is not needed.  &lt;/p&gt;&lt;p&gt;&lt;b&gt;Using Reinforcement Learning (RL) to Learn the Simulator and Refine the Policy&lt;/b&gt;&lt;br/&gt;Putting everything together, we formulate simulation learning as an RL problem. A neural network learns the state-dependent contact and motor parameters from a small number of real-world trajectories. The neural network is optimized to minimize the error between the simulated and the real trajectories. Note that it is important to minimize this error over an extended period of time — a simulation that accurately predicts a more distant future will lead to a better control policy. RL is well suited to this because it optimizes the accumulated reward over time, rather than just optimizing a single-step reward.  &lt;/p&gt;&lt;p&gt;After the hybrid simulator is learned and becomes more accurate, we use RL again to refine the robot’s control policy within the simulation (e.g., walking across a surface, shown below). &lt;/p&gt;   &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-oUUzJtkFMbE/YMoSYv99SBI/AAAAAAAAHvQ/vn0KAq6693ozrGcwBL5n-7hcSkI2cXhZgCLcBGAsYHQ/s1280/image4.gif" imageanchor="1" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="720" data-original-width="1280" height="360" src="https://1.bp.blogspot.com/-oUUzJtkFMbE/YMoSYv99SBI/AAAAAAAAHvQ/vn0KAq6693ozrGcwBL5n-7hcSkI2cXhZgCLcBGAsYHQ/w640-h360/image4.gif" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Following the arrows clockwise: (&lt;strong&gt;upper left&lt;/strong&gt;) recording a small number of robot's failed attempts in the target domain (e.g., a real-world proxy in which the leg in red is modified to be much heavier than the source domain); (&lt;strong&gt;upper right&lt;/strong&gt;) learning the hybrid simulator to match trajectories collected in the target domain; (&lt;strong&gt;lower right&lt;/strong&gt;) refining control policies in this learned simulator; (&lt;strong&gt;lower left&lt;/strong&gt;) testing the refined controller directly in the target domain.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;b&gt;Evaluation&lt;/b&gt;&lt;br/&gt;Due to limited access to real robots during 2020, we created a second and different simulation (target domain) as a proxy of the real-world. The change of dynamics between the source and the target domains are large enough to approximate different sim-to-real gaps (e.g., making one leg heavier, walking on deformable surfaces instead of hard floor). We assessed whether our hybrid simulator, with no knowledge of these changes, could learn to match the dynamics in the target domain, and if the refined policy in this learned simulator could be successfully deployed in the target domain. &lt;/p&gt;&lt;p&gt;Qualitative results below show that simulation learning with less than 10 minutes of data collected in the target domain (where the floor is deformable) is able to generate a refined policy that performs much better for two robots with different morphologies and dynamics.  &lt;/p&gt; &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-JrgnyEsjx3c/YMoTCs9RqzI/AAAAAAAAHvg/aK9zcXzJ51cit4CwCkdq26GR3bkoq2JMgCLcBGAsYHQ/s720/image5.gif" imageanchor="1" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="720" data-original-width="720" height="400" src="https://1.bp.blogspot.com/-JrgnyEsjx3c/YMoTCs9RqzI/AAAAAAAAHvg/aK9zcXzJ51cit4CwCkdq26GR3bkoq2JMgCLcBGAsYHQ/w400-h400/image5.gif" width="400" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Comparison of performance between the initial and refined policy in the target domain (deformable floor) for the hopper and the quadruped robot.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Quantitative results below show that SimGAN outperforms multiple state-of-the-art baselines, including domain randomization (DR) and direct finetuning in target domains (FT). &lt;/p&gt; &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-PL3obTqri2k/YMoS0QbMD8I/AAAAAAAAHvc/IVr86c1h1iAHuv17gazUnJpAVXGaXKvXgCLcBGAsYHQ/s640/image1.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="480" data-original-width="640" height="480" src="https://1.bp.blogspot.com/-PL3obTqri2k/YMoS0QbMD8I/AAAAAAAAHvc/IVr86c1h1iAHuv17gazUnJpAVXGaXKvXgCLcBGAsYHQ/w640-h480/image1.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Comparison of policy performance using different sim-to-real transfer methods in three different target domains for the Quadruped robot: locomotion on deformable surface, with weakened motors, and with heavier bodies.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;p&gt;&lt;b&gt;Conclusion&lt;/b&gt;&lt;br/&gt;The sim-to-real gap is one of the key bottlenecks that prevents robots from tapping into the power of reinforcement learning. We tackle this challenge by learning a simulator that can more faithfully model real-world dynamics, while using only a small amount of real-world data. The control policy that is refined in this simulator can be successfully deployed. To achieve this, we augment a classical physics simulator with learnable components, and train this hybrid simulator using adversarial reinforcement learning. To date we have tested its application to locomotion tasks, we hope to build on this general framework by applying it to other robot learning tasks, such as navigation and manipulation. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Acknowledgements&lt;/b&gt;&lt;br&gt;&lt;em&gt;We'd like to thank our paper co-authors: Tingnan Zhang, Daniel Ho, Yunfei Bai, C. Karen Liu, and Sergey Levine. We would also like to thank the team members of Robotics at Google for discussions and feedback.&lt;/em&gt;&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/blogspot/gJZg?a=B3nT7GSYcCE:l7TBzRX88Os:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/blogspot/gJZg/~4/B3nT7GSYcCE" height="1" width="1" alt=""/&gt;</content>
    <link rel="replies" type="application/atom+xml" href="http://ai.googleblog.com/feeds/9110431088752752802/comments/default" title="Post Comments"/>
    <link rel="replies" type="text/html" href="http://ai.googleblog.com/2021/06/learning-accurate-physics-simulator-via.html#comment-form" title="0 Comments"/>
    <link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/9110431088752752802"/>
    <link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/9110431088752752802"/>
    <link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/blogspot/gJZg/~3/B3nT7GSYcCE/learning-accurate-physics-simulator-via.html" title="Learning an Accurate Physics Simulator via Adversarial Reinforcement Learning"/>
    <author>
      <name>Google AI</name>
      <uri>http://www.blogger.com/profile/12098626514775266161</uri>
      <email>noreply@blogger.com</email>
      <gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="16" height="16" src="https://img1.blogblog.com/img/b16-rounded.gif"/>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://1.bp.blogspot.com/-4LgSX2uc-AM/YMoSG_Hh5-I/AAAAAAAAHvE/kym9kas4M7sWVRcIi1J6bSEAA6tLSlf_QCLcBGAsYHQ/s72-w640-h164-c/image3.png" height="72" width="72"/>
    <thr:total>0</thr:total>
    <feedburner:origLink>http://ai.googleblog.com/2021/06/learning-accurate-physics-simulator-via.html</feedburner:origLink>
  </entry>
  <entry>
    <id>tag:blogger.com,1999:blog-8474926331452026626.post-7585526402741935248</id>
    <published>2021-06-15T12:09:00.000-07:00</published>
    <updated>2021-06-15T12:09:44.290-07:00</updated>
    <category scheme="http://www.blogger.com/atom/ns#" term="Computer Vision"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="ML Fairness"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="Research"/>
    <title type="text">A Step Toward More Inclusive People Annotations in the Open Images Extended Dataset</title>
    <content type="html">&lt;span class="byline-author"&gt;Posted by Candice Schumann and Susanna Ricco, Software Engineers, Google Research&lt;/span&gt; &lt;p&gt;In 2016, &lt;a href="https://ai.googleblog.com/2016/09/introducing-open-images-dataset.html"&gt;we introduced Open Images&lt;/a&gt;, a collaborative release of ~9 million images annotated with image labels spanning thousands of object categories and bounding box annotations for 600 classes. Since then, we have made several &lt;a href="https://ai.googleblog.com/2019/05/announcing-open-images-v5-and-iccv-2019.html"&gt;updates&lt;/a&gt;, including the release of &lt;a href="https://ai.googleblog.com/2018/12/adding-diversity-to-images-with-open.html"&gt;crowdsourced data&lt;/a&gt; to the &lt;a href="https://storage.googleapis.com/openimages/web/extended.html"&gt;Open Images Extended&lt;/a&gt; collection to improve diversity of object annotations. While the labels provided with these datasets were expansive, they did not focus on sensitive attributes for people, which are critically important for many machine learning (ML) &lt;a href="https://developers.google.com/machine-learning/crash-course/fairness/video-lecture"&gt;fairness tasks&lt;/a&gt;, such as &lt;a href="https://ai.googleblog.com/2019/12/fairness-indicators-scalable.html"&gt;fairness evaluations&lt;/a&gt; and &lt;a href="https://ai.googleblog.com/2020/11/mitigating-unfair-bias-in-ml-models.html"&gt;bias mitigation&lt;/a&gt;. In fact, finding datasets that include thorough labeling of such sensitive attributes is difficult, particularly in the domain of computer vision. &lt;/p&gt;&lt;p&gt;Today, we introduce the &lt;a href="https://storage.googleapis.com/openimages/web/extended.html"&gt;More Inclusive Annotations for People&lt;/a&gt; (MIAP) dataset in the Open Images Extended collection. The collection contains more complete bounding box annotations for the &lt;em&gt;person&lt;/em&gt; class hierarchy in 100k images containing people. Each annotation is also labeled with fairness-related attributes, including perceived gender presentation and perceived age range. With the increasing focus on reducing unfair bias as part of &lt;a href="https://ai.google/principles/"&gt;responsible AI research&lt;/a&gt;, we hope these annotations will encourage researchers already leveraging Open Images to incorporate fairness analysis in their research. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-00mP_T5vagw/YMjqRmHHFOI/AAAAAAAAHus/vP3VUsq4-o4kZ4l6Z1odHPrTILz-Hh7_QCLcBGAsYHQ/s1534/OpenImages.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="399" data-original-width="1534" height="166" src="https://1.bp.blogspot.com/-00mP_T5vagw/YMjqRmHHFOI/AAAAAAAAHus/vP3VUsq4-o4kZ4l6Z1odHPrTILz-Hh7_QCLcBGAsYHQ/w640-h166/OpenImages.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Examples of new boxes in MIAP. In each subfigure the magenta boxes are from the original Open Images dataset, while the yellow boxes are additional boxes added by the MIAP Dataset. Original photo credits — &lt;strong&gt;left:&lt;/strong&gt; &lt;a href="https://www.flickr.com/photos/boston_public_library/8242010414/"&gt;Boston Public Library&lt;/a&gt;; &lt;strong&gt;middle:&lt;/strong&gt; &lt;a href="https://www.flickr.com/photos/jenrobinson/20183915655/"&gt;jen robinson&lt;/a&gt;; &lt;strong&gt;right:&lt;/strong&gt; &lt;a href="https://www.flickr.com/photos/mrgarin/2484859086/"&gt;Garin Fons&lt;/a&gt;; all used with permission under the &lt;a href="https://creativecommons.org/licenses/by/2.0/"&gt;CC- BY 2.0 license&lt;/a&gt;.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;b&gt;Annotations in Open Images&lt;/b&gt;&lt;br&gt;Each image in the original Open Images dataset contains &lt;a href="https://cloud.google.com/vision/docs/labels"&gt;image-level annotations&lt;/a&gt; that broadly describe the image and &lt;a href="https://cloud.google.com/vision/docs/object-localizer"&gt;bounding boxes&lt;/a&gt; drawn around specific objects. To avoid drawing multiple boxes around the same object, &lt;a href="https://storage.googleapis.com/openimages/2018_04/bbox_labels_600_hierarchy_visualizer/circle.html"&gt;less specific classes&lt;/a&gt; were temporarily pruned from the label candidate set, a process that we refer to as &lt;em&gt;hierarchical de-duplication&lt;/em&gt;. For example, an image with labels &lt;em&gt;animal&lt;/em&gt;, &lt;em&gt;cat&lt;/em&gt;, and &lt;em&gt;washing machine&lt;/em&gt; has &lt;a href="https://storage.googleapis.com/openimages/web/visualizer/index.html?set=train&amp;amp;type=detection&amp;amp;c=%2Fm%2F01yrx"&gt;bounding boxes annotated&lt;/a&gt; for &lt;em&gt;cat&lt;/em&gt; and &lt;em&gt;washing machine&lt;/em&gt;, but not for the redundant class &lt;em&gt;animal&lt;/em&gt;.  &lt;/p&gt;&lt;p&gt;The MIAP dataset addresses the five classes that are part of the &lt;em&gt;person&lt;/em&gt; hierarchy in the original Open Images dataset: &lt;em&gt;person&lt;/em&gt;, &lt;em&gt;man&lt;/em&gt;, &lt;em&gt;woman&lt;/em&gt;, &lt;em&gt;boy&lt;/em&gt;, &lt;em&gt;girl&lt;/em&gt;. The existence of these labels make the Open Images dataset uniquely valuable for research advancing &lt;a href="https://ai.google/responsibilities/responsible-ai-practices/"&gt;responsible AI&lt;/a&gt;, allowing one to train a general &lt;a href="https://cloud.google.com/video-intelligence/docs/people-detection"&gt;person detector&lt;/a&gt; with access to gender- and age-range-specific labels for &lt;a href="https://ai.googleblog.com/2019/12/fairness-indicators-scalable.html"&gt;fairness analysis&lt;/a&gt; and &lt;a href="https://ai.googleblog.com/2020/11/mitigating-unfair-bias-in-ml-models.html"&gt;bias mitigation&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;However, we found that the combination of hierarchical de-duplication and societally imposed distinctions between &lt;em&gt;woman&lt;/em&gt;/&lt;em&gt;girl&lt;/em&gt; and &lt;em&gt;man&lt;/em&gt;/&lt;em&gt;boy&lt;/em&gt; introduced limitations in the original annotations. For example, if annotators were asked to draw boxes for the class &lt;em&gt;girl&lt;/em&gt;, they would not draw a box around a boy in the image. They may or may not draw a box around a woman depending on their assessment of the age of the individual and their cultural understanding of the concept of girl. These decisions could be applied inconsistently between images, depending on the cultural background of the individual annotator, the appearance of an individual, and the context of the scene. Consequently, the bounding box annotations in some images were incomplete, with some people who appeared prominently not being annotated. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Annotations in MIAP&lt;/b&gt;&lt;br&gt;The &lt;a href="https://storage.googleapis.com/openimages/open_images_extended_miap/Open%20Images%20Extended%20-%20MIAP%20-%20Data%20Card.pdf"&gt;new MIAP annotations&lt;/a&gt; are designed to address these limitations and fulfill the promise of Open Images as a dataset that will enable new advances in machine learning fairness research. Rather than asking annotators to draw boxes for the most specific class from the hierarchy (e.g., &lt;em&gt;girl&lt;/em&gt;), we invert the procedure, always requesting bounding boxes for the gender- and age-agnostic &lt;em&gt;person&lt;/em&gt; class. All &lt;em&gt;person&lt;/em&gt; boxes are then separately associated with labels for perceived gender presentation (predominantly feminine, predominantly masculine, or unknown) and age presentation (young, middle, older, or unknown). We recognize that gender is not binary and that an individual's gender identity may not match their perceived or intended gender presentation and, in an effort to mitigate the effects of unconscious bias on the annotations, we reminded annotators that norms around gender expression vary across cultures and have changed over time. &lt;/p&gt;&lt;p&gt;This procedure adds a significant number of boxes that were previously missing.  &lt;/p&gt;&lt;p&gt;Over the 100k images that include people, the number of person bounding boxes have increased from ~358k to ~454k. The number of bounding boxes per perceived gender presentation and perceived age presentation increased consistently. These new annotations provide more complete ground truth for training a person detector as well as more accurate subgroup labels for incorporating fairness into computer vision research. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-XVvQZFD4AJA/YMjqbncefJI/AAAAAAAAHuw/m4rrJcBCwQEqUlEseLzjTgfL-quu5XFQgCLcBGAsYHQ/s448/image5.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="288" data-original-width="448" height="258" src="https://1.bp.blogspot.com/-XVvQZFD4AJA/YMjqbncefJI/AAAAAAAAHuw/m4rrJcBCwQEqUlEseLzjTgfL-quu5XFQgCLcBGAsYHQ/w400-h258/image5.png" width="400" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-axLYpk2VozQ/YMjqfOBM3jI/AAAAAAAAHu0/i_Y8bY_zbHEqUEzavOwoR9kde25dLkIcQCLcBGAsYHQ/s459/image1.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="283" data-original-width="459" height="246" src="https://1.bp.blogspot.com/-axLYpk2VozQ/YMjqfOBM3jI/AAAAAAAAHu0/i_Y8bY_zbHEqUEzavOwoR9kde25dLkIcQCLcBGAsYHQ/w400-h246/image1.png" width="400" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Comparison of number of person bounding boxes between the original Open Images and the new MIAP dataset.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;b&gt;Intended Use&lt;/b&gt;&lt;br&gt;We include annotations for perceived age range and gender presentation for &lt;em&gt;person&lt;/em&gt; bounding boxes because we believe these annotations are necessary to advance the ability to better understand and work to mitigate and eliminate unfair bias or disparate performance across protected subgroups within the field of image understanding. We note that the labels capture the gender and age range presentation as assessed by a third party based on visual cues alone, rather than an individual's self-identified gender or actual age. We do not support or condone building or deploying gender and/or age presentation classifiers trained from these annotations as we believe the &lt;a href="https://dl.acm.org/doi/10.1145/3173574.3173582"&gt;risks&lt;/a&gt; associated with the use of these technologies outside fairness research outweigh any potential benefits. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Acknowledgements&lt;/b&gt;&lt;br&gt;&lt;em&gt;The core team behind this work included Utsav Prabhu, Vittorio Ferrari, and Caroline Pantofaru. We would also like to thank Alex Hanna, Reena Jana, Alina Kuznetsova, Matteo Malloci, Stefano Pellegrini, Jordi Pont-Tuset, and Mahima Pushkarna, for their contributions to the project.&lt;/em&gt;&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/blogspot/gJZg?a=KvfTLJdCXrI:EWeTOLt1Uxw:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/blogspot/gJZg/~4/KvfTLJdCXrI" height="1" width="1" alt=""/&gt;</content>
    <link rel="replies" type="application/atom+xml" href="http://ai.googleblog.com/feeds/7585526402741935248/comments/default" title="Post Comments"/>
    <link rel="replies" type="text/html" href="http://ai.googleblog.com/2021/06/a-step-toward-more-inclusive-people.html#comment-form" title="0 Comments"/>
    <link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/7585526402741935248"/>
    <link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/7585526402741935248"/>
    <link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/blogspot/gJZg/~3/KvfTLJdCXrI/a-step-toward-more-inclusive-people.html" title="A Step Toward More Inclusive People Annotations in the Open Images Extended Dataset"/>
    <author>
      <name>Google AI</name>
      <uri>http://www.blogger.com/profile/12098626514775266161</uri>
      <email>noreply@blogger.com</email>
      <gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="16" height="16" src="https://img1.blogblog.com/img/b16-rounded.gif"/>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://1.bp.blogspot.com/-00mP_T5vagw/YMjqRmHHFOI/AAAAAAAAHus/vP3VUsq4-o4kZ4l6Z1odHPrTILz-Hh7_QCLcBGAsYHQ/s72-w640-h166-c/OpenImages.png" height="72" width="72"/>
    <thr:total>0</thr:total>
    <feedburner:origLink>http://ai.googleblog.com/2021/06/a-step-toward-more-inclusive-people.html</feedburner:origLink>
  </entry>
  <entry>
    <id>tag:blogger.com,1999:blog-8474926331452026626.post-4952801655708792428</id>
    <published>2021-06-10T13:01:00.000-07:00</published>
    <updated>2021-06-10T13:01:44.265-07:00</updated>
    <category scheme="http://www.blogger.com/atom/ns#" term="Google Brain"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="Research"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="Robotics"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="statistics"/>
    <title type="text">The Importance of A/B Testing in Robotics</title>
    <content type="html">&lt;span class="byline-author"&gt;Arnab Bose and Yuheng Kuang, Staff Software Engineers, Robotics at Google&lt;/span&gt; &lt;p&gt;Disciplines in the natural sciences, social sciences, and medicine all have to grapple with &lt;a href="https://en.wikipedia.org/wiki/Design_of_experiments"&gt;how to evaluate and compare results&lt;/a&gt; within the context of the &lt;em&gt;continually changing&lt;/em&gt; real world. In contrast, a significant body of machine learning (ML) research uses a different method that relies on the assumption of a &lt;em&gt;fixed&lt;/em&gt; world: measure the performance of a baseline model on fixed data sets, then build a new model aimed at improving on the baseline, and evaluate its performance (on the same fixed data) by comparing its performance to the baseline.  &lt;/p&gt;&lt;p&gt;Research into robotics systems and their applications to the real world requires a rethinking of this experiment design. Even in controlled robotic lab environments, it is possible that real-world changes cause the baseline model to perform inconsistently over time, making it unclear whether new models’ performance is an improvement compared to the baseline, or just the result of unintentional, random changes in the experiment setup. As robotics research advances into more complex and challenging real-world scenarios, there is a growing need for both understanding the impact of the ever-changing world on baselines and developing systematic methods to generate informative and clear results. &lt;/p&gt;&lt;p&gt;In this post, we demonstrate how robotics research, even in the relatively controlled environment of a lab, is meaningfully affected by changes in the environment, and discuss how to address this fundamental challenge using &lt;a href="https://en.wikipedia.org/wiki/Random_assignment"&gt;random assignment&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/A/B_testing"&gt;A/B testing&lt;/a&gt;. Although these are classical research methods, they are not generally employed by default in robotics research — yet, they are critical to producing meaningful and measurable scientific results for robotics in real-world scenarios. Additionally, we cover the costs, benefits, and other considerations of using these methods.  &lt;/p&gt;&lt;p&gt;&lt;b&gt;The Ever-Changing Real World in Robotics&lt;/b&gt;&lt;br /&gt;Even in a robotics lab environment, which is designed to minimize all changes that are not experimental conditions, it is notoriously difficult to set up a perfectly reproducible experiment. Robots get bumped and are subject to wear and tear, lighting changes affect perception, battery charge influences the torque applied to motors — all things that can affect results in ways large and small. &lt;/p&gt;&lt;p&gt;To illustrate this on real robot data, we collected success rate data on one of our simplest setups —  moving identical foam dice from one bin to another. For this task, we ran about 33k task trials on two robots over more than five months with the same software and ML model, and took the overall success rate of the last two weeks as baseline. We then measured the historic performance over time in this “very well controlled” environment. &lt;/p&gt;  &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-ImVMeEYXlhM/YLqSQ0unv3I/AAAAAAAAHsw/39AZs92RANMdLD337r2RuGa7zvnMEB9IwCLcBGAsYHQ/s640/image3.gif" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="360" data-original-width="640" height="360" src="https://1.bp.blogspot.com/-ImVMeEYXlhM/YLqSQ0unv3I/AAAAAAAAHsw/39AZs92RANMdLD337r2RuGa7zvnMEB9IwCLcBGAsYHQ/w640-h360/image3.gif" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Video of a real robot completing the task: moving identical foam dice from one bin to another.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;p&gt;Given that we did not purposefully change anything during data collection, one would expect the success rate to be statistically similar over time. And yet, this is &lt;i&gt;not&lt;/i&gt; what was observed. &lt;/p&gt;  &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-i4e7-Cqfo6A/YLqSoAnaKqI/AAAAAAAAHs4/RRjqpKVH7Ew_ELHNSguh1lQE6GUWgVzcQCLcBGAsYHQ/s1032/image4.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="597" data-original-width="1032" height="370" src="https://1.bp.blogspot.com/-i4e7-Cqfo6A/YLqSoAnaKqI/AAAAAAAAHs4/RRjqpKVH7Ew_ELHNSguh1lQE6GUWgVzcQCLcBGAsYHQ/w640-h370/image4.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;The y-axis represents the 95% confidence interval of % change in success rate relative to baseline. If the confidence intervals contain zero, that indicates the success rate is statistically similar to the success rate of baseline. Confidence intervals were computed using &lt;a href="https://en.wikipedia.org/wiki/Jackknife_resampling"&gt;Jackknife&lt;/a&gt;, with &lt;a href="https://en.wikipedia.org/wiki/Cochran%E2%80%93Mantel%E2%80%93Haenszel_statistics"&gt;Cochran-Mantel-Haenszel correction&lt;/a&gt; to remove operator bias.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;p&gt;Using the sequential data from the plot above, one might conclude that the model ran during weeks 13-14 performed best and that ran during weeks 9-10 performed the worst. One might also expect most, if not all, of the confidence intervals above to contain 0, but only one did.  Because no changes were made at any time during these trials, this example effectively demonstrates the impact of unintentional, random real-world changes on even very simple setups. It’s also worth noting that having more trials per experiment wouldn’t remove these differences, instead they will more likely produce a narrower confidence interval making the impact more obvious.  &lt;/p&gt;&lt;p&gt;However, what happens when one uses random assignment to compare results, grouping the data randomly rather than sequentially? To answer this, we randomly assigned the above data to the same number of groups for comparison with the baseline. This is equivalent to performing A/B testing where all groups receive the same treatment. &lt;/p&gt;  &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-dH1IIzUK6u4/YLqS4Qj5j8I/AAAAAAAAHtA/7nGIu8QTlnkTGGeaK-6ARLTCRSwDVAbeQCLcBGAsYHQ/s1050/image1.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="623" data-original-width="1050" height="381" src="https://1.bp.blogspot.com/-dH1IIzUK6u4/YLqS4Qj5j8I/AAAAAAAAHtA/7nGIu8QTlnkTGGeaK-6ARLTCRSwDVAbeQCLcBGAsYHQ/w640-h381/image1.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Looking at the chart, we observe that the confidence intervals include zero, indicating success similar to the baseline, as expected.&lt;/p&gt; &lt;p&gt;We performed similar studies with a few other robotics tasks, comparing between sequential and random assignments. They all yielded similar results. &lt;/p&gt;  &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-Qc_Wtyzur1I/YLqTEpXpiRI/AAAAAAAAHtE/oOgmBOrLfpIJoT6rk_g0bloKhZCpt9s7gCLcBGAsYHQ/s1501/image2.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="1116" data-original-width="1501" height="476" src="https://1.bp.blogspot.com/-Qc_Wtyzur1I/YLqTEpXpiRI/AAAAAAAAHtE/oOgmBOrLfpIJoT6rk_g0bloKhZCpt9s7gCLcBGAsYHQ/w640-h476/image2.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;p&gt;We see that even with no intentional changes, there are statistically significant differences observed for sequential assignment, while random assignment shows the expected result of no statistically significant differences. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Considerations for A/B testing in robotics&lt;/b&gt;&lt;br /&gt;While it’s clear based on the above that A/B testing with random assignment is an effective way to control for the unexplainable variance of the real world in robotics, there are some considerations when adopting this approach. Here are several, along with their accompanying pros, cons, and solutions: &lt;/p&gt;&lt;ul&gt; &lt;li&gt;&lt;i&gt;Absolute vs relative performance:&lt;/i&gt; Each experiment needs to be measured against a baseline that is run concurrently. The relative performance metric between baseline and experiment is published with a confidence interval. The absolute performance metric (in baseline or experiment) is less informative, because it depends to an unknown degree on the state of the world when the measurement was taken. However, the &lt;em&gt;statistical differences&lt;/em&gt; we’ve measured between the experiment and baseline are sound and robust to reproduction.  &lt;/li&gt;&lt;li&gt;&lt;i&gt;Data efficiency:&lt;/i&gt;&lt;b&gt; &lt;/b&gt;With this approach, the baseline always needs to run in parallel with the experimental conditions so they can be compared against each other. Although this may seem wasteful, it is worth the cost when compared against the drawbacks of making an invalid inference against a stale baseline. Furthermore, as the number of random assignment experiments scale up, we can use a single baseline arm with multiple simultaneous experiment arms across independent &lt;em&gt;factors&lt;/em&gt; leveraging &lt;a href="https://research.google/pubs/pub36500/"&gt;Google’s overlapping experiment infrastructure&lt;/a&gt;. Data efficiency improves with scale.  &lt;/li&gt;&lt;li&gt;&lt;i&gt;Environmental biases:&lt;/i&gt; If there’s any external factor affecting performance overall (lighting, slicker surfaces, etc.), both the baseline and all experiment arms will encounter this factor with similar probability, so its effect will cancel if there’s no relative impact.  If there is a correlation between environmental factors and experiment arms, this will show up as differences over time (each environmental factor accumulates in the episodes collected). This can substantially reduce or eliminate the need for effortful environmental resets, and lets us run lifelong experiments and still measure improvements across experimental arms.  &lt;/li&gt;&lt;li&gt;&lt;i&gt;Human biases:&lt;/i&gt; One advantage of random assignment is a reduction in biases introduced by humans. Since human operators cannot know which data sample gets routed to which arm of the experiment, it is harder to have biased experimenters influence any particular outcome. &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;The Path Forward&lt;/b&gt;&lt;br /&gt;The A/B testing experiment framework has been successfully used for a long time in many scientific disciplines to measure performance against changing, unpredictable real-world environments. In this blog post, we show that robotics research can benefit from using this same methodology: it improves the quality and confidence of research results, and avoids the impossible task of perfectly controlling all elements of a fundamentally changing environment. Doing this well requires infrastructure to continuously operate robots, collect data, and tools to make the statistical framework easily accessible to researchers. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Acknowledgements&lt;/b&gt;&lt;br /&gt;&lt;em&gt;Arnab Bose, Tuna Toksoz, Yuheng Kuang, Anthony Brohan, Razvan Sudulescu developed the experiment infrastructure and conducted the research.  Matthieu Devin suggested the A/A analysis to showcase the differences using existing data.  Special thanks to Bill Heavlin, Chris Harris, Vincent Vanhoucke who provided invaluable feedback and support to the work.&lt;/em&gt;&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/blogspot/gJZg?a=9N11pOvMNr0:E8Y-gVAz4vM:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/blogspot/gJZg/~4/9N11pOvMNr0" height="1" width="1" alt=""/&gt;</content>
    <link rel="replies" type="application/atom+xml" href="http://ai.googleblog.com/feeds/4952801655708792428/comments/default" title="Post Comments"/>
    <link rel="replies" type="text/html" href="http://ai.googleblog.com/2021/06/the-importance-of-ab-testing-in-robotics.html#comment-form" title="0 Comments"/>
    <link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/4952801655708792428"/>
    <link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/4952801655708792428"/>
    <link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/blogspot/gJZg/~3/9N11pOvMNr0/the-importance-of-ab-testing-in-robotics.html" title="The Importance of A/B Testing in Robotics"/>
    <author>
      <name>Google AI</name>
      <uri>http://www.blogger.com/profile/12098626514775266161</uri>
      <email>noreply@blogger.com</email>
      <gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="16" height="16" src="https://img1.blogblog.com/img/b16-rounded.gif"/>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://1.bp.blogspot.com/-ImVMeEYXlhM/YLqSQ0unv3I/AAAAAAAAHsw/39AZs92RANMdLD337r2RuGa7zvnMEB9IwCLcBGAsYHQ/s72-w640-h360-c/image3.gif" height="72" width="72"/>
    <thr:total>0</thr:total>
    <feedburner:origLink>http://ai.googleblog.com/2021/06/the-importance-of-ab-testing-in-robotics.html</feedburner:origLink>
  </entry>
  <entry>
    <id>tag:blogger.com,1999:blog-8474926331452026626.post-24422599744309840</id>
    <published>2021-06-10T10:03:00.004-07:00</published>
    <updated>2021-06-15T15:47:04.388-07:00</updated>
    <category scheme="http://www.blogger.com/atom/ns#" term="On-device Learning"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="Self-Supervised Learning"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="Speech"/>
    <title type="text">FRILL: On-Device Speech Representations using TensorFlow-Lite</title>
    <content type="html">&lt;span class="byline-author"&gt;Posted by Joel Shor, Software Engineer, Google Research, Tokyo and Sachin Joglekar, Software Engineer, TensorFlow&lt;/span&gt; &lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Feature_learning"&gt;Representation learning&lt;/a&gt; is a machine learning (ML) method that trains a model to identify salient features that can be applied to a variety of downstream tasks, ranging from natural language processing (e.g., &lt;a href="https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html"&gt;BERT&lt;/a&gt; and &lt;a href="https://ai.googleblog.com/2019/12/albert-lite-bert-for-self-supervised.html"&gt;ALBERT&lt;/a&gt;) to image analysis and classification (e.g., &lt;a href="https://arxiv.org/abs/1411.1792"&gt;Inception layers&lt;/a&gt; and &lt;a href="https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html"&gt;SimCLR&lt;/a&gt;). Last year, we &lt;a href="https://ai.googleblog.com/2020/06/improving-speech-representations-and.html"&gt;introduced&lt;/a&gt; a benchmark for comparing speech representations and a new, generally-useful speech representation model (&lt;a href="https://arxiv.org/abs/2002.12764"&gt;TRILL&lt;/a&gt;). TRILL is based on temporal proximity, and tries to map speech that occurs close together in time to a lower-dimensional embedding that captures temporal proximity in the embedding space. Since its release, the research community has used TRILL on a diverse set of tasks, such as &lt;a href="http://essv.de/essv2021/pdfs/02_burkhardt_v2.pdf"&gt;age classification&lt;/a&gt;, &lt;a href="https://arxiv.org/pdf/2101.00073.pdf"&gt;video thumbnail selection&lt;/a&gt;, and &lt;a href="https://arxiv.org/pdf/2103.06695.pdf"&gt;language identification&lt;/a&gt;. However, despite achieving state-of-the-art performance, TRILL and other neural network-based approaches require more memory and take longer to compute than signal processing operations that deal with simple features, like loudness, average energy, pitch, etc. &lt;/p&gt;&lt;p&gt;In our recent paper "&lt;a href="https://arxiv.org/abs/2011.04609"&gt;FRILL: A Non-Semantic Speech Embedding for Mobile Devices&lt;/a&gt;", to appear at &lt;a href="https://www.interspeech2021.org/"&gt;Interspeech 2021&lt;/a&gt;, we create a new model that is 40% the size of TRILL and and a feature set that can be computed over 32x faster on mobile phone, with an average decrease in accuracy of less than 2%. This marks an important step towards fully on-device applications of speech ML models, which will lead to better personalization, improved user experiences and greater privacy, an important aspect of &lt;a href="https://ai.google/responsibilities/responsible-ai-practices/"&gt;developing AI responsibly&lt;/a&gt;. We release the code to create FRILL &lt;a href="https://github.com/google-research/google-research/tree/master/non_semantic_speech_benchmark/distillation"&gt;on github&lt;/a&gt;, and a pre-trained FRILL model on &lt;a href="https://tfhub.dev/s?q=nonsemantic-speech-benchmark%2Ffrill"&gt;TensorFlow Hub&lt;/a&gt;.  &lt;/p&gt;&lt;p&gt;&lt;b&gt;FRILL: Smaller, Faster TRILL&lt;/b&gt;&lt;br /&gt;The TRILL architecture is based on a &lt;a href="https://arxiv.org/abs/1609.09430"&gt;modified version&lt;/a&gt; of &lt;a href="https://arxiv.org/abs/1512.03385"&gt;ResNet50&lt;/a&gt;, an architecture that is computationally taxing for constrained hardware, like mobile phones or smart home devices. On the other hand, architectures like &lt;a href="https://arxiv.org/abs/1905.02244"&gt;MobileNetV3&lt;/a&gt; have been designed with hardware-aware &lt;a href="https://ai.googleblog.com/2017/05/using-machine-learning-to-explore.html"&gt;AutoML&lt;/a&gt; to perform well on mobile devices. To take advantage of this, we leverage &lt;a href="https://research.google/pubs/pub44873/"&gt;knowledge distillation&lt;/a&gt; to combine the benefits of MobileNetV3’s performance with TRILL’s representations. &lt;/p&gt;&lt;p&gt;In the distillation process, the smaller model (i.e., the "student") tries to match the output of the larger model ("teacher") on the &lt;a href="https://ai.googleblog.com/2017/03/announcing-audioset-dataset-for-audio.html"&gt;AudioSet&lt;/a&gt; dataset. Whereas the original TRILL model learned its weights by optimizing a self-supervised loss that clustered audio segments close in time, the student model learns its weights through a fully-supervised loss that ignores temporal matching and instead tries to match TRILL outputs on the training data. The fully-supervised learning signal is often stronger than self-supervision, and allows us to train more quickly. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-O_md_o62dzA/YMOWCM9XsaI/AAAAAAAAHuY/UyCHkdRikh8xFUDRPcrwomvCWsBtukQbACLcBGAsYHQ/s1999/image3.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="1334" data-original-width="1999" height="268" src="https://1.bp.blogspot.com/-O_md_o62dzA/YMOWCM9XsaI/AAAAAAAAHuY/UyCHkdRikh8xFUDRPcrwomvCWsBtukQbACLcBGAsYHQ/w400-h268/image3.png" width="400" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Knowledge distillation for non-semantic speech embeddings. The dashed line shows the student model output. The "teacher network" is the&amp;nbsp;&lt;a href="https://tfhub.dev/google/nonsemantic-speech-benchmark/trill/3" style="text-align: start;"&gt;TRILL network&lt;/a&gt;, where "&lt;a href="https://arxiv.org/abs/2002.12764" style="text-align: start;"&gt;Layer 19&lt;/a&gt;" was the best-performing internal representation. The "Student Hyperparameters" on the left are the options explored in this study, the result of which are 144 distinct models. These models were trained with&amp;nbsp;&lt;a href="https://en.wikipedia.org/wiki/Mean_squared_error" style="text-align: start;"&gt;mean-squared error&lt;/a&gt;&amp;nbsp;(MSE) to try to match TRILL's Layer 19.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;!-- &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-GAC5cxhqpJk/YMDgE7aDQLI/AAAAAAAAHuA/HT7K9Y_kGII71Qm8dd1Pd7N_nevOklCqgCLcBGAsYHQ/s1576/image2.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="1078" data-original-width="1576" height="274" src="https://1.bp.blogspot.com/-GAC5cxhqpJk/YMDgE7aDQLI/AAAAAAAAHuA/HT7K9Y_kGII71Qm8dd1Pd7N_nevOklCqgCLcBGAsYHQ/w400-h274/image2.png" width="400" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Knowledge distillation for non-semantic speech embeddings. The dashed line shows the student model output. The "teacher network" is the&amp;nbsp;&lt;a href="https://tfhub.dev/google/nonsemantic-speech-benchmark/trill/3" style="text-align: start;"&gt;TRILL network&lt;/a&gt;, where "&lt;a href="https://arxiv.org/abs/2002.12764" style="text-align: start;"&gt;Layer 19&lt;/a&gt;" was the best-performing internal representation. The "Student Hyperparameters" on the left are the options explored in this study, the result of which are 144 distinct models. These models were trained with&amp;nbsp;&lt;a href="https://en.wikipedia.org/wiki/Mean_squared_error" style="text-align: start;"&gt;mean-squared error&lt;/a&gt;&amp;nbsp;(MSE) to try to match TRILL's Layer 19.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;--&gt;&lt;p&gt;&lt;b&gt;Choosing the Best Student Model&lt;/b&gt;&lt;br /&gt;We perform distillation with a variety of student models, each trained with a specific combination of architecture choices (explained below). To measure each student model’s latency, we leverage &lt;a href="https://www.tensorflow.org/lite"&gt;TensorFlow Lite&lt;/a&gt; (TFLite), a framework that enables execution of TensorFlow models on &lt;a href="https://en.wikipedia.org/wiki/Edge_device"&gt;edge devices&lt;/a&gt;.  Each candidate model is first &lt;a href="https://www.tensorflow.org/lite/convert#python_api"&gt;converted&lt;/a&gt; into TFLite’s &lt;a href="https://google.github.io/flatbuffers/"&gt;flatbuffer&lt;/a&gt; format for 32-bit floating point inference and then sent to the target device (in this case, a Pixel 1) for benchmarking. These measurements help us to accurately assess the latency versus quality tradeoffs across all student models and to minimize  the loss of quality in the conversion process. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Architecture Choices and Optimizations&lt;/b&gt;&lt;br /&gt;We explored different neural network architectures and features that balance latency and accuracy — models with fewer parameters are usually smaller and faster, but have less representational power and therefore generate less generally-useful representations. We trained 144 different models across a number of hyperparameters, all based on the MobileNetV3 architecture: &lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;em&gt;MobileNetV3 size and width:&lt;/em&gt; MobileNetV3 was released in different sizes for use in different environments. The &lt;em&gt;size&lt;/em&gt; refers to which MobileNetV3 architecture we used. The &lt;em&gt;width&lt;/em&gt;, sometimes known as &lt;a href="https://www.tensorflow.org/api_docs/python/tf/keras/applications/MobileNet"&gt;alpha&lt;/a&gt;, proportionally decreases or increases the number of filters in each layer. A width of 1.0 corresponds to the number of filters in the original paper.  &lt;/li&gt;  &lt;li&gt;&lt;em&gt;Global average pooling:&lt;/em&gt; MobileNetV3 normally produces a set of two-dimensional feature maps. These are flattened, concatenated, and passed to the bottleneck layer. However, this bottleneck is often still too large to be computed quickly. We reduce the size of the bottleneck layer kernel by taking the global average of all ”pixels” in each output feature map. Our intuition is that the discarded temporal information is less important for learning a non-semantic speech representation due to the fact that relevant aspects of the signal are stable across time. &lt;/li&gt;  &lt;li&gt;&lt;em&gt;Bottleneck compression:&lt;/em&gt; A significant portion of the student model’s weights are located in the bottleneck layer. To reduce the size of this layer, we apply a compression operator based on &lt;a href="https://en.wikipedia.org/wiki/Singular_value_decomposition"&gt;singular value decomposition&lt;/a&gt; (SVD) that &lt;a href="https://blog.tensorflow.org/2020/02/matrix-compression-operator-tensorflow.html"&gt;learns a low-rank approximation of the bottleneck weight matrix&lt;/a&gt;. &lt;/li&gt;  &lt;li&gt;&lt;em&gt;Quantization-aware training:&lt;/em&gt; Since the bottleneck layer has most of the model weights, we use &lt;a href="https://www.tensorflow.org/model_optimization/guide/quantization/training"&gt;quantization-aware training&lt;/a&gt; (QAT) to gradually reduce the numerical precision of the bottleneck weights during training. QAT allows the model to adjust to the lower numerical precision during training, instead of potentially causing performance degradation by introducing quantization after training finishes. &lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;b&gt;Results&lt;/b&gt;&lt;br /&gt;We evaluated each of these models on the &lt;a href="https://ai.googleblog.com/2020/06/improving-speech-representations-and.html"&gt;Non-Semantic Speech Benchmark&lt;/a&gt; (NOSS) and two new tasks — a challenging task to detect whether a speaker is &lt;a href="https://www.isca-speech.org/archive/Interspeech_2020/pdfs/0032.pdf"&gt;wearing a mask&lt;/a&gt; and the human-noise subset of the &lt;a href="https://github.com/karolpiczak/ESC-50"&gt;Environment Sound Classification&lt;/a&gt; dataset, which includes labels like “coughing” and “sneezing”. After eliminating models that have strictly better alternatives, we are left with eight ”frontier” models on the quality vs. latency curve, which are the models that had no faster and better performance alternatives at a corresponding quality threshold or latency in our batch of 144 models. We plot the latency vs. quality curve of only these "frontier" models below, and we ignore models that are strictly worse. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-xk0BNRmPhp0/YMOWLAl2nrI/AAAAAAAAHuc/YYgevJIjeWU0ZLZ1V2ww4vZBrQaEtMwvQCLcBGAsYHQ/s1999/image1.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="1143" data-original-width="1999" height="229" src="https://1.bp.blogspot.com/-xk0BNRmPhp0/YMOWLAl2nrI/AAAAAAAAHuc/YYgevJIjeWU0ZLZ1V2ww4vZBrQaEtMwvQCLcBGAsYHQ/w400-h229/image1.png" width="400" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Embedding quality and latency tradeoff. The x-axis represents the inference latency and the y-axis shows the difference in accuracy from TRILL’s performance, averaged across benchmark datasets.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;!-- &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-0fXDjt2seoc/YMDgRZOg_9I/AAAAAAAAHuE/8xl259QL0hgiec4yQvqYi-7Fo__A8CDqgCLcBGAsYHQ/s1192/image1.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="692" data-original-width="1192" height="233" src="https://1.bp.blogspot.com/-0fXDjt2seoc/YMDgRZOg_9I/AAAAAAAAHuE/8xl259QL0hgiec4yQvqYi-7Fo__A8CDqgCLcBGAsYHQ/w400-h233/image1.png" width="400" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Embedding quality and latency tradeoff. The x-axis represents the inference latency and the y-axis shows the difference in accuracy from TRILL’s performance, averaged across benchmark datasets.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;--&gt;&lt;p&gt;FRILL is the best performing sub-10ms inference model, with an inference time of 8.5 ms on a Pixel 1 (about 32x faster than TRILL), and is also roughly 40% the size of TRILL. The frontier curve plateaus at about 10ms latency, which means that at low latency, one can achieve much better performance with minimal latency costs, while achieving improved performance at latencies beyond 10ms is more difficult. This supports our choice of experiment hyperparameters. FRILL's per-task performance is shown in the table below. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; width: 50%;"&gt; &lt;colgroup&gt;   &lt;col span="1" style="width: 20%;"&gt;&lt;/col&gt;   &lt;col span="1" style="width: 20%;"&gt;&lt;/col&gt;   &lt;col span="1" style="width: 10%;"&gt;&lt;/col&gt; &lt;/colgroup&gt;&lt;tbody&gt;&lt;tr&gt;   &lt;td&gt;   &lt;/td&gt;   &lt;td&gt;&lt;b&gt;&lt;em&gt;FRILL&lt;/em&gt;&lt;/b&gt;   &lt;/td&gt;   &lt;td&gt;&lt;b&gt;&lt;em&gt;TRILL&lt;/em&gt;&lt;/b&gt;   &lt;/td&gt;  &lt;/tr&gt;   &lt;tr&gt;&lt;td&gt;&lt;br /&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;  &lt;tr&gt;    &lt;td style="text-align: right;"&gt;&lt;em&gt;Size (MB)&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;38.5    &lt;/td&gt;   &lt;td&gt;98.1    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td style="text-align: right;"&gt;&lt;em&gt;Latency (ms)&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;8.5     &lt;/td&gt;   &lt;td&gt;275.3    &lt;/td&gt;  &lt;/tr&gt;   &lt;tr&gt;&lt;td&gt;&lt;br /&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;  &lt;tr&gt;    &lt;td style="text-align: right;"&gt;&lt;em&gt;&lt;a href="https://arxiv.org/abs/1706.08612"&gt;Voxceleb1&lt;/a&gt;&lt;/em&gt;*    &lt;/td&gt;   &lt;td&gt;45.5    &lt;/td&gt;   &lt;td&gt;46.8    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td style="text-align: right;"&gt;&lt;em&gt;&lt;a href="http://www.voxforge.org/home"&gt;Voxforge&lt;/a&gt;&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;78.8    &lt;/td&gt;   &lt;td&gt;84.5    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td style="text-align: right;"&gt;&lt;em&gt;&lt;a href="https://arxiv.org/abs/1804.03209"&gt;Speech Commands&lt;/a&gt;&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;81.0    &lt;/td&gt;   &lt;td&gt;81.7    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td style="text-align: right;"&gt;&lt;em&gt;&lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4313618/"&gt;CREMA-D&lt;/a&gt;&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;71.3    &lt;/td&gt;   &lt;td&gt;65.9    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td style="text-align: right;"&gt;&lt;em&gt;&lt;a href="https://www.researchgate.net/publication/260311132_Surrey_Audio-Visual_Expressed_Emotion_SAVEE_database"&gt;SAVEE&lt;/a&gt;&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;63.3    &lt;/td&gt;   &lt;td&gt;70.0    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td style="text-align: right;"&gt;&lt;em&gt;&lt;a href="https://www.isca-speech.org/archive/Interspeech_2020/pdfs/0032.pdf"&gt;Masked Speech&lt;/a&gt;&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;68.0    &lt;/td&gt;   &lt;td&gt;65.8    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td style="text-align: right;"&gt;&lt;em&gt;&lt;a href="https://github.com/karolpiczak/ESC-50"&gt;ESC-50 HS&lt;/a&gt;&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;87.9    &lt;/td&gt;   &lt;td&gt;86.4    &lt;/td&gt;  &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;  &lt;tbody&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Accuracy on each of the classification tasks (higher is better).&lt;br /&gt;*Results in our study use a small subset of Voxceleb1 filtered according to internal privacy guidelines. Interested readers can run our study on the full dataset using TensorFlow Datasets and our open-source evaluation code.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Finally, we evaluate the relative contribution of each of our hyperparameters. We find that for our experiments, quantization-aware training, bottleneck compression and global average pooling most reduced the latency of the resulting models. At the same time bottleneck compression most reduced the quality of the resulting model, while pooling reduced the model performance the least. The architecture width parameter was an important factor in reducing the model size, with minimal performance degradation. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-S8L1pWEdw6c/YMOWPhIp5JI/AAAAAAAAHug/hOm5S6tQ8RsYV4zi5CgeMClFRN3oENbHQCLcBGAsYHQ/s1999/image2.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="1329" data-original-width="1999" height="266" src="https://1.bp.blogspot.com/-S8L1pWEdw6c/YMOWPhIp5JI/AAAAAAAAHug/hOm5S6tQ8RsYV4zi5CgeMClFRN3oENbHQCLcBGAsYHQ/w400-h266/image2.png" width="400" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Linear regression weight magnitudes for predicting model quality, latency, and size. The weights indicate the expected impact of changing the input hyperparameter. A higher weight magnitude indicates a greater expected impact.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;!-- &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-xjPyLPg2R0g/YMDgq92m5zI/AAAAAAAAHuQ/SvfC506SWuYprqTQTr01kJQbiSMcfp7qwCLcBGAsYHQ/s1546/image3.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="1076" data-original-width="1546" height="279" src="https://1.bp.blogspot.com/-xjPyLPg2R0g/YMDgq92m5zI/AAAAAAAAHuQ/SvfC506SWuYprqTQTr01kJQbiSMcfp7qwCLcBGAsYHQ/w400-h279/image3.png" width="400" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Linear regression weight magnitudes for predicting model quality, latency, and size. The weights indicate the expected impact of changing the input hyperparameter. A higher weight magnitude indicates a greater expected impact.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;--&gt;&lt;p&gt;Our work is an important step in bringing the full benefits of speech machine learning research to mobile devices. We also provide our&amp;nbsp;&lt;a href="https://tfhub.dev/s?q=nonsemantic-speech-benchmark%2Ffrill"&gt;public model&lt;/a&gt;,&amp;nbsp;corresponding &lt;a href="https://tfhub.dev/s?q=nonsemantic-speech-benchmark%2Ffrill"&gt;model card&lt;/a&gt;,&amp;nbsp;and &lt;a href="https://github.com/google-research/google-research/tree/master/non_semantic_speech_benchmark"&gt;evaluation code&lt;/a&gt; to help the research community &lt;a href="https://www.tensorflow.org/responsible_ai"&gt;responsibly develop&lt;/a&gt; even more applications for on-device speech representation research. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Acknowledgements&lt;/b&gt;&lt;br /&gt;&lt;em&gt;We'd like to thank our paper co-authors: Jacob Peplinski, Jake Garrison and Shwetak Patel. We'd like to thank Aren Jansen for his technical support on this project, Françoise Beaufays, and Tulsee Doshi for help open sourcing the model, and Google Research, Tokyo for logistical support.&lt;/em&gt;&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/blogspot/gJZg?a=q4thoYinM5E:0lFTzxN_-y8:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/blogspot/gJZg/~4/q4thoYinM5E" height="1" width="1" alt=""/&gt;</content>
    <link rel="replies" type="application/atom+xml" href="http://ai.googleblog.com/feeds/24422599744309840/comments/default" title="Post Comments"/>
    <link rel="replies" type="text/html" href="http://ai.googleblog.com/2021/06/frill-on-device-speech-representations.html#comment-form" title="0 Comments"/>
    <link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/24422599744309840"/>
    <link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/24422599744309840"/>
    <link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/blogspot/gJZg/~3/q4thoYinM5E/frill-on-device-speech-representations.html" title="FRILL: On-Device Speech Representations using TensorFlow-Lite"/>
    <author>
      <name>Google AI</name>
      <uri>http://www.blogger.com/profile/12098626514775266161</uri>
      <email>noreply@blogger.com</email>
      <gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="16" height="16" src="https://img1.blogblog.com/img/b16-rounded.gif"/>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://1.bp.blogspot.com/-O_md_o62dzA/YMOWCM9XsaI/AAAAAAAAHuY/UyCHkdRikh8xFUDRPcrwomvCWsBtukQbACLcBGAsYHQ/s72-w400-h268-c/image3.png" height="72" width="72"/>
    <thr:total>0</thr:total>
    <feedburner:origLink>http://ai.googleblog.com/2021/06/frill-on-device-speech-representations.html</feedburner:origLink>
  </entry>
  <entry>
    <id>tag:blogger.com,1999:blog-8474926331452026626.post-8475924323792519400</id>
    <published>2021-06-08T11:05:00.000-07:00</published>
    <updated>2021-06-08T11:05:58.425-07:00</updated>
    <category scheme="http://www.blogger.com/atom/ns#" term="Computer Vision"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="CVPR"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="Deep Learning"/>
    <title type="text">Using Variational Transformer Networks to Automate Document Layout Design</title>
    <content type="html">&lt;span class="byline-author"&gt;Posted by Diego Martin Arroyo, Software Engineer and Federico Tombari, Research Scientist, Google Research&lt;/span&gt; &lt;p&gt;Information in a written document is not only conveyed by the meaning of the words contained in it, but also by the overall document layout. Layouts are commonly used to direct the order in which the reader parses a document to enable a better understanding (e.g., with columns or paragraphs), to provide helpful summaries (e.g., with titles) or for aesthetic purposes (e.g., when displaying advertisements).  &lt;/p&gt;&lt;p&gt;While these design rules are easy to follow, it is difficult to explicitly define them without quickly needing to include exceptions or encountering ambiguous cases. This makes the automation of document design difficult, as any system with a hardcoded set of production rules will either be overly simplistic and thus incapable of producing original layouts (causing a lack of diversity in the layout of synthesized data), or too complex, with a large set of rules and their accompanying exceptions. In an attempt to solve this challenge, some &lt;a href="https://arxiv.org/abs/1901.06767"&gt;have&lt;/a&gt; &lt;a href="https://arxiv.org/abs/1912.09421"&gt;proposed&lt;/a&gt; machine learning (ML) techniques to synthesize document layouts. However, most ML-based solutions for automatic document design do not scale to a large number of layout components, or they rely on additional information for training, such as the relationships between the different components of a document. &lt;/p&gt;&lt;p&gt;In “&lt;em&gt;&lt;a href="https://arxiv.org/pdf/2104.02416.pdf"&gt;Variational Transformer Networks for Layout Generation&lt;/a&gt;&lt;/em&gt;”, to be presented at &lt;a href="http://cvpr2021.thecvf.com/"&gt;CVPR 2021&lt;/a&gt;, we create a document layout generation system that scales to an arbitrarily large number of elements and does not require any additional information to capture the relationships between design elements. We use &lt;a href="https://en.wikipedia.org/wiki/Attention_(machine_learning)"&gt;self-attention layers&lt;/a&gt; as building blocks of a &lt;a href="https://arxiv.org/abs/1312.6114"&gt;variational autoencoder&lt;/a&gt; (VAE), which is able to model document layout design rules as a distribution, rather than using a set of predetermined heuristics, increasing the diversity of the generated layouts. The resulting Variational Transformer Network (VTN) model is able to extract meaningful relationships between the layout elements (paragraphs, tables, images, etc.), resulting in realistic synthetic documents (e.g., better alignment and margins). We show the effectiveness of this combination across different domains, such as scientific papers, UI layouts, and even furniture arrangements. &lt;/p&gt;&lt;p&gt;&lt;b&gt;VAEs for Layout Generation&lt;/b&gt;&lt;br&gt;The ultimate goal of this system is to infer the design rules for a given type of layout from a collection of examples. If one considers these design rules as the distribution underlying the data, it is possible to use probabilistic models to discover it. We propose doing this with a VAE (widely used for tasks like &lt;a href="https://openreview.net/pdf?id=ryeBN88Ku4"&gt;image generation&lt;/a&gt; or &lt;a href="https://arxiv.org/pdf/2010.05531.pdf"&gt;anomaly detection&lt;/a&gt;), an &lt;a href="https://arxiv.org/abs/1312.6114"&gt;autoencoder&lt;/a&gt; architecture that consists of two distinct subparts, the encoder and decoder. The encoder learns to compress the input to fewer dimensions, retaining only the necessary information to reconstruct the input, while the decoder learns to undo this operation. The compressed representation (also called the &lt;em&gt;bottleneck&lt;/em&gt;) can be forced to behave like a known distribution (e.g., a uniform Gaussian). Feeding samples from this &lt;em&gt;a priori&lt;/em&gt; distribution to the decoder segment of the network results in outputs similar to the training data.  &lt;/p&gt;&lt;p&gt;An additional advantage of the VAE formulation is that it is agnostic to the type of operations used to implement the encoder and decoder segments. As such, we use &lt;a href="https://en.wikipedia.org/wiki/Attention_(machine_learning)"&gt;self-attention&lt;/a&gt; layers (typically seen in &lt;a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html"&gt;Transformer&lt;/a&gt; architectures) to automatically capture the influence that each layout element has over the rest. &lt;/p&gt;&lt;p&gt;Transformers use self-attention layers to model long, sequenced relationships, often applied to an array of natural language understanding tasks, such as &lt;a href="https://ai.googleblog.com/2020/06/recent-advances-in-google-translate.html"&gt;translation&lt;/a&gt; and &lt;a href="https://ai.googleblog.com/2020/06/pegasus-state-of-art-model-for.html"&gt;summarization&lt;/a&gt;, as well as beyond the language domain in &lt;a href="https://arxiv.org/abs/2005.12872"&gt;object detection&lt;/a&gt; or &lt;a href="https://arxiv.org/abs/2006.14615"&gt;document layout understanding&lt;/a&gt; tasks. The self-attention operation relates every element in a sequence to every other and determines how they influence each other. This property is ideal to model relationships across different elements in a layout without the need for explicit annotations. &lt;/p&gt;&lt;p&gt;In order to synthesize new samples from these relationships, some approaches for layout generation [e.g., &lt;a href="https://arxiv.org/abs/2006.14615"&gt;1&lt;/a&gt;] and even for other domains [e.g., &lt;a href="https://arxiv.org/pdf/1706.03762.pdf"&gt;2&lt;/a&gt;, &lt;a href="https://arxiv.org/pdf/1909.05858.pdf"&gt;3&lt;/a&gt;] rely on &lt;a href="https://en.wikipedia.org/wiki/Greedy_algorithm"&gt;greedy search algorithms&lt;/a&gt;, such as &lt;a href="https://en.wikipedia.org/wiki/Beam_search"&gt;beam search&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/1904.09751"&gt;nucleus sampling&lt;/a&gt; or &lt;a href="https://arxiv.org/abs/1805.04833"&gt;top-k sampling&lt;/a&gt;. Since these strategies are often based on exploration rules that tend to favor the most likely outcome at every step, the diversity of the generated samples is not guaranteed. However, by combining self-attention with the VAE’s probabilistic techniques, the model is able to directly learn a distribution from which it can extract new elements.  &lt;/p&gt;&lt;p&gt;&lt;b&gt;Modeling the Variational Bottleneck&lt;/b&gt;&lt;br&gt;The bottleneck of a VAE is commonly modeled as a vector representing the input. Since self-attention layers are a &lt;a href="https://papers.nips.cc/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf"&gt;sequence-to-sequence&lt;/a&gt; architecture, i.e., a sequence of &lt;em&gt;n&lt;/em&gt; input elements is mapped onto &lt;em&gt;n&lt;/em&gt; output elements, the standard VAE formulation is difficult to apply. Inspired by &lt;a href="https://arxiv.org/abs/1810.04805v2"&gt;BERT&lt;/a&gt;, we append an auxiliary token to the beginning of the sequence and treat it as the autoencoder bottleneck vector &lt;em&gt;z&lt;/em&gt;. During training, the vector associated with this token is the only piece of information passed to the decoder, so the encoder needs to learn how to compress the entire document information in this vector. The decoder then learns to infer the number of elements in the document as well as the locations of each element in the input sequence from this vector alone. This strategy allows us to use standard techniques to regularize the bottleneck, such as the&lt;a href="https://de.wikipedia.org/wiki/Kullback-Leibler-Divergenz"&gt; KL divergence&lt;/a&gt;.  &lt;/p&gt;&lt;p&gt;&lt;b&gt;Decoding&lt;/b&gt;&lt;br&gt;In order to synthesize documents with varying numbers of elements, the network needs to model sequences of arbitrary length, which is not trivial. While self-attention enables the encoder to adapt automatically to any number of elements, the decoder segment does not know the number of elements in advance. We overcome this issue by decoding sequences in an  autoregressive way — at every step, the decoder produces an element, which is concatenated to the previously decoded elements (starting with the bottleneck vector &lt;em&gt;z&lt;/em&gt; as input), until a special &lt;em&gt;stop&lt;/em&gt; element is produced. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-HQOCEY6CpLA/YL-nw7Dm-xI/AAAAAAAAHtY/Jhph-vyjq5o6yz4Ui1zA3W6gTHtJcVprgCLcBGAsYHQ/s1600/image3.gif" imageanchor="1" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="560" data-original-width="1600" height="224" src="https://1.bp.blogspot.com/-HQOCEY6CpLA/YL-nw7Dm-xI/AAAAAAAAHtY/Jhph-vyjq5o6yz4Ui1zA3W6gTHtJcVprgCLcBGAsYHQ/w640-h224/image3.gif" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;A visualization of our proposed architecture&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;b&gt;Turning Layouts into Input Data&lt;/b&gt;&lt;br&gt;A document is often composed of several design elements, such as paragraphs, tables, images, titles, footnotes, etc. In terms of design, layout elements are often represented by the coordinates of their enclosing bounding boxes. To make this information easily digestible for a neural network, we define each element with four variables (x, y, width, height), representing the element’s location on the page (x, y) and size (width, height).  &lt;/p&gt;&lt;p&gt;&lt;b&gt;Results&lt;/b&gt;&lt;br&gt;We evaluate the performance of the VTN following two criteria: layout &lt;em&gt;quality&lt;/em&gt; and layout &lt;em&gt;diversity&lt;/em&gt;. We train the model on publicly available document datasets, such as &lt;a href="https://arxiv.org/abs/1908.07836"&gt;PubLayNet&lt;/a&gt;, a collection of scientific papers with layout annotations, and evaluate the quality of generated layouts by quantifying the amount of overlap and alignment between elements. We measure how well the synthetic layouts resemble the training distribution using the &lt;a href="https://en.wikipedia.org/wiki/Wasserstein_metric"&gt;Wasserstein distance&lt;/a&gt; over the distributions of element classes (e.g., paragraphs, images, etc.) and bounding boxes. In order to capture the layout diversity, we find the most similar real sample for each generated document using the &lt;a href="https://arxiv.org/abs/1909.00302"&gt;DocSim&lt;/a&gt; metric, where a higher number of unique matches to the real data indicates a more diverse outcome. &lt;/p&gt;&lt;p&gt;We compare the VTN approach to previous works like &lt;a href="https://arxiv.org/abs/1907.10719"&gt;LayoutVAE&lt;/a&gt; and &lt;a href="https://arxiv.org/abs/2006.14615"&gt;Gupta et al.&lt;/a&gt; The former is a VAE-based formulation with an &lt;a href="https://en.wikipedia.org/wiki/Long_short-term_memory"&gt;LSTM backbone&lt;/a&gt;, whereas Gupta et al. use a self-attention mechanism similar to ours, combined with standard search strategies (beam search). The results below show that LayoutVAE struggles to comply with design rules, like strict alignments, as in the case of PubLayNet. Thanks to the self-attention operation, Gupta &lt;em&gt;et al. &lt;/em&gt;can model these constraints much more effectively, but the usage of beam search affects the diversity of the results.  &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt; &lt;colgroup&gt;   &lt;col span="1" style="width: 16%;"&gt;&lt;/col&gt;   &lt;col span="1" style="width: 12%;"&gt;&lt;/col&gt;   &lt;col span="1" style="width: 12%;"&gt;&lt;/col&gt;   &lt;col span="1" style="width: 12%;"&gt;&lt;/col&gt;   &lt;col span="1" style="width: 12%;"&gt;&lt;/col&gt;   &lt;col span="1" style="width: 12%;"&gt;&lt;/col&gt;   &lt;col span="1" style="width: 12%;"&gt;&lt;/col&gt; &lt;/colgroup&gt;  &lt;tbody&gt;  &lt;tr&gt;   &lt;td&gt;   &lt;/td&gt;   &lt;td&gt;&lt;em&gt;IoU&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;&lt;em&gt;Overlap&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;&lt;em&gt;Alignment&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;&lt;em&gt;Wasserstein Class ↓&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;&lt;em&gt;Wasserstein Box ↓&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;&lt;em&gt;# Unique Matches ↑&lt;/em&gt;   &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td&gt;&lt;em&gt;LayoutVAE&amp;nbsp;&amp;nbsp;&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;0.171    &lt;/td&gt;   &lt;td&gt;0.321    &lt;/td&gt;   &lt;td&gt;0.472    &lt;/td&gt;   &lt;td&gt;-    &lt;/td&gt;   &lt;td&gt;0.045    &lt;/td&gt;   &lt;td&gt;241    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td&gt;&lt;em&gt;Gupta et al.&amp;nbsp;&amp;nbsp;&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;0.039    &lt;/td&gt;   &lt;td&gt;0.006    &lt;/td&gt;   &lt;td&gt;0.361    &lt;/td&gt;   &lt;td&gt;&lt;b&gt;0.018&lt;/b&gt;   &lt;/td&gt;   &lt;td&gt;0.012    &lt;/td&gt;   &lt;td&gt;546    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td&gt;&lt;em&gt;VTN&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;0.031    &lt;/td&gt;   &lt;td&gt;0.017    &lt;/td&gt;   &lt;td&gt;0.347    &lt;/td&gt;   &lt;td&gt;0.022    &lt;/td&gt;   &lt;td&gt;0.012    &lt;/td&gt;   &lt;td&gt;697    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td&gt;&lt;em&gt;Real Data&amp;nbsp;&amp;nbsp;&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;0.048    &lt;/td&gt;   &lt;td&gt;0.007    &lt;/td&gt;   &lt;td&gt;0.353    &lt;/td&gt;   &lt;td&gt;-    &lt;/td&gt;   &lt;td&gt;-    &lt;/td&gt;   &lt;td&gt;-    &lt;/td&gt;  &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Results on PubLayNet. Down arrows (↓) indicate that a lower score is better, whereas up arrows (↑) indicate higher is better.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;We also explore the ability of our approach to learn design rules in other domains, such as Android UIs (&lt;a href="http://interactionmining.org/rico"&gt;RICO&lt;/a&gt;), natural scenes (&lt;a href="http://cocodataset.org/"&gt;COCO&lt;/a&gt;) and indoor scenes (&lt;a href="https://rgbd.cs.princeton.edu/"&gt;SUN RGB-D&lt;/a&gt;). Our method effectively learns the design rules of these datasets and produces synthetic layouts of similar quality as the current state of the art and a higher degree of diversity. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;   &lt;colgroup&gt;   &lt;col span="1" style="width: 16%;"&gt;&lt;/col&gt;   &lt;col span="1" style="width: 12%;"&gt;&lt;/col&gt;   &lt;col span="1" style="width: 12%;"&gt;&lt;/col&gt;   &lt;col span="1" style="width: 12%;"&gt;&lt;/col&gt;   &lt;col span="1" style="width: 12%;"&gt;&lt;/col&gt;   &lt;col span="1" style="width: 12%;"&gt;&lt;/col&gt;   &lt;col span="1" style="width: 12%;"&gt;&lt;/col&gt; &lt;/colgroup&gt;&lt;tbody&gt;  &lt;tr&gt;   &lt;td&gt;   &lt;/td&gt;    &lt;td&gt;&lt;em&gt;IoU&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;&lt;em&gt;Overlap&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;&lt;em&gt;Alignment&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;&lt;em&gt;Wasserstein Class ↓&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;&lt;em&gt;Wasserstein Box ↓&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;&lt;em&gt;# Unique Matches ↑&lt;/em&gt;   &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td&gt;&lt;em&gt;LayoutVAE&amp;nbsp;&amp;nbsp;&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;0.193    &lt;/td&gt;   &lt;td&gt;0.400    &lt;/td&gt;   &lt;td&gt;0.416    &lt;/td&gt;   &lt;td&gt;-    &lt;/td&gt;   &lt;td&gt;0.045    &lt;/td&gt;   &lt;td&gt;496    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td&gt;&lt;em&gt;Gupta et al.&amp;nbsp;&amp;nbsp;&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;0.086    &lt;/td&gt;   &lt;td&gt;0.145    &lt;/td&gt;   &lt;td&gt;0.366    &lt;/td&gt;   &lt;td&gt;&lt;b&gt;0.004&lt;/b&gt;   &lt;/td&gt;   &lt;td&gt;0.023    &lt;/td&gt;   &lt;td&gt;604    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td&gt;&lt;em&gt;VTN&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;0.115    &lt;/td&gt;   &lt;td&gt;0.165    &lt;/td&gt;   &lt;td&gt;0.373    &lt;/td&gt;   &lt;td&gt;0.007    &lt;/td&gt;   &lt;td&gt;&lt;b&gt;0.018&lt;/b&gt;   &lt;/td&gt;   &lt;td&gt;680    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td&gt;&lt;em&gt;Real Data&amp;nbsp;&amp;nbsp;&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;0.084    &lt;/td&gt;   &lt;td&gt;0.175    &lt;/td&gt;   &lt;td&gt;0.410    &lt;/td&gt;   &lt;td&gt;-    &lt;/td&gt;   &lt;td&gt;-    &lt;/td&gt;   &lt;td&gt;-    &lt;/td&gt;  &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Results on RICO. Down arrows (↓) indicate that a lower score is better, whereas up arrows (↑) indicate higher is better.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;   &lt;colgroup&gt;   &lt;col span="1" style="width: 16%;"&gt;&lt;/col&gt;   &lt;col span="1" style="width: 12%;"&gt;&lt;/col&gt;   &lt;col span="1" style="width: 12%;"&gt;&lt;/col&gt;   &lt;col span="1" style="width: 12%;"&gt;&lt;/col&gt;   &lt;col span="1" style="width: 12%;"&gt;&lt;/col&gt;   &lt;col span="1" style="width: 12%;"&gt;&lt;/col&gt;   &lt;col span="1" style="width: 12%;"&gt;&lt;/col&gt; &lt;/colgroup&gt;  &lt;tbody&gt; &lt;tr&gt;   &lt;td&gt;   &lt;/td&gt;   &lt;td&gt;&lt;em&gt;IoU&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;&lt;em&gt;Overlap&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;&lt;em&gt;Alignment&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;&lt;em&gt;Wasserstein Class ↓&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;&lt;em&gt;Wasserstein Box ↓&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;&lt;em&gt;# Unique Matches ↑&lt;/em&gt;   &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td&gt;&lt;em&gt;LayoutVAE&amp;nbsp;&amp;nbsp;&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;0.325    &lt;/td&gt;   &lt;td&gt;2.819    &lt;/td&gt;   &lt;td&gt;0.246    &lt;/td&gt;   &lt;td&gt;-    &lt;/td&gt;   &lt;td&gt;0.062    &lt;/td&gt;   &lt;td&gt;700    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td&gt;&lt;em&gt;Gupta et al.&amp;nbsp;&amp;nbsp;&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;0.194    &lt;/td&gt;   &lt;td&gt;1.709    &lt;/td&gt;   &lt;td&gt;0.334    &lt;/td&gt;   &lt;td&gt;0.001    &lt;/td&gt;   &lt;td&gt;0.016    &lt;/td&gt;   &lt;td&gt;601    &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td&gt;&lt;em&gt;VTN&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;0.197    &lt;/td&gt;   &lt;td&gt;2.384    &lt;/td&gt;   &lt;td&gt;0.330    &lt;/td&gt;   &lt;td&gt;&lt;b&gt;0.0005&lt;/b&gt;   &lt;/td&gt;   &lt;td&gt;&lt;b&gt;0.013&lt;/b&gt;   &lt;/td&gt;   &lt;td&gt;&lt;b&gt;776&lt;/b&gt;   &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td&gt;&lt;em&gt;Real Data&amp;nbsp;&amp;nbsp;&lt;/em&gt;   &lt;/td&gt;   &lt;td&gt;0.192    &lt;/td&gt;   &lt;td&gt;1.724    &lt;/td&gt;   &lt;td&gt;0.347    &lt;/td&gt;   &lt;td&gt;-    &lt;/td&gt;   &lt;td&gt;-    &lt;/td&gt;   &lt;td&gt;-    &lt;/td&gt;  &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Results for COCO. Down arrows (↓) indicate that a lower score is better, whereas up arrows (↑) indicate higher is better.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Below are some examples of layouts produced by our method compared to existing methods. The design rules learned by the network (location, margins, alignment) resemble those of the original data and show a high degree of variability. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;colgroup&gt;   &lt;col span="1" style="width: 15%;"&gt;&lt;/col&gt;   &lt;col span="1" style="width: 85%;"&gt;&lt;/col&gt; &lt;/colgroup&gt;&lt;tbody&gt;  &lt;tr&gt;   &lt;td&gt;LayoutVAE&amp;nbsp;&amp;nbsp;    &lt;/td&gt;   &lt;td&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-A_1HNfAqvJc/YL-oBht2-qI/AAAAAAAAHtc/z9SmPilNFB4OrihvZTtCcL7n-6Blzau0wCLcBGAsYHQ/s1999/image4.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="355" data-original-width="1999" height="114" src="https://1.bp.blogspot.com/-A_1HNfAqvJc/YL-oBht2-qI/AAAAAAAAHtc/z9SmPilNFB4OrihvZTtCcL7n-6Blzau0wCLcBGAsYHQ/w640-h114/image4.png" width="640" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td&gt;Gupta et al.&amp;nbsp;&amp;nbsp;    &lt;/td&gt;   &lt;td&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-oTJqSl4EvHk/YL-oO9lJl3I/AAAAAAAAHto/y453IZg-VqsQA5rWwTpX4KIcuUUovZgdACLcBGAsYHQ/s1999/image2.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="389" data-original-width="1999" height="124" src="https://1.bp.blogspot.com/-oTJqSl4EvHk/YL-oO9lJl3I/AAAAAAAAHto/y453IZg-VqsQA5rWwTpX4KIcuUUovZgdACLcBGAsYHQ/w640-h124/image2.png" width="640" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;   &lt;td&gt;VTN    &lt;/td&gt;   &lt;td&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-5ZoJjHwWBgY/YL-oTcyCnuI/AAAAAAAAHtw/Z7AVg73GnMw8tsQjghlSY7E0ngyJPuOSQCLcBGAsYHQ/s1999/image1.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="380" data-original-width="1999" height="122" src="https://1.bp.blogspot.com/-5ZoJjHwWBgY/YL-oTcyCnuI/AAAAAAAAHtw/Z7AVg73GnMw8tsQjghlSY7E0ngyJPuOSQCLcBGAsYHQ/w640-h122/image1.png" width="640" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;/td&gt;  &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Qualitative results of our method on PubLayNet compared to existing state-of-the-art methods.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;b&gt;Conclusion&lt;/b&gt;&lt;br&gt;In this work we show the feasibility of using self-attention as part of the VAE formulation. We validate the effectiveness of this approach for layout generation, achieving state-of-the-art performance on various datasets and across different tasks. Our research paper also explores alternative architectures for the integration of self-attention and VAEs, exploring non-autoregressive decoding strategies and different types of priors, and analyzes advantages and disadvantages. The layouts produced by our method can help to create synthetic training data for downstream tasks, such as document parsing or automating graphic design tasks. We hope that this work provides a foundation for continued research in this area, as many subproblems are still not completely solved, such as how to suggest styles for the elements in the layout (text font, which image to choose, etc.) or how to reduce the amount of training data necessary for the model to generalize. &lt;/p&gt;&lt;p&gt;&lt;strong&gt;Acknowledgements&lt;/strong&gt;&lt;em&gt;We thank our co-author Janis Postels, as well as Alessio Tonioni and Luca Prasso for helping with the design of several of our experiments. We also thank Tom Small for his help creating the animations for this post.&lt;/em&gt;&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/blogspot/gJZg?a=EWse3NrI8W4:9kpo0aGKIno:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/blogspot/gJZg/~4/EWse3NrI8W4" height="1" width="1" alt=""/&gt;</content>
    <link rel="replies" type="application/atom+xml" href="http://ai.googleblog.com/feeds/8475924323792519400/comments/default" title="Post Comments"/>
    <link rel="replies" type="text/html" href="http://ai.googleblog.com/2021/06/using-variational-transformer-networks.html#comment-form" title="0 Comments"/>
    <link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/8475924323792519400"/>
    <link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/8475924323792519400"/>
    <link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/blogspot/gJZg/~3/EWse3NrI8W4/using-variational-transformer-networks.html" title="Using Variational Transformer Networks to Automate Document Layout Design"/>
    <author>
      <name>Google AI</name>
      <uri>http://www.blogger.com/profile/12098626514775266161</uri>
      <email>noreply@blogger.com</email>
      <gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="16" height="16" src="https://img1.blogblog.com/img/b16-rounded.gif"/>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://1.bp.blogspot.com/-HQOCEY6CpLA/YL-nw7Dm-xI/AAAAAAAAHtY/Jhph-vyjq5o6yz4Ui1zA3W6gTHtJcVprgCLcBGAsYHQ/s72-w640-h224-c/image3.gif" height="72" width="72"/>
    <thr:total>0</thr:total>
    <feedburner:origLink>http://ai.googleblog.com/2021/06/using-variational-transformer-networks.html</feedburner:origLink>
  </entry>
  <entry>
    <id>tag:blogger.com,1999:blog-8474926331452026626.post-7927167454181190382</id>
    <published>2021-06-04T11:58:00.001-07:00</published>
    <updated>2021-06-04T12:20:46.301-07:00</updated>
    <category scheme="http://www.blogger.com/atom/ns#" term="Computer Vision"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="Image Classification"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="Supervised Learning"/>
    <title type="text">Extending Contrastive Learning to the Supervised Setting</title>
    <content type="html">&lt;span class="byline-author"&gt;Posted by AJ Maschinot, Senior Software Engineer and Jenny Huang, Product Manager, Google Research&lt;/span&gt; &lt;p&gt;In recent years, &lt;a href="https://arxiv.org/pdf/2011.00362.pdf"&gt;self-supervised representation learning&lt;/a&gt;, which is used in a variety of &lt;a href="https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html"&gt;image&lt;/a&gt; and &lt;a href="https://arxiv.org/abs/1704.06888"&gt;video&lt;/a&gt; tasks, has significantly advanced due to the application of &lt;a href="https://arxiv.org/abs/2002.05709"&gt;contrastive learning&lt;/a&gt;. These contrastive learning approaches typically teach a model to pull together the representations of a target image (a.k.a., the “anchor”) and a matching (“positive”) image in embedding space, while also pushing apart the anchor from many non-matching (“negative”) images. Because labels are assumed to be unavailable in self-supervised learning, the positive is often an augmentation of the anchor, and the negatives are chosen to be the other samples from the training minibatch. However, because of this random sampling, false negatives, i.e., negatives generated from samples of the same class as the anchor, can cause a &lt;a href="https://arxiv.org/abs/2011.11765"&gt;degradation&lt;/a&gt; in the representation quality. Furthermore, determining the optimal method to generate positives is still an area of &lt;a href="https://ai.googleblog.com/2020/08/understanding-view-selection-for.html"&gt;active research&lt;/a&gt;. &lt;/p&gt;&lt;p&gt;In contrast to the self-supervised approach, a fully-supervised approach could use labeled data to generate positives from existing same-class examples, providing more variability in pretraining than could typically be achieved by simply augmenting the anchor. However, very little work has been done to successfully apply contrastive learning in the fully-supervised domain. &lt;/p&gt;&lt;p&gt;In “&lt;a href="https://arxiv.org/abs/2004.11362"&gt;Supervised Contrastive Learning&lt;/a&gt;”, presented at &lt;a href="https://nips.cc/virtual/2020/public/poster_d89a66c7c80a29b1bdbab0f2a1a94af8.html"&gt;NeurIPS 2020&lt;/a&gt;, we propose a novel loss function, called SupCon, that bridges the gap between self-supervised learning and fully supervised learning and enables contrastive learning to be applied in the supervised setting. Leveraging labeled data, SupCon encourages normalized embeddings from the &lt;em&gt;same class&lt;/em&gt; to be pulled closer together, while embeddings from &lt;em&gt;different classes&lt;/em&gt; are pushed apart. This simplifies the process of positive selection, while avoiding potential false negatives. Because it accommodates multiple positives per anchor, this approach results in an improved selection of positive examples that are more varied, while still containing semantically relevant information. SupCon also allows label information to play an active role in representation learning rather than restricting it to be used only in downstream training, as is the case for conventional contrastive learning. To the best of our knowledge, this is the first contrastive loss to consistently perform better on large-scale image classification problems than the common approach of using &lt;a href="https://en.wikipedia.org/wiki/Cross_entropy"&gt;cross-entropy loss&lt;/a&gt; to train the model directly. Importantly, SupCon is straightforward to implement and stable to train, provides consistent improvement to top-1 accuracy for a number of datasets and architectures (including &lt;a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html"&gt;Transformer&lt;/a&gt; architectures), and is robust to &lt;a href="https://arxiv.org/abs/1903.12261"&gt;image corruptions&lt;/a&gt; and hyperparameter variations.  &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-_fMby-ad2AA/YLpXBtlI7fI/AAAAAAAAHsI/7TE6W0F5yNAhGEZFwPSvIJUM51A0ETs5wCLcBGAsYHQ/s1999/image5.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="922" data-original-width="1999" height="296" src="https://1.bp.blogspot.com/-_fMby-ad2AA/YLpXBtlI7fI/AAAAAAAAHsI/7TE6W0F5yNAhGEZFwPSvIJUM51A0ETs5wCLcBGAsYHQ/w640-h296/image5.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Self-supervised (&lt;b&gt;left&lt;/b&gt;) vs supervised (&lt;b&gt;right&lt;/b&gt;) contrastive losses: The self-supervised contrastive loss contrasts a single positive for each anchor (i.e., an augmented version of the same image) against a set of negatives consisting of the entire remainder of the minibatch. The supervised contrastive loss considered in this paper, however, contrasts the set of all samples from the same class as positives against the negatives from the remainder of the batch.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;b&gt;The Supervised Contrastive Learning Framework&lt;/b&gt;&lt;br /&gt;SupCon can be seen as a generalization of both the &lt;a href="https://arxiv.org/abs/2002.05709"&gt;SimCLR&lt;/a&gt; and &lt;a href="https://papers.nips.cc/paper/2016/file/6b180037abbebea991d8b1232f8a8ca9-Paper.pdf"&gt;N-pair&lt;/a&gt; losses — the former uses positives generated from the same sample as that of the anchor, and the latter uses positives generated from different samples by exploiting known class labels. The use of many positives and many negatives for each anchor allows SupCon to achieve state-of-the-art performance without the need for &lt;a href="https://arxiv.org/abs/2010.04592"&gt;hard negative mining&lt;/a&gt; (i.e., searching for negatives similar to the anchor), which can be difficult to tune properly.  &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-yfy-d_jamHI/YLpXGzdVOpI/AAAAAAAAHsM/gMN-Us-Rg7sZsycXO7UyF7UTK9JyYTAOgCLcBGAsYHQ/s1680/image1.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="1124" data-original-width="1680" src="https://1.bp.blogspot.com/-yfy-d_jamHI/YLpXGzdVOpI/AAAAAAAAHsM/gMN-Us-Rg7sZsycXO7UyF7UTK9JyYTAOgCLcBGAsYHQ/s320/image1.png" width="320" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;SupCon subsumes multiple losses from the literature and is a generalization of the SimCLR and N-Pair losses.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;This method is structurally similar to those used in self-supervised contrastive learning, with modifications for supervised classification. Given an input batch of data, we first apply data augmentation twice to obtain two copies, or “views,” of each sample in the batch (though one could create and use any number of augmented views). Both copies are forward propagated through an encoder network, and the resulting embedding is then &lt;a href="https://mathworld.wolfram.com/L2-Norm.html"&gt;L2-normalized&lt;/a&gt;. Following standard practice, the representation is further propagated through an &lt;a href="https://arxiv.org/abs/2002.05709"&gt;optional projection network&lt;/a&gt; to help identify meaningful features. The supervised contrastive loss is computed on the normalized outputs of the projection network. Positives for an anchor consist of the  representations originating from the same batch instance as the anchor or from other instances with the same label as the anchor; the negatives are then all remaining instances. To measure performance on downstream tasks, we train a linear classifier on top of the frozen representations. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-dqz--25E0_k/YLpXMDv-uJI/AAAAAAAAHsQ/Qwir9J1mjWUfchGiiaI1okkX-KVitKVagCLcBGAsYHQ/s1999/image3.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="940" data-original-width="1999" height="300" src="https://1.bp.blogspot.com/-dqz--25E0_k/YLpXMDv-uJI/AAAAAAAAHsQ/Qwir9J1mjWUfchGiiaI1okkX-KVitKVagCLcBGAsYHQ/w640-h300/image3.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Cross-entropy, self-supervised contrastive loss and supervised contrastive loss &lt;strong&gt;Left:&lt;/strong&gt; The cross-entropy loss uses labels and a softmax loss to train a classifier. &lt;strong&gt;Middle:&lt;/strong&gt; The self-supervised contrastive loss uses a contrastive loss and data augmentations to learn representations. &lt;strong&gt;Right: &lt;/strong&gt;The supervised contrastive loss also learns representations using a contrastive loss, but uses label information to sample positives in addition to augmentations of the same image.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;b&gt;Key Findings &lt;/b&gt;&lt;br /&gt;SupCon consistently boosts top-1 accuracy compared to cross-entropy, &lt;a href="https://en.wikipedia.org/wiki/Margin_classifier"&gt;margin classifiers&lt;/a&gt; (with use of labels), and self-supervised contrastive learning techniques on &lt;a href="http://www.cs.toronto.edu/~kriz/cifar.html"&gt;CIFAR-10 and CIFAR-100&lt;/a&gt; and &lt;a href="https://www.image-net.org/"&gt;ImageNet&lt;/a&gt; datasets. With SupCon, we achieve excellent top-1 accuracy on the ImageNet dataset with the &lt;a href="https://arxiv.org/abs/1512.03385"&gt;ResNet-50 and ResNet-200&lt;/a&gt; architectures. On ResNet-200, we achieve a top-1 accuracy of 81.4%, which is a 0.8% improvement over the &lt;a href="https://arxiv.org/abs/1905.00397"&gt;state-of-the-art&lt;/a&gt; cross-entropy loss using the same architecture (which represents a &lt;a href="https://paperswithcode.com/sota/image-classification-on-imagenet"&gt;significant advance for ImageNet&lt;/a&gt;). We also compared cross-entropy and SupCon on a &lt;a href="https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html"&gt;Transformer-based ViT-B/16&lt;/a&gt; model and found a consistent improvement over cross-entropy (77.8% versus 76% for ImageNet; 92.6% versus 91.6% for CIFAR-10) under the same data augmentation regime (without any higher-resolution fine-tuning). &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-bWernq6iaDU/YLpXRjyIU0I/AAAAAAAAHsU/Q_em95oBV5wK1ymi1cZFXGc87a_0ubZ2ACLcBGAsYHQ/s1484/image2.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="918" data-original-width="1484" height="397" src="https://1.bp.blogspot.com/-bWernq6iaDU/YLpXRjyIU0I/AAAAAAAAHsU/Q_em95oBV5wK1ymi1cZFXGc87a_0ubZ2ACLcBGAsYHQ/w640-h397/image2.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;The SupCon loss consistently outperforms cross-entropy with standard data augmentation strategies (&lt;a href="https://arxiv.org/abs/1805.09501"&gt;AutoAugment&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/1909.13719"&gt;RandAugment&lt;/a&gt; and &lt;a href="https://arxiv.org/abs/1905.04899"&gt;CutMix&lt;/a&gt;). We show top-1 accuracy for ImageNet, on ResNet-50, ResNet-101 and ResNet200.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;We also demonstrate analytically that the gradient of our loss function encourages learning from hard positives and hard negatives. The gradient contributions from hard positives/negatives are large while those for easy positives/negatives are small. This implicit property allows the contrastive loss to sidestep the need for explicit hard mining, which is a delicate but critical part of many losses, such as triplet loss. See the supplementary material of &lt;a href="https://arxiv.org/abs/2004.11362"&gt;our paper&lt;/a&gt; for a full derivation. &lt;/p&gt;&lt;p&gt;SupCon is also more robust to natural corruptions, such as noise, blur and JPEG compression. The &lt;a href="https://arxiv.org/abs/1903.12261"&gt;mean Corruption Error&lt;/a&gt; (mCE) measures the average degradation in performance compared to the benchmark &lt;a href="https://github.com/hendrycks/robustness"&gt;ImageNet-C&lt;/a&gt; dataset. The SupCon models have lower mCE values across different corruptions compared to cross-entropy models, showing increased robustness. &lt;/p&gt;&lt;p&gt;We show empirically that the SupCon loss is less sensitive than cross-entropy to a range of hyperparameters. Across changes in augmentations, optimizers, and learning rates, we observe significantly lower variance in the output of the contrastive loss. Moreover, applying different batch sizes while holding all other hyperparameters constant results in consistently better top-1 accuracy of SupCon to that of cross-entropy at each batch size. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-KZnJ8P1FABc/YLpXaPwslVI/AAAAAAAAHsc/dinL4h5X7Ici2p7CXGF2UHihm8RJhZwYQCLcBGAsYHQ/s1558/image6.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="492" data-original-width="1558" height="202" src="https://1.bp.blogspot.com/-KZnJ8P1FABc/YLpXaPwslVI/AAAAAAAAHsc/dinL4h5X7Ici2p7CXGF2UHihm8RJhZwYQCLcBGAsYHQ/w640-h202/image6.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Accuracy of cross-entropy and supervised contrastive loss as a function of hyperparameters and training data size, measured on ImageNet with a ResNet-50 encoder. &lt;strong&gt;Left:&lt;/strong&gt; Boxplot showing Top-1 accuracy vs changes in augmentation, optimizer and learning rates. SupCon yields more consistent results across variations in each, which is useful when the best strategies are unknown &lt;em&gt;a priori&lt;/em&gt;. &lt;strong&gt;Right:&lt;/strong&gt; Top-1 accuracy as a function of batch size shows both losses benefit from larger batch sizes while SupCon has higher Top-1 accuracy, even when trained with small batch sizes.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-YZ0t1Cpji9k/YLpXee49WjI/AAAAAAAAHsk/O7P93IFUEN8su4ny9SymjiuNhAyuGoQawCLcBGAsYHQ/s1999/image4.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="666" data-original-width="1999" height="214" src="https://1.bp.blogspot.com/-YZ0t1Cpji9k/YLpXee49WjI/AAAAAAAAHsk/O7P93IFUEN8su4ny9SymjiuNhAyuGoQawCLcBGAsYHQ/w640-h214/image4.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Accuracy of supervised contrastive loss as a function of training duration and the temperature hyperparameter, measured on ImageNet with a ResNet-50 encoder. &lt;strong&gt;Left: &lt;/strong&gt;Top-1 accuracy as a function of SupCon pre-training epochs. &lt;strong&gt;Right: &lt;/strong&gt;Top-1 accuracy as a function of temperature during the pre-training stage for SupCon. Temperature is an &lt;a href="https://arxiv.org/pdf/2012.09740.pdf"&gt;important hyperparameter&lt;/a&gt; in contrastive learning and reducing sensitivity to temperature is desirable.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;b&gt;Broader Impact and Next Steps&lt;/b&gt;&lt;br /&gt;This work provides a technical advancement in the field of supervised classification. Supervised contrastive learning can improve both the accuracy and robustness of classifiers with minimal complexity. The classic cross-entropy loss can be seen as a special case of SupCon where the views correspond to the images and the learned embeddings in the final linear layer corresponding to the labels. We note that SupCon benefits from large batch sizes, and being able to train the models on smaller batches is an important topic for future research.  &lt;/p&gt;&lt;p&gt;&lt;a href="https://github.com/google-research/google-research/tree/master/supcon"&gt;Our Github repository&lt;/a&gt; includes Tensorflow code to train the models in the &lt;a href="https://arxiv.org/abs/2004.11362"&gt;paper&lt;/a&gt;. Our pre-trained models are also &lt;a href="https://tfhub.dev/s?publisher=google&amp;amp;q=supcon"&gt;released&lt;/a&gt; on TF-Hub.  &lt;/p&gt;&lt;p&gt;&lt;b&gt;Acknowledgements&lt;/b&gt;&lt;br /&gt;&lt;em&gt;The NeurIPS paper was jointly co-authored with Prannay Khosla, Piotr Teterwak, Chen Wang,  Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Special thanks to Jenny Huang for leading the writing process for this blogpost.&lt;/em&gt;&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/blogspot/gJZg?a=YxHuHBOiX6U:wly85rlObuA:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/blogspot/gJZg/~4/YxHuHBOiX6U" height="1" width="1" alt=""/&gt;</content>
    <link rel="replies" type="application/atom+xml" href="http://ai.googleblog.com/feeds/7927167454181190382/comments/default" title="Post Comments"/>
    <link rel="replies" type="text/html" href="http://ai.googleblog.com/2021/06/extending-contrastive-learning-to.html#comment-form" title="0 Comments"/>
    <link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/7927167454181190382"/>
    <link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/7927167454181190382"/>
    <link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/blogspot/gJZg/~3/YxHuHBOiX6U/extending-contrastive-learning-to.html" title="Extending Contrastive Learning to the Supervised Setting"/>
    <author>
      <name>Google AI</name>
      <uri>http://www.blogger.com/profile/12098626514775266161</uri>
      <email>noreply@blogger.com</email>
      <gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="16" height="16" src="https://img1.blogblog.com/img/b16-rounded.gif"/>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://1.bp.blogspot.com/-_fMby-ad2AA/YLpXBtlI7fI/AAAAAAAAHsI/7TE6W0F5yNAhGEZFwPSvIJUM51A0ETs5wCLcBGAsYHQ/s72-w640-h296-c/image5.png" height="72" width="72"/>
    <thr:total>0</thr:total>
    <feedburner:origLink>http://ai.googleblog.com/2021/06/extending-contrastive-learning-to.html</feedburner:origLink>
  </entry>
  <entry>
    <id>tag:blogger.com,1999:blog-8474926331452026626.post-3872842883218714444</id>
    <published>2021-06-04T09:33:00.000-07:00</published>
    <updated>2021-06-04T09:33:16.713-07:00</updated>
    <category scheme="http://www.blogger.com/atom/ns#" term="AI"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="HCI"/>
    <title type="text">Data Cascades in Machine Learning</title>
    <content type="html">&lt;span class="byline-author"&gt;Nithya Sambasivan, Research Scientist, Google Research&lt;/span&gt; &lt;p&gt;Data is a &lt;a href="https://ieeexplore.ieee.org/abstract/document/4804817"&gt;foundational aspect&lt;/a&gt; of machine learning (ML) that can impact performance, fairness, robustness, and scalability of ML systems. Paradoxically, while building ML models is often highly prioritized,  the work related to data itself is often the least prioritized aspect. This data work can require multiple roles (such as data collectors, annotators, and ML developers) and often involves multiple teams (such as database, legal, or licensing teams) to power a data infrastructure, which adds complexity to any data-related project. As such, the field of &lt;a href="https://en.wikipedia.org/wiki/Human%E2%80%93computer_interaction"&gt;human-computer interaction&lt;/a&gt; (HCI), which is focused on making technology useful and usable for people, can help both to identify potential issues and to assess the impact on models when data-related work is not prioritized. &lt;/p&gt;&lt;p&gt;In “&lt;a href="https://research.google/pubs/pub49953/"&gt;‘Everyone wants to do the model work, not the data work’: Data Cascades in High-Stakes AI&lt;/a&gt;”, published at the 2021 &lt;a href="https://chi2021.acm.org/"&gt;ACM CHI Conference&lt;/a&gt;, we study and validate downstream effects from data issues that result in technical debt over time (defined as "data cascades"). Specifically, we illustrate the phenomenon of data cascades with the data practices and challenges of ML practitioners across the globe working in important ML domains, such as cancer detection, landslide detection, loan allocation and more — domains where ML systems have enabled progress, but also where there is opportunity to improve by addressing data cascades. This work is the first that we know of to formalize, measure, and discuss data cascades in ML as applied to real-world projects. We further discuss the opportunity presented by a collective re-imagining of ML data as a high priority, including rewarding ML data work and workers, recognizing the scientific empiricism in ML data research, improving the visibility of data pipelines, and improving data equity around the world. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Origins of Data Cascades&lt;/b&gt;&lt;br/&gt;We observe that data cascades often originate early in the lifecycle of an ML system, at the stage of data definition and collection. Cascades also tend to be complex and opaque in diagnosis and manifestation, so there are often no clear indicators, tools, or metrics to detect and measure their effects. Because of this, small data-related obstacles can grow into larger and more complex challenges that affect how a model is developed and deployed. Challenges from data cascades include the need to perform costly system-level changes much later in the development process, or the decrease in users’ trust due to model mis-predictions that result from data issues. Nevertheless and encouragingly, we also observe that such data cascades can be avoided through early interventions in ML development. &lt;/p&gt;  &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-xcHF2HnezK8/YLpLkBLXlcI/AAAAAAAAHsA/-Zvn2iT7KZ4HLnQhcAbDar92AFOV6YlswCLcBGAsYHQ/s1999/image1.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="576" data-original-width="1999" height="184" src="https://1.bp.blogspot.com/-xcHF2HnezK8/YLpLkBLXlcI/AAAAAAAAHsA/-Zvn2iT7KZ4HLnQhcAbDar92AFOV6YlswCLcBGAsYHQ/w640-h184/image1.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Different color arrows indicate different types of data cascades, which typically originate upstream, compound over the ML development process, and manifest downstream.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;    &lt;p&gt;&lt;b&gt;Examples of Data Cascades&lt;/b&gt;&lt;br/&gt;One of the most common causes of data cascades is when models that are trained on noise-free datasets are deployed in the often-noisy real world. For example, a common type of data cascade originates from model &lt;em&gt;drifts&lt;/em&gt;, which occur when target and independent variables deviate, resulting in less accurate models. Drifts are more common when models closely interact with new digital environments — including high-stakes domains, such as air quality sensing, ocean sensing, and ultrasound scanning — because there are no pre-existing and/or curated datasets. Such drifts can lead to more factors that further decrease a model’s performance (e.g., related to hardware, environmental, and human knowledge). For example, to ensure good model performance, data is often collected in controlled, in-house environments. But in the live systems of new digital environments with resource constraints, it is more common for data to be collected with physical artefacts such as fingerprints, shadows, dust, improper lighting, and pen markings, which can add noise that affects model performance. In other cases, environmental factors such as rain and wind can unexpectedly move image sensors in deployment, which also trigger cascades. As one of the model developers we interviewed reported, even a small drop of oil or water can affect data that could be used to train a cancer prediction model, therefore affecting the model’s performance. Because drifts are often caused by the noise in real-world environments, they also take the longest — up to 2-3 years — to manifest, almost always in production. &lt;/p&gt;&lt;p&gt;Another common type of data cascade can occur when ML practitioners are tasked with managing data in domains in which they have limited expertise. For instance, certain kinds of information, such as identifying poaching locations or data collected during underwater exploration, rely on expertise in the biological sciences, social sciences, and community context. However, some developers in our study described having to take a range of data-related actions that surpassed their domain expertise — e.g., discarding data, correcting values, merging data, or restarting data collection — leading to data cascades that limited model performance. The practice of relying on technical expertise more than domain expertise (e.g., by engaging with domain experts) is what appeared to set off these cascades.  &lt;/p&gt;&lt;p&gt;Two other cascades observed in this paper resulted from conflicting incentives and organizational practices between data collectors, ML developers, and other partners — for example, one cascade was caused by poor dataset documentation. While work related to data requires careful coordination across multiple teams, this is especially challenging when stakeholders are not aligned on priorities or workflows. &lt;/p&gt;&lt;p&gt;&lt;b&gt;How to Address Data Cascades&lt;/b&gt;&lt;br/&gt;Addressing data cascades requires a multi-part, systemic approach in ML research and practice:  &lt;/p&gt;&lt;ol&gt; &lt;li&gt;Develop and communicate the concept of &lt;em&gt;goodness of the data &lt;/em&gt;that an ML system  starts with, similar to how we think about &lt;em&gt;goodness of fit&lt;/em&gt; with models. This includes developing standardized metrics and frequently using those metrics to measure data aspects like phenomenological fidelity (how accurately and comprehensively does the data represent the phenomena) and validity (how well the data explains things related to the phenomena captured by the data), similar to how we have developed good metrics to measure model performance, like &lt;a href="https://en.wikipedia.org/wiki/F-score"&gt;F1-scores&lt;/a&gt;.   &lt;/li&gt;&lt;li&gt;Innovate on incentives to recognize work on data, such as welcoming empiricism on data in conference tracks, rewarding dataset maintenance, or rewarding employees for their work on data (collection, labelling, cleaning, or maintenance) in organizations.   &lt;/li&gt;&lt;li&gt;Data work often requires coordination across multiple roles and multiple teams, but this is quite limited currently (partly, but not wholly, because of the previously stated factors). Our research points to the value of fostering greater collaboration, transparency, and fairer distribution of benefits between data collectors, domain experts, and ML developers, especially with ML systems that rely on collecting or labelling niche datasets.   &lt;/li&gt;&lt;li&gt;Finally, our research across multiple countries indicates that data scarcity is pronounced in lower-income countries, where ML developers face the additional problem of defining and hand-curating new datasets, which makes it difficult to even start developing ML systems. It is important to enable open dataset banks, create data policies, and foster ML literacy of policy makers and civil society to address the current data inequalities globally. &lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;b&gt;Conclusion&lt;/b&gt;&lt;br/&gt;In this work we both provide empirical evidence and formalize the concept of data cascades in ML systems. We hope to create an awareness of the potential value that could come from incentivising data excellence. We also hope to introduce an under-explored but significant new research agenda for HCI. Our research on data cascades has led to evidence-backed, state-of-the-art guidelines for data collection and evaluation in the revised &lt;a href="https://pair.withgoogle.com/guidebook/"&gt;PAIR Guidebook&lt;/a&gt;, aimed at ML developers and designers. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Acknowledgements&lt;/b&gt;&lt;br/&gt;&lt;em&gt;This paper was written in collaboration with Shivani Kapania, Hannah Highfill, Diana Akrong, Praveen Paritosh and Lora Aroyo. We thank our study participants, and Sures Kumar Thoddu Srinivasan, Jose M. Faleiro, Kristen Olson, Biswajeet Malik, Siddhant Agarwal, Manish Gupta, Aneidi Udo-Obong, Divy Thakkar, Di Dang, and Solomon Awosupin.&lt;/em&gt;&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/blogspot/gJZg?a=lDAMFyteOvM:pQAUmAjFgGk:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/blogspot/gJZg/~4/lDAMFyteOvM" height="1" width="1" alt=""/&gt;</content>
    <link rel="replies" type="application/atom+xml" href="http://ai.googleblog.com/feeds/3872842883218714444/comments/default" title="Post Comments"/>
    <link rel="replies" type="text/html" href="http://ai.googleblog.com/2021/06/data-cascades-in-machine-learning.html#comment-form" title="0 Comments"/>
    <link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/3872842883218714444"/>
    <link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/3872842883218714444"/>
    <link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/blogspot/gJZg/~3/lDAMFyteOvM/data-cascades-in-machine-learning.html" title="Data Cascades in Machine Learning"/>
    <author>
      <name>Google AI</name>
      <uri>http://www.blogger.com/profile/12098626514775266161</uri>
      <email>noreply@blogger.com</email>
      <gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="16" height="16" src="https://img1.blogblog.com/img/b16-rounded.gif"/>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://1.bp.blogspot.com/-xcHF2HnezK8/YLpLkBLXlcI/AAAAAAAAHsA/-Zvn2iT7KZ4HLnQhcAbDar92AFOV6YlswCLcBGAsYHQ/s72-w640-h184-c/image1.png" height="72" width="72"/>
    <thr:total>0</thr:total>
    <feedburner:origLink>http://ai.googleblog.com/2021/06/data-cascades-in-machine-learning.html</feedburner:origLink>
  </entry>
  <entry>
    <id>tag:blogger.com,1999:blog-8474926331452026626.post-179707356845109938</id>
    <published>2021-06-03T09:44:00.000-07:00</published>
    <updated>2021-06-03T09:44:49.774-07:00</updated>
    <category scheme="http://www.blogger.com/atom/ns#" term="Machine Perception"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="Publications"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="Research"/>
    <category scheme="http://www.blogger.com/atom/ns#" term="Robotics"/>
    <title type="text">Toward Generalized Sim-to-Real Transfer for Robot Learning</title>
    <content type="html">&lt;span class="byline-author"&gt;Daniel Ho, Software Engineer, The Everyday Robot Project and Kanishka Rao, Staff Software Engineer, Robotics at Google&lt;/span&gt; &lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Reinforcement_learning#:~:text=Reinforcement%20learning%20(RL)%20is%20an,supervised%20learning%20and%20unsupervised%20learning."&gt;Reinforcement&lt;/a&gt; and &lt;a href="https://arxiv.org/pdf/1811.06711.pdf"&gt;imitation learning&lt;/a&gt; methods in robotics research can enable autonomous &lt;a href="https://ai.googleblog.com/2021/04/model-based-rl-for-decentralized-multi.html"&gt;environmental navigation&lt;/a&gt; and efficient &lt;a href="https://ai.googleblog.com/2021/05/learning-to-manipulate-deformable.html"&gt;object manipulation&lt;/a&gt;, which in turn opens up a breadth of useful real-life applications. &lt;a href="https://ai.googleblog.com/2017/10/closing-simulation-to-reality-gap-for.html"&gt;Previous work&lt;/a&gt; has demonstrated how robots that learn end-to-end using deep neural networks can reliably and safely interact with the unstructured world around us by comprehending camera observations to take actions and solve tasks. However, while end-to-end learning methods can generalize and scale for complicated robot manipulation tasks, they require hundreds of thousands real world robot training episodes, which can be difficult to obtain. One can attempt to alleviate this constraint by using a simulation of the environment that allows virtual robots to learn more quickly and at scale, but the simulations’ inability to exactly match the real world presents a challenge c   ommonly referred to as the &lt;a href="https://ai.googleblog.com/2017/10/closing-simulation-to-reality-gap-for.html"&gt;sim-to-real gap&lt;/a&gt;. One important source of the gap comes from discrepancies between the images rendered in simulation and the real robot camera observations, which then causes the robot to perform poorly in the real world.  &lt;/p&gt;&lt;p&gt;To-date, work on bridging this gap has employed a technique called pixel-level domain adaptation, which translates synthetic images to realistic ones at the pixel level. One example of this technique is &lt;a href="https://ai.googleblog.com/2017/10/closing-simulation-to-reality-gap-for.html"&gt;GraspGAN&lt;/a&gt;, which employs a &lt;a href="https://en.wikipedia.org/wiki/Generative_adversarial_network"&gt;generative adversarial network&lt;/a&gt; (GAN), a framework that has been very effective at image generation, to model this transformation between simulated and real images given datasets of each domain. These pseudo-real images correct some sim-to-real gap, so policies learned with simulation execute more successfully on real robots. A limitation for their use in sim-to-real transfer, however, is that because GANs translate images at the pixel-level, multi-pixel features or structures that are necessary for robot task learning may be arbitrarily modified or even removed. &lt;/p&gt;&lt;p&gt;To address the above limitation, and in collaboration with the &lt;a href="https://x.company/projects/everyday-robots/"&gt;Everyday Robot Project&lt;/a&gt; at &lt;a href="https://x.company/"&gt;X&lt;/a&gt;, we introduce two works, &lt;a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Rao_RL-CycleGAN_Reinforcement_Learning_Aware_Simulation-to-Real_CVPR_2020_paper.pdf"&gt;RL-CycleGAN&lt;/a&gt; and &lt;a href="https://arxiv.org/abs/2011.03148"&gt;RetinaGAN&lt;/a&gt;, that train GANs with robot-specific consistencies — so that they do not arbitrarily modify visual features that are specifically necessary for robot task learning — and thus bridge the visual discrepancy between sim and real. We demonstrate how these consistencies preserve features critical to policy learning, eliminating the need for hand-engineered, task-specific tuning, which in turn allows for this sim-to-real methodology to work flexibly across tasks, domains, and learning algorithms. With RL-CycleGAN, we describe our sim-to-real transfer methodology and demonstrate state-of-the-art performance on real world grasping tasks trained with RL. With RetinaGAN, we extend our approach to include imitation learning with a door opening task. &lt;/p&gt;&lt;p&gt;&lt;b&gt;RL-CycleGAN&lt;/b&gt;&lt;br /&gt;In “&lt;a href="https://arxiv.org/pdf/2006.09001.pdf"&gt;RL-CycleGAN: Reinforcement Learning Aware Simulation-To-Real&lt;/a&gt;”, we leverage a variation of &lt;a href="https://junyanz.github.io/CycleGAN/"&gt;CycleGAN&lt;/a&gt; for sim-to-real adaptation by ensuring consistency of task-relevant features between real and simulated images. CycleGAN encourages preservation of image contents by ensuring an adapted image transformed back to the original domain is identical to the original image, which is called &lt;em&gt;cycle consistency&lt;/em&gt;. To further encourage the adapted images to be useful for robotics, the CycleGAN is jointly trained with a &lt;a href="https://en.wikipedia.org/wiki/Reinforcement_learning"&gt;reinforcement learning&lt;/a&gt; (RL) robot agent that ensures the robot’s actions are the same given both the original images and those after GAN-adaptation. That is, task-specific features like robot arm or graspable object locations are unaltered, but the GAN may still alter lighting or textural differences between domains that do not affect task-level decisions. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Evaluating RL-CycleGAN&lt;/b&gt;&lt;br /&gt;We evaluated RL-CycleGAN on a robotic &lt;a href="https://arxiv.org/abs/1806.10293"&gt;indiscriminate grasping task&lt;/a&gt;. Trained on 580,000 real trials and simulations adapted with RL-CycleGAN, the robot grasps objects with 94% success, surpassing the 89% success rate of the prior state-of-the-art sim-to-real method GraspGAN and the 87% mark using real-only data without simulation. With only 28,000 trials, the RL-CycleGAN method reaches 86%, comparable to the previous baselines with 20x the data. Some examples of the RL-CycleGAN output alongside the simulation images are shown below.&lt;/p&gt;  &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-KY56CJIH6bg/YLflfKbYM8I/AAAAAAAAHrk/VCG8g3bqXpY8Dx4p0df0790Nc9n7uxctQCLcBGAsYHQ/s1280/image5.gif" style="margin-left: auto; margin-right: auto; text-align: center;"&gt;&lt;img border="0" data-original-height="720" data-original-width="1280" height="360" src="https://1.bp.blogspot.com/-KY56CJIH6bg/YLflfKbYM8I/AAAAAAAAHrk/VCG8g3bqXpY8Dx4p0df0790Nc9n7uxctQCLcBGAsYHQ/w640-h360/image5.gif" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Comparison between simulation images of robot grasping before (&lt;b&gt;left&lt;/b&gt;) and after RL-CycleGAN translation (&lt;b&gt;right&lt;/b&gt;).&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;p&gt;&lt;b&gt;RetinaGAN&lt;/b&gt;&lt;br /&gt;While RL-CycleGAN reliably transfers from sim-to-real for the RL domain using task awareness, a natural question arises: can we develop a more flexible sim-to-real transfer technique that applies broadly to different tasks and robot learning techniques? &lt;/p&gt;&lt;p&gt;In “&lt;a href="https://arxiv.org/abs/2011.03148"&gt;RetinaGAN: An Object-Aware Approach to Sim-to-Real Transfer&lt;/a&gt;”, presented at &lt;a href="http://www.icra2021.org/"&gt;ICRA 2021&lt;/a&gt;, we develop such a task-decoupled, algorithm-decoupled GAN approach to sim-to-real transfer by instead focusing on robots’ perception of objects. RetinaGAN enforces strong object-semantic awareness through perception consistency via &lt;a href="https://en.wikipedia.org/wiki/Object_detection"&gt;object detection&lt;/a&gt; to predict bounding box locations for all objects on all images. In an ideal sim-to-real model, we expect the object detector to predict the same box locations before and after GAN translation, as objects should not change structurally. RetinaGAN is trained toward this ideal by backpropagation, such that there is consistency in perception of objects both when &lt;em&gt;a&lt;/em&gt;) simulated images are transformed from simulation to real and then back to simulation and &lt;em&gt;b&lt;/em&gt;) when real images are transformed from real to simulation and then back to real. We find this object-based consistency to be more widely applicable than the task-specific consistency required by RL-CycleGAN. &lt;/p&gt;  &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-2h6q02p44zw/YLfleVLKy_I/AAAAAAAAHrY/LdgeD_X3figI_9sG4IhLJsjVol9GxftrACLcBGAsYHQ/s764/image2.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="764" data-original-width="739" height="400" src="https://1.bp.blogspot.com/-2h6q02p44zw/YLfleVLKy_I/AAAAAAAAHrY/LdgeD_X3figI_9sG4IhLJsjVol9GxftrACLcBGAsYHQ/w388-h400/image2.png" width="388" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Diagram of RetinaGAN stages. The simulated image (&lt;b&gt;top left&lt;/b&gt;) is transformed by the sim-to-real generator and subsequently by the real-to-sim generator. The real image (&lt;b&gt;bottom left&lt;/b&gt;) undergoes the transformation in reverse order. Having separate pipelines that start with the simulated and real images improves the GAN’s performance.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;p&gt;&lt;b&gt;Evaluating RetinaGAN on a Real Robot&lt;/b&gt;&lt;br /&gt;Given the goal of building a more flexible sim-to-real transfer technique, we evaluate RetinaGAN in multiple ways to understand for which tasks and under what conditions it accomplishes sim-to-real transfer. &lt;/p&gt;&lt;p&gt;We first apply RetinaGAN to a grasping task. As demonstrated visually below, RetinaGAN emphasizes the translation of realistic object textures, shadows, and lighting, while maintaining the visual quality and saliency of the graspable objects. We couple a pre-trained RetinaGAN model with the distributed reinforcement learning method &lt;a href="https://arxiv.org/abs/1910.02787"&gt;Q2-Opt&lt;/a&gt; to train a vision-based task model for instance grasping. On real robots, this policy grasps object instances with 80% success when trained on a hundred thousand episodes — outperforming prior adaptation methods RL-CycleGAN and CycleGAN (both achieving ~68%) and training without domain adaptation (grey bars below: 19% with sim data, 22% with real data, and 54% with mixed data). This gives us confidence that perception consistency is a valuable strategy for sim-to-real transfer. Further, with just 10,000 training episodes (8% of the data), the RL policy with RetinaGAN grasps with 66% success, demonstrating performance of prior methods with significantly less data. &lt;/p&gt;  &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-CPstEnAHbms/YLfleMt-gPI/AAAAAAAAHrU/lhSTFMMWwNAfWl7JvEHx7_k_AkecqJvWACLcBGAsYHQ/s1201/image1.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="678" data-original-width="1201" height="362" src="https://1.bp.blogspot.com/-CPstEnAHbms/YLfleMt-gPI/AAAAAAAAHrU/lhSTFMMWwNAfWl7JvEHx7_k_AkecqJvWACLcBGAsYHQ/w640-h362/image1.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Evaluation performance of RL policies on instance grasping, trained with various datasets and sim-to-real methods. Low-Data RetinaGAN uses 8% of the real dataset.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-DTmu0TyTkDA/YLflfPR7zII/AAAAAAAAHrc/WGWtWsfWsNU15GTbYoFxbTlJiaGRAEYfwCLcBGAsYHQ/s600/image4.gif" style="margin-left: auto; margin-right: auto; text-align: center;"&gt;&lt;img border="0" data-original-height="300" data-original-width="600" height="320" src="https://1.bp.blogspot.com/-DTmu0TyTkDA/YLflfPR7zII/AAAAAAAAHrc/WGWtWsfWsNU15GTbYoFxbTlJiaGRAEYfwCLcBGAsYHQ/w640-h320/image4.gif" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;The simulated grasping environment (left) is translated to a realistic image (right) using RetinaGAN.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;p&gt;Next, we pair RetinaGAN with a different learning method, &lt;a href="https://link.springer.com/referenceworkentry/10.1007%2F978-0-387-30164-8_69"&gt;behavioral cloning&lt;/a&gt;, to open conference room doors given demonstrations by human operators. Using images from both simulated and real demonstrations, we train RetinaGAN to translate the synthetic images to look realistic, bridging the sim-to-real gap. We then train a behavior cloning model to imitate the task-solving actions of the human operators within real and RetinaGAN-adapted sim demonstrations. When evaluating this model by predicting actions to take, the robot enters real conference rooms over 93% of the time, surpassing baselines of 75% and below. &lt;/p&gt; &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-pjLpXBYNDIo/YLf_ugJKAPI/AAAAAAAAHr4/Xv1JT8ooY6Qt_OYHN2YkehiYsyuifZ1ngCLcBGAsYHQ/s800/meeting_room_gan_combined_all_linear_3x.gif" imageanchor="1" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="317" data-original-width="800" height="254" src="https://1.bp.blogspot.com/-pjLpXBYNDIo/YLf_ugJKAPI/AAAAAAAAHr4/Xv1JT8ooY6Qt_OYHN2YkehiYsyuifZ1ngCLcBGAsYHQ/w640-h254/meeting_room_gan_combined_all_linear_3x.gif" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Both of the above images show the same simulation, but RetinaGAN translates simulated door opening images (&lt;b&gt;left&lt;/b&gt;) to look more like real robot sensor data (&lt;b&gt;right&lt;/b&gt;).&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-7oOCC5GC-dw/YLfle0zVJnI/AAAAAAAAHrg/jsswbzFiemYbHrU-aOEKLy2Zd1pe-fKdACLcBGAsYHQ/s1920/image3.gif" style="margin-left: auto; margin-right: auto; text-align: center;"&gt;&lt;img border="0" data-original-height="512" data-original-width="1920" height="170" src="https://1.bp.blogspot.com/-7oOCC5GC-dw/YLfle0zVJnI/AAAAAAAAHrg/jsswbzFiemYbHrU-aOEKLy2Zd1pe-fKdACLcBGAsYHQ/w640-h170/image3.gif" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Three examples of the real robot successfully opening conference room doors using the RetinaGAN-trained behavior cloning policy.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;b&gt;Conclusion&lt;/b&gt;&lt;br /&gt;This work has demonstrated how additional constraints on GANs may address the visual sim-to-real gap without requiring task-specific tuning; these approaches reach higher real robot success rates with less data collection. RL-CycleGAN translates synthetic images to realistic ones with an RL-consistency loss that automatically preserves task-relevant features. RetinaGAN is an object-aware sim-to-real adaptation technique that transfers robustly across environments and tasks, agnostic to the task learning method. Since RetinaGAN is not trained with any task-specific knowledge, we show how it can be &lt;a href="https://retinagan.github.io/"&gt;reused for a novel object pushing task&lt;/a&gt;. We hope that work on the sim-to-real gap further generalizes toward solving task-agnostic robotic manipulation in unstructured environments. &lt;/p&gt;&lt;p&gt;&lt;b&gt;Acknowledgements&lt;/b&gt;&lt;br /&gt;&lt;em&gt;Research into RL-CycleGAN was conducted by Kanishka Rao, Chris Harris, Alex Irpan, Sergey Levine, Julian Ibarz, and Mohi Khansari. Research into RetinaGAN was conducted by Daniel Ho, Kanishka Rao, Zhuo Xu, Eric Jang, Mohi Khansari, and Yunfei Bai. We’d also like to give special thanks to Ivonne Fajardo, Noah Brown, Benjamin Swanson, Christopher Paguyo, Armando Fuentes, and Sphurti More for overseeing the robot operations. We thank Paul Wohlhart, Konstantinos Bousmalis, Daniel Kappler, Alexander Herzog, Anthony Brohan, Yao Lu, Chad Richards, Vincent Vanhoucke, and Mrinal Kalakrishnan, Max Braun and others in the &lt;a href="https://research.google/teams/brain/robotics/"&gt;Robotics at Google team&lt;/a&gt; and the &lt;a href="https://x.company/projects/everyday-robots/"&gt;Everyday Robot Project&lt;/a&gt; for valuable discussions and help.&lt;/em&gt;&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/blogspot/gJZg?a=_nfpSkl7C3M:U-cMpHPl4bc:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/blogspot/gJZg/~4/_nfpSkl7C3M" height="1" width="1" alt=""/&gt;</content>
    <link rel="replies" type="application/atom+xml" href="http://ai.googleblog.com/feeds/179707356845109938/comments/default" title="Post Comments"/>
    <link rel="replies" type="text/html" href="http://ai.googleblog.com/2021/06/toward-generalized-sim-to-real-transfer.html#comment-form" title="0 Comments"/>
    <link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/179707356845109938"/>
    <link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/179707356845109938"/>
    <link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/blogspot/gJZg/~3/_nfpSkl7C3M/toward-generalized-sim-to-real-transfer.html" title="Toward Generalized Sim-to-Real Transfer for Robot Learning"/>
    <author>
      <name>Google AI</name>
      <uri>http://www.blogger.com/profile/12098626514775266161</uri>
      <email>noreply@blogger.com</email>
      <gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="16" height="16" src="https://img1.blogblog.com/img/b16-rounded.gif"/>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://1.bp.blogspot.com/-KY56CJIH6bg/YLflfKbYM8I/AAAAAAAAHrk/VCG8g3bqXpY8Dx4p0df0790Nc9n7uxctQCLcBGAsYHQ/s72-w640-h360-c/image5.gif" height="72" width="72"/>
    <thr:total>0</thr:total>
    <feedburner:origLink>http://ai.googleblog.com/2021/06/toward-generalized-sim-to-real-transfer.html</feedburner:origLink>
  </entry>
  <entry>
    <id>tag:blogger.com,1999:blog-8474926331452026626.post-224803688131831951</id>
    <published>2021-06-01T12:24:00.002-07:00</published>
    <updated>2021-06-01T12:58:37.406-07:00</updated>
    <title type="text">A Browsable Petascale Reconstruction of the Human Cortex</title>
    <content type="html">&lt;span class="byline-author"&gt;Posted by Tim Blakely, Software Engineer and Michał Januszewski, Research Scientist, Connectomics at Google&lt;/span&gt; &lt;p&gt;In January 2020 we &lt;a href="https://ai.googleblog.com/2020/01/releasing-drosophila-hemibrain.html"&gt;released&lt;/a&gt; the fly “hemibrain” &lt;a href="https://elifesciences.org/articles/57443"&gt;connectome&lt;/a&gt; — an online database providing the morphological structure and synaptic connectivity of roughly half of the brain of a fruit fly (&lt;em&gt;Drosophila melanogaster&lt;/em&gt;). This database and its &lt;a href="https://hemibrain-dot-neuroglancer-demo.appspot.com/#!%7B%22dimensions%22:%7B%22x%22:%5B8e-9%2C%22m%22%5D%2C%22y%22:%5B8e-9%2C%22m%22%5D%2C%22z%22:%5B8e-9%2C%22m%22%5D%7D%2C%22position%22:%5B17114%2C20543%2C18610%5D%2C%22crossSectionScale%22:54.23751620061224%2C%22crossSectionDepth%22:-37.62185354999912%2C%22projectionScale%22:64770.91726975332%2C%22layers%22:%5B%7B%22type%22:%22image%22%2C%22source%22:%22precomputed://gs://neuroglancer-janelia-flyem-hemibrain/emdata/clahe_yz/jpeg%22%2C%22tab%22:%22source%22%2C%22name%22:%22emdata%22%7D%2C%7B%22type%22:%22segmentation%22%2C%22source%22:%22precomputed://gs://neuroglancer-janelia-flyem-hemibrain/v1.0/segmentation%22%2C%22tab%22:%22segments%22%2C%22name%22:%22segmentation%22%7D%2C%7B%22type%22:%22segmentation%22%2C%22source%22:%7B%22url%22:%22precomputed://gs://neuroglancer-janelia-flyem-hemibrain/v1.0/rois%22%2C%22subsources%22:%7B%22default%22:true%2C%22properties%22:true%2C%22mesh%22:true%7D%2C%22enableDefaultSubsources%22:false%7D%2C%22pick%22:false%2C%22tab%22:%22segments%22%2C%22selectedAlpha%22:0%2C%22saturation%22:0%2C%22objectAlpha%22:0.8%2C%22ignoreNullVisibleSet%22:false%2C%22meshSilhouetteRendering%22:3%2C%22colorSeed%22:2685294016%2C%22name%22:%22roi%22%7D%2C%7B%22type%22:%22annotation%22%2C%22source%22:%22precomputed://gs://neuroglancer-janelia-flyem-hemibrain/v1.0/synapses%22%2C%22tab%22:%22rendering%22%2C%22ignoreNullSegmentFilter%22:false%2C%22shader%22:%22#uicontrol%20vec3%20preColor%20color%28default=%5C%22red%5C%22%29%5Cn#uicontrol%20vec3%20postColor%20color%28default=%5C%22blue%5C%22%29%5Cn#uicontrol%20float%20preConfidence%20slider%28min=0%2C%20max=1%2C%20default=0%29%5Cn#uicontrol%20float%20postConfidence%20slider%28min=0%2C%20max=1%2C%20default=0%29%5Cn%5Cnvoid%20main%28%29%20%7B%5Cn%20%20setColor%28defaultColor%28%29%29%3B%5Cn%20%20setEndpointMarkerColor%28%5Cn%20%20%20%20vec4%28preColor%2C%200.5%29%2C%5Cn%20%20%20%20vec4%28postColor%2C%200.5%29%29%3B%5Cn%20%20setEndpointMarkerSize%282.0%2C%202.0%29%3B%5Cn%20%20setLineWidth%282.0%29%3B%5Cn%20%20if%20%28prop_pre_synaptic_confidence%28%29%3C%20preConfidence%20%7C%7C%5Cn%20%20%20%20%20%20prop_post_synaptic_confidence%28%29%3C%20postConfidence%29%20discard%3B%5Cn%7D%5Cn%22%2C%22linkedSegmentationLayer%22:%7B%22pre_synaptic_cell%22:%22segmentation%22%2C%22post_synaptic_cell%22:%22segmentation%22%7D%2C%22filterBySegmentation%22:%5B%22post_synaptic_cell%22%2C%22pre_synaptic_cell%22%5D%2C%22name%22:%22synapse%22%7D%2C%7B%22type%22:%22segmentation%22%2C%22source%22:%22precomputed://gs://neuroglancer-janelia-flyem-hemibrain/mito_20190717.27250582%22%2C%22pick%22:false%2C%22tab%22:%22segments%22%2C%22selectedAlpha%22:0.82%2C%22name%22:%22mito%22%2C%22visible%22:false%7D%2C%7B%22type%22:%22segmentation%22%2C%22source%22:%22precomputed://gs://neuroglancer-janelia-flyem-hemibrain/mask_normalized_round6%22%2C%22pick%22:false%2C%22tab%22:%22segments%22%2C%22selectedAlpha%22:0.53%2C%22segments%22:%5B%222%22%5D%2C%22name%22:%22mask%22%2C%22visible%22:false%7D%5D%2C%22showSlices%22:false%2C%22selectedLayer%22:%7B%22visible%22:true%2C%22layer%22:%22segmentation%22%7D%2C%22layout%22:%22xy-3d%22%2C%22selection%22:%7B%7D%7D"&gt;supporting visualization&lt;/a&gt; has &lt;a href="https://www.simonsfoundation.org/2021/02/25/the-connected-connectome/"&gt;reframed&lt;/a&gt; the way that neural circuits are studied and understood in the fly brain. While the fruit fly brain is small enough to attain a relatively complete map using modern mapping techniques, the insights gained are, at best, only partially informative to understanding the most interesting object in neuroscience — the &lt;em&gt;human&lt;/em&gt; brain.  &lt;/p&gt;&lt;p&gt;Today, in collaboration with the &lt;a href="https://lichtmanlab.fas.harvard.edu/"&gt;Lichtman Laboratory&lt;/a&gt; at Harvard University, we are releasing the &lt;a href="https://h01-release.storage.googleapis.com/landing.html"&gt;“H01” dataset&lt;/a&gt;, a 1.4 petabyte rendering of a small sample of human brain tissue, along with a companion paper, “&lt;a href="https://www.biorxiv.org/content/10.1101/2021.05.29.446289v1"&gt;A connectomic study of a petascale fragment of human cerebral cortex&lt;/a&gt;.”  The H01 sample was imaged at 4nm-resolution by &lt;a href="https://onlinelibrary.wiley.com/doi/pdf/10.1111/jmi.12224"&gt;serial section electron microscopy&lt;/a&gt;, reconstructed and annotated by automated computational techniques, and analyzed for preliminary insights into the structure of the human cortex. The dataset comprises imaging data that covers roughly one cubic millimeter of brain tissue, and includes tens of thousands of reconstructed neurons, millions of neuron fragments, 130 million annotated synapses, 104 proofread cells, and many additional subcellular annotations and structures — all easily accessible with the &lt;a href="https://h01-release-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/c3_library.json"&gt;Neuroglancer browser interface&lt;/a&gt;. H01 is thus far the largest sample of brain tissue imaged and reconstructed in this level of detail, in any species, and the first large-scale study of synaptic connectivity in the human cortex that spans multiple cell types across &lt;a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/cortical_layers.json"&gt;all layers of the cortex&lt;/a&gt;. The primary goals of this project are to produce a novel resource for studying the human brain and to improve and scale the underlying connectomics technologies. &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-LHNSzpDQsNg/YLZeqOiXedI/AAAAAAAAHps/96StiGoAdbIAghujEEnd9zTkimdigv9UACLcBGAsYHQ/s1600/image5.png" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="577" data-original-width="1600" height="230" src="https://1.bp.blogspot.com/-LHNSzpDQsNg/YLZeqOiXedI/AAAAAAAAHps/96StiGoAdbIAghujEEnd9zTkimdigv9UACLcBGAsYHQ/w640-h230/image5.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Petabyte connectomic reconstruction of a volume of human neocortex. &lt;strong&gt;Left:&lt;/strong&gt; Small subvolume of the dataset. &lt;strong&gt;Right:&lt;/strong&gt; A subgraph of 5000 neurons and excitatory (green) and inhibitory (red) connections in the dataset. The full graph (connectome) would be far too dense to visualize.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;b&gt;What is the Human Cortex?&lt;/b&gt;&lt;br /&gt;The &lt;a href="https://en.wikipedia.org/wiki/Cerebral_cortex"&gt;cerebral cortex&lt;/a&gt; is the thin surface layer of the brain found in vertebrate animals that has evolved most recently, showing the greatest variation in size among different mammals (it is especially large in humans). Each part of the cerebral cortex is six layered (e.g., &lt;a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/l2.json"&gt;L2&lt;/a&gt;), with &lt;a href="https://h01-release.storage.googleapis.com/explore.html"&gt;different kinds of nerve cells&lt;/a&gt; (e.g., &lt;a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/spiny_stellate.json"&gt;spiny stellate&lt;/a&gt;) in each layer. The cerebral cortex plays a crucial role in most higher level cognitive functions, such as thinking, memory, planning, perception, language, and attention. Although there has been some progress in understanding the macroscopic organization of this very complicated tissue, its organization at the level of individual nerve cells and their interconnecting synapses is largely unknown.  &lt;/p&gt;&lt;p&gt;&lt;b&gt;Human Brain Connectomics: From Surgical Biopsy to a 3D Database&lt;/b&gt;&lt;br /&gt;Mapping the structure of the brain at the resolution of individual synapses requires high-resolution microscopy techniques that can image biochemically stabilized (&lt;a href="https://en.wikipedia.org/wiki/Fixation_(histology)"&gt;fixed&lt;/a&gt;) tissue. We collaborated with brain surgeons at &lt;a href="https://www.massgeneral.org/pathology/research/frosch-lab"&gt;Massachusetts General Hospital&lt;/a&gt; in Boston (MGH) who sometimes remove pieces of normal human cerebral cortex when performing a surgery to cure &lt;a href="https://en.wikipedia.org/wiki/Epilepsy"&gt;epilepsy&lt;/a&gt; in order to gain access to a site in the deeper brain where an epileptic seizure is being initiated. Patients anonymously donated this tissue, which is normally discarded, to our colleagues in the Lichtman lab. The Harvard researchers cut the tissue into ~5300 individual 30 nanometer sections using an &lt;a href="https://www.wormatlas.org/EMmethods/ATUM.htm"&gt;automated tape collecting ultra-microtome&lt;/a&gt;, mounted those sections onto silicon wafers, and then imaged the brain tissue at 4 nm resolution in a customized &lt;a href="https://www.zeiss.com/semiconductor-manufacturing-technology/products-solutions/process-control-solutions/multisem-multi-beam-sem.html"&gt;61-beam parallelized scanning electron microscope&lt;/a&gt; for rapid image acquisition.  &lt;/p&gt;&lt;p&gt;Imaging the ~5300 physical sections produced 225 million individual 2D images. Our team then computationally stitched and aligned this data to produce a &lt;em&gt;single&lt;/em&gt; 3D volume. While the quality of the data was generally excellent, these alignment pipelines had to robustly handle a number of challenges, including imaging artifacts, missing sections, variation in microscope parameters, and physical stretching and compression of the tissue. Once aligned, a multiscale &lt;a href="https://ai.googleblog.com/2018/07/improving-connectomics-by-order-of.html"&gt;flood-filling network&lt;/a&gt; pipeline was applied (using thousands of Google Cloud &lt;a href="https://cloud.google.com/tpu"&gt;TPUs&lt;/a&gt;) to produce a 3D segmentation of each individual cell in the tissue. Additional machine learning pipelines were applied to identify and characterize 130 million &lt;a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/synapse_annotations.json"&gt;synapses&lt;/a&gt;, classify each 3D fragment into various “subcompartments” (e.g., &lt;a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/axon.json"&gt;axon&lt;/a&gt;, &lt;a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/dendrite.json"&gt;dendrite&lt;/a&gt;, or &lt;a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/soma.json"&gt;cell body&lt;/a&gt;), and identify other structures of interest such as &lt;a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/myelin.json"&gt;myelin&lt;/a&gt; and &lt;a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/cilia.json"&gt;cilia&lt;/a&gt;. Automated reconstruction results were imperfect, so manual efforts were used to “proofread” roughly &lt;a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/proofread_104.json"&gt;one hundred cells&lt;/a&gt; in the data. Over time, we expect to add additional cells to this verified set through additional manual efforts and further advances in automation.  &lt;/p&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;iframe allowfullscreen="" class="BLOG_video_class" height="360" src="https://www.youtube.com/embed/bvlSV_6wKO4" width="640" youtube-src-id="bvlSV_6wKO4"&gt;&lt;/iframe&gt;&lt;/div&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;The H01 volume: roughly one cubic millimeter of human brain tissue captured in 1.4 petabytes of images.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;The imaging data, reconstruction results, and annotations are viewable through an interactive web-based 3D visualization interface, called &lt;a href="https://h01-release-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/c3_library.json"&gt;Neuroglancer&lt;/a&gt;, that was &lt;a href="https://ai.googleblog.com/2019/08/an-interactive-automated-3d.html"&gt;originally developed&lt;/a&gt; to visualize the fruit fly brain. Neuroglancer is available as &lt;a href="https://github.com/google/neuroglancer"&gt;open-source software&lt;/a&gt;, and widely used in the broader connectomics community. Several new features were introduced to support analysis of the H01 dataset, in particular support for searching for specific neurons in the dataset based on their type or other properties.  &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; width: 100%;"&gt; &lt;colgroup&gt;   &lt;col span="1" style="width: 30%;"&gt;&lt;/col&gt;   &lt;col span="1" style="width: 5%;"&gt;&lt;/col&gt;   &lt;col span="1" style="width: 30%;"&gt;&lt;/col&gt;   &lt;col span="1" style="width: 5%;"&gt;&lt;/col&gt;   &lt;col span="1" style="width: 30%;"&gt;&lt;/col&gt; &lt;/colgroup&gt; &lt;tbody&gt;    &lt;tr&gt;    &lt;td&gt;&lt;a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/cortical_layers.json" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="976" data-original-width="1880" src="https://1.bp.blogspot.com/-GoOFTeVcOsc/YLZhNRZmRdI/AAAAAAAAHp0/RYOhSneEzBk7VoASsrFPNja08PR4WgohgCLcBGAsYHQ/s320/image4.png" width="100%" /&gt;   &lt;/a&gt;&lt;/td&gt;      &lt;td&gt;&amp;nbsp;&lt;/td&gt;   &lt;td&gt;&lt;a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/l2.json" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="972" data-original-width="1874" src="https://1.bp.blogspot.com/-QZyDDDlFpLc/YLZhUEb-GBI/AAAAAAAAHp4/IjU5arqo_LE8A5XGuepRPI6m7IesDZaCQCLcBGAsYHQ/s320/image8.png" width="100%" /&gt;&lt;/a&gt;   &lt;/td&gt;      &lt;td&gt;&amp;nbsp;&lt;/td&gt;   &lt;td&gt;&lt;a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/excitatory_inhibitory.json" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="928" data-original-width="1874" src="https://1.bp.blogspot.com/-A4RdiUNjbrE/YLZhUICikuI/AAAAAAAAHp8/T3QsNma-c-8eSI33bj1nMERCu5s7UFNUACLcBGAsYHQ/s320/image6.png" width="100%" /&gt;&lt;/a&gt;   &lt;/td&gt;  &lt;/tr&gt;  &lt;tr&gt;    &lt;td&gt;&lt;em style="font-size: small; text-align: center;"&gt;The volume spans all six cortical layers&lt;/em&gt;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&lt;em style="font-size: small; text-align: center;"&gt;Highlighting Layer 2 interneurons&lt;/em&gt;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&lt;em style="font-size: small; text-align: center;"&gt;Excitatory and inhibitory incoming synapses&lt;/em&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; width: 100%;"&gt; &lt;colgroup&gt;   &lt;col span="1" style="width: 30%;"&gt;&lt;/col&gt;   &lt;col span="1" style="width: 5%;"&gt;&lt;/col&gt;   &lt;col span="1" style="width: 30%;"&gt;&lt;/col&gt;   &lt;col span="1" style="width: 5%;"&gt;&lt;/col&gt;   &lt;col span="1" style="width: 30%;"&gt;&lt;/col&gt; &lt;/colgroup&gt; &lt;tbody&gt;  &lt;tr&gt;   &lt;td&gt;&lt;a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/subcompartments_render.json" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="931" data-original-width="1880" src="https://1.bp.blogspot.com/-2WSdXDQuNAo/YLZiJe2FA2I/AAAAAAAAHqQ/iY4Nx_F5cM4l6LLGWqs_y-8gH_jMgUm5QCLcBGAsYHQ/s320/image2.png" width="100%" /&gt;   &lt;/a&gt;&lt;/td&gt;    &lt;td&gt;&amp;nbsp;&lt;/td&gt;   &lt;td&gt;&lt;a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/chandelier.json" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="931" data-original-width="1880" src="https://1.bp.blogspot.com/-XhCJyXYFhhw/YLZiJRfaltI/AAAAAAAAHqI/Sr20T6TutiAXGPGd-r9cuLFJiZJSCj3WgCLcBGAsYHQ/s320/image11.png" width="100%" /&gt;&lt;/a&gt;   &lt;/td&gt;    &lt;td&gt;&amp;nbsp;&lt;/td&gt;   &lt;td&gt;&lt;a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/blood_vessels.json" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" data-original-height="928" data-original-width="1876" src="https://1.bp.blogspot.com/-j9VHIgHLG5o/YLZiJdRWlpI/AAAAAAAAHqM/-yyXQ8BWuH4UpBgQL-UPcWQq8zQo1K-iQCLcBGAsYHQ/s320/image1.png" width="100%" /&gt;&lt;/a&gt;      &lt;/td&gt;  &lt;/tr&gt;     &lt;tr&gt;    &lt;td&gt;&lt;em style="font-size: small; text-align: center;"&gt;Neuronal subcompartments classified&lt;/em&gt;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&lt;em style="font-size: small; text-align: center;"&gt;A Chandelier cell and some of the Pyramidal neurons it inhibits&lt;/em&gt;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&lt;em style="font-size: small; text-align: center;"&gt;Blood vessels traced throughout the volume&lt;/em&gt;&lt;/td&gt;   &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; width: 100%;"&gt; &lt;colgroup&gt;   &lt;col span="1" style="width: 30%;"&gt;&lt;/col&gt;   &lt;col span="1" style="width: 5%;"&gt;&lt;/col&gt;   &lt;col span="1" style="width: 30%;"&gt;&lt;/col&gt;   &lt;col span="1" style="width: 5%;"&gt;&lt;/col&gt;   &lt;col span="1" style="width: 30%;"&gt;&lt;/col&gt; &lt;/colgroup&gt; &lt;tbody&gt;  &lt;tr&gt;   &lt;td&gt;&lt;a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/figs/figS19.json" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img alt="" border="0" data-original-height="923" data-original-width="1873" src="https://1.bp.blogspot.com/-GupTziQjsQU/YLZjgZhncMI/AAAAAAAAHq8/P2pvIH8lgnYOqV159TYSF4mTHO820cPqQCLcBGAsYHQ/s320/image9.png" width="100%" /&gt;&lt;/a&gt;       &lt;/td&gt;    &lt;td&gt;&amp;nbsp;&lt;/td&gt;   &lt;td&gt;&lt;a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/axon_whorl.json" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img alt="" border="0" data-original-height="925" data-original-width="1877" src="https://1.bp.blogspot.com/-gOMtm0TBZhg/YLZjgW3BmlI/AAAAAAAAHq4/wKtVSLrEcU8IV8qjHuubb9F7pb4BjOw6QCLcBGAsYHQ/s320/image7.png" width="100%" /&gt;&lt;/a&gt;   &lt;/td&gt;    &lt;td&gt;&amp;nbsp;&lt;/td&gt;   &lt;td&gt;&lt;a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/reflexive_junctions.json" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img alt="" border="0" data-original-height="928" data-original-width="1870" src="https://1.bp.blogspot.com/-jNk9BkA_9Ho/YLZjgdEQs9I/AAAAAAAAHq0/6SqnIMBOvu02kgt6cpE5XNDIlrunEZveACLcBGAsYHQ/s320/image10.png" width="100%" /&gt;&lt;/a&gt;   &lt;/td&gt;  &lt;/tr&gt;    &lt;tr&gt;    &lt;td&gt;&lt;em style="font-size: small; text-align: center;"&gt;Serial contact between a pair of neurons&lt;/em&gt;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&lt;em style="font-size: small; text-align: center;"&gt;An axon with elaborate whorl structures&lt;/em&gt;&lt;/td&gt;&lt;td&gt;&amp;nbsp;&lt;/td&gt;&lt;td&gt;&lt;em style="font-size: small; text-align: center;"&gt;A neuron with unusual propensity for self-contact (Credit: Rachael Han)&lt;/em&gt;&lt;/td&gt;  &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;  &lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;The Neuroglancer interface to the H01 volume and annotations. The user can select specific cells on the basis of their layer and type, can view incoming and outgoing synapses for the cell, and much more. (Click on the images above to take you to the Neuroglancer view shown.)&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;   &lt;p&gt;&lt;b&gt;Analysis of the Human Cortex&lt;/b&gt;&lt;br /&gt;In a &lt;a href="https://www.biorxiv.org/content/10.1101/2021.05.29.446289v1"&gt;companion preprint&lt;/a&gt;, we show how H01 has already been used to study several interesting aspects of the organization of the human cortex. In particular, new cell types have been discovered, as well as the presence of “outlier” axonal inputs, which establish powerful synaptic connections with target dendrites. While these findings are a promising start, the vastness of the H01 dataset will provide a basis for many years of further study by researchers interested in the human cortex. &lt;/p&gt;&lt;p&gt;In order to accelerate the analysis of H01, we also provide &lt;a href="https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture"&gt;embeddings&lt;/a&gt; of the H01 data that were generated by a neural network trained using a variant of the &lt;a href="https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html"&gt;SimCLR&lt;/a&gt; self-supervised learning technique. &lt;a href="https://h01-release.storage.googleapis.com/embeddings.html"&gt;These embeddings&lt;/a&gt; provide highly informative representations of local parts of the dataset that can be used to rapidly annotate new structures and develop new ways of clustering and categorizing brain structures according to purely data-driven criteria. We trained these embeddings using Google Cloud &lt;a href="https://cloud.google.com/tpu/docs/training-on-tpu-pods"&gt;TPU pods&lt;/a&gt; and then performed inference at roughly four billion data locations spread throughout the volume.  &lt;/p&gt;&lt;p&gt;&lt;b&gt;Managing Dataset Size with Improved Compression&lt;/b&gt;&lt;br /&gt;H01 is a petabyte-scale dataset, but is only one-&lt;em&gt;millionth&lt;/em&gt; the volume of an entire human brain. Serious technical challenges remain in scaling up synapse-level brain mapping to an &lt;a href="https://www.sciencedirect.com/science/article/abs/pii/S0092867420310011"&gt;entire mouse brain&lt;/a&gt; (500x bigger than H01), let alone an entire human brain. One of these challenges is data storage: a mouse brain could generate an exabyte worth of data, which is costly to store. To address this, we are today also releasing a paper, “&lt;a href="https://www.biorxiv.org/content/10.1101/2021.05.29.445828v1"&gt;Denoising-based Image Compression for Connectomics&lt;/a&gt;”, that details how a machine learning-based denoising strategy can be used to compress data, such as H01, at least 17-fold (dashed line in the figure below), with negligible loss of accuracy in the automated reconstruction.  &lt;/p&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-TpKb0Ycw72Q/YLaHKBDDERI/AAAAAAAAHrM/Om5ioZmrnVYgVKaKRu4d3LkdSn-GrNphwCLcBGAsYHQ/s1999/image5.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="1428" data-original-width="1999" height="286" src="https://1.bp.blogspot.com/-TpKb0Ycw72Q/YLaHKBDDERI/AAAAAAAAHrM/Om5ioZmrnVYgVKaKRu4d3LkdSn-GrNphwCLcBGAsYHQ/w400-h286/image5.png" width="400" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Reconstruction quality of noisy and denoised images as a function of compression rate for &lt;a href="https://en.wikipedia.org/wiki/JPEG_XL"&gt;JPEG XL&lt;/a&gt; (JXL) and &lt;a href="https://en.wikipedia.org/wiki/AV1#AV1_Image_File_Format_(AVIF)"&gt;AV Image Format&lt;/a&gt; (AVIF) codecs. Points and lines show the means, and the shaded area covers ±1 standard deviation around the mean.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;Random variations in the electron microscopy imaging process lead to image noise that is difficult to compress even in principle, as the noise lacks spatial correlations or other structure that could be described with fewer bytes. Therefore we acquired images of the same piece of tissue in both a “fast” acquisition regime (resulting in high amounts of noise) and a “slow” acquisition regime (resulting in low amounts of noise) and then trained a neural network to infer “slow” scans from “fast” scans. Standard image compression codecs were then able to (lossily) compress the “virtual” slow scans with fewer artifacts compared to the raw data. We believe this advance has the potential to significantly mitigate the costs associated with future large scale connectomics projects.&lt;/p&gt;&lt;p&gt;&lt;b&gt;Next Steps&lt;/b&gt;&lt;br /&gt;But storage is not the only problem. The sheer size of future data sets will require developing new strategies for researchers to organize and access the rich information inherent in connectomic data. These are challenges that will require new modes of interaction between humans and the brain mapping data that will be forthcoming.&lt;/p&gt;&lt;div class="feedflare"&gt;
&lt;a href="http://feeds.feedburner.com/~ff/blogspot/gJZg?a=b8Xww-h31S0:SDcFG-kFKt8:yIl2AUoC8zA"&gt;&lt;img src="http://feeds.feedburner.com/~ff/blogspot/gJZg?d=yIl2AUoC8zA" border="0"&gt;&lt;/img&gt;&lt;/a&gt;
&lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/blogspot/gJZg/~4/b8Xww-h31S0" height="1" width="1" alt=""/&gt;</content>
    <link rel="replies" type="application/atom+xml" href="http://ai.googleblog.com/feeds/224803688131831951/comments/default" title="Post Comments"/>
    <link rel="replies" type="text/html" href="http://ai.googleblog.com/2021/06/a-browsable-petascale-reconstruction-of.html#comment-form" title="0 Comments"/>
    <link rel="edit" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/224803688131831951"/>
    <link rel="self" type="application/atom+xml" href="http://www.blogger.com/feeds/8474926331452026626/posts/default/224803688131831951"/>
    <link rel="alternate" type="text/html" href="http://feedproxy.google.com/~r/blogspot/gJZg/~3/b8Xww-h31S0/a-browsable-petascale-reconstruction-of.html" title="A Browsable Petascale Reconstruction of the Human Cortex"/>
    <author>
      <name>Google AI</name>
      <uri>http://www.blogger.com/profile/12098626514775266161</uri>
      <email>noreply@blogger.com</email>
      <gd:image rel="http://schemas.google.com/g/2005#thumbnail" width="16" height="16" src="https://img1.blogblog.com/img/b16-rounded.gif"/>
    </author>
    <media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://1.bp.blogspot.com/-LHNSzpDQsNg/YLZeqOiXedI/AAAAAAAAHps/96StiGoAdbIAghujEEnd9zTkimdigv9UACLcBGAsYHQ/s72-w640-h230-c/image5.png" height="72" width="72"/>
    <thr:total>0</thr:total>
    <feedburner:origLink>http://ai.googleblog.com/2021/06/a-browsable-petascale-reconstruction-of.html</feedburner:origLink>
  </entry>
</feed>
